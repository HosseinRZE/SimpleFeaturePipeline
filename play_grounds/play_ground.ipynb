{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3917bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717e253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN   = 5          # 1,3,5-candle sequences\n",
    "TOL       = 0.002      # acceptance band on raw coefficient\n",
    "DOWNSAMPLE_NO_LINE = 0.1   # keep only 10 % of no-line rows\n",
    "\n",
    "# 1. load candle file (has header row)\n",
    "df_c = pd.read_csv(\n",
    "    \"/home/iatell/financial_data/merge_data/archive/Bitcoin_BTCUSDT_kaggle_1D_candles_prop.csv\",\n",
    "    parse_dates=['timestamp']          # ensure datetime\n",
    ")\n",
    "\n",
    "# 2. load label file (no header)\n",
    "df_l = pd.read_csv(\n",
    "    \"/home/iatell/financial_data/merge_data/archive/meta_learning_data/ohlcv_log(2).csv\",\n",
    "    header=None,\n",
    "    names=['timestamp', 'last_close', 'line1_raw']\n",
    ")\n",
    "\n",
    "# 3. convert label timestamp to datetime **explicitly**\n",
    "df_l['timestamp'] = pd.to_datetime(df_l['timestamp'], errors='coerce')\n",
    "\n",
    "# 4. extract the parenthesised value\n",
    "df_l['line1'] = (\n",
    "    df_l['line1_raw']\n",
    "      .astype(str)\n",
    "      .str.extract(r'\\(([^)]+)\\)', expand=False)\n",
    "      .astype(float)\n",
    ")\n",
    "df_l = df_l.drop(columns=['line1_raw'])\n",
    "\n",
    "# 5. align lengths\n",
    "last_labeled = df_l['timestamp'].max()\n",
    "df_c = df_c[df_c['timestamp'] <= last_labeled].copy()\n",
    "# 2.2 join and create binary flag + log-ratio target\n",
    "df = pd.merge(df_c, df_l[['timestamp','line1']], on='timestamp')\n",
    "# create them once, right after you read df_c and df_l\n",
    "df['has_line'] = (df['line1'] != -1).astype(int)\n",
    "df['log_coef'] = np.where(df['has_line'] == 1,\n",
    "                          np.log(df['line1']),\n",
    "                          np.nan)     # will be masked later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdda0474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.4\n",
      "{'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'gpu_hist', 'validate_parameters': None, 'verbosity': None, 'gpu_id': 0}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "\n",
    "# Test if GPU is recognized\n",
    "model = xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "print(model.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e561ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with has_line==1 : 151\n"
     ]
    }
   ],
   "source": [
    "# print('total rows in label file :', len(df_l))\n",
    "print('rows with has_line==1 :', df['has_line'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e7a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multi_sequences(candles, lengths=(1,3,5)):\n",
    "    max_len = max(lengths)\n",
    "    X, y_class, y_reg = [], [], []\n",
    "    for i in range(max_len-1, len(candles)):\n",
    "        parts = []\n",
    "        for L in lengths:\n",
    "            seq = candles.iloc[i-L+1:i+1]\n",
    "            norm = seq.iloc[-1]['close']\n",
    "            parts.append((seq[['open','high','low','close']] / norm).values.ravel())\n",
    "        meta = candles.iloc[i][['upper_shadow','body','lower_shadow',\n",
    "                                'Candle_Color','upper_body_ratio',\n",
    "                                'lower_body_ratio','upper_lower_body_ratio']].values\n",
    "        X.append(np.concatenate(parts + [meta]))\n",
    "        y_class.append(candles.iloc[i]['has_line'])\n",
    "        y_reg.append(candles.iloc[i]['log_coef'])\n",
    "    return np.array(X), np.array(y_class), np.array(y_reg)\n",
    "\n",
    "X_all, y_cls_all, y_reg_all = make_multi_sequences(df, (1,3,5))\n",
    "\n",
    "# mask rows with NaN regression target\n",
    "mask = ~np.isnan(y_reg_all)\n",
    "X_reg   = X_all[mask]\n",
    "y_reg   = y_reg_all[mask]\n",
    "\n",
    "# build binary-classification dataset (all rows)\n",
    "X_cls   = X_all\n",
    "y_cls   = y_cls_all\n",
    "\n",
    "# down-sample majority (no-line) class to reduce “always predict 1.0”\n",
    "# 1. balanced classification indices\n",
    "neg_idx   = np.where(y_cls_all == 0)[0]\n",
    "pos_idx   = np.where(y_cls_all == 1)[0]\n",
    "neg_keep  = np.random.choice(neg_idx,\n",
    "                             size=int(DOWNSAMPLE_NO_LINE * len(neg_idx)),\n",
    "                             replace=False)\n",
    "idx_cls   = np.concatenate([pos_idx, neg_keep])        # indices into X_all / y_cls_all\n",
    "\n",
    "# 2. build the classification arrays\n",
    "X_cls_bal = X_all[idx_cls]\n",
    "y_cls_bal = y_cls_all[idx_cls]\n",
    "\n",
    "# 3. indices inside the balanced set that have a regression label\n",
    "idx_pos_in_bal = idx_cls[y_cls_bal == 1]               # original row numbers of positives\n",
    "\n",
    "# 4. convert those original row numbers to positions inside the regression subset\n",
    "#    (because X_reg / y_reg only contain the positive rows)\n",
    "orig_pos_rows   = np.where(y_cls_all == 1)[0]          # original indices of positives\n",
    "pos_in_reg      = np.searchsorted(orig_pos_rows, idx_pos_in_bal)\n",
    "\n",
    "X_reg_bal = X_reg[pos_in_reg]\n",
    "y_reg_bal = y_reg[pos_in_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd64131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_obj(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    n_cls = len(y_cls_bal)\n",
    "    n_reg = len(y_reg_bal)\n",
    "\n",
    "    p_hat  = predt[:n_cls]        # classification logits\n",
    "    mu_hat = predt[n_cls:]        # regression predictions\n",
    "\n",
    "    y_cls = y_cls_bal\n",
    "    y_reg = y_reg_bal\n",
    "\n",
    "    # classification\n",
    "    grad_cls = p_hat - y_cls\n",
    "    hess_cls = p_hat * (1. - p_hat)\n",
    "\n",
    "    # regression (log-ratio)\n",
    "    delta    = mu_hat - y_reg\n",
    "    grad_reg = delta\n",
    "    hess_reg = np.ones_like(delta)\n",
    "\n",
    "    grad = np.concatenate([grad_cls, grad_reg])\n",
    "    hess = np.concatenate([hess_cls, hess_reg])\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def custom_eval(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    n_cls = len(y_cls_bal)\n",
    "    n_reg = len(y_reg_bal)\n",
    "\n",
    "    p_hat  = predt[:n_cls]\n",
    "    mu_hat = predt[n_cls:]\n",
    "\n",
    "    # classification metric\n",
    "    logloss = -np.mean(\n",
    "        y_cls_bal * np.log(p_hat + 1e-12) +\n",
    "        (1 - y_cls_bal) * np.log(1 - p_hat + 1e-12)\n",
    "    )\n",
    "\n",
    "    # regression metric\n",
    "    log_mae = mean_absolute_error(y_reg_bal, mu_hat)\n",
    "\n",
    "    return [('logloss', logloss), ('log_mae', log_mae)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d949f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. balanced classification rows\n",
    "X_cls_bal = X_cls[idx_cls]\n",
    "y_cls_bal = y_cls[idx_cls]\n",
    "\n",
    "# 2. regression rows (positives only, already aligned to regression arrays)\n",
    "idx_pos_in_bal = idx_cls[y_cls[idx_cls] == 1]\n",
    "pos_in_reg     = np.searchsorted(np.where(y_cls_all == 1)[0], idx_pos_in_bal)\n",
    "\n",
    "X_reg_bal = X_reg[pos_in_reg]\n",
    "y_reg_bal = y_reg[pos_in_reg]\n",
    "\n",
    "# 3. concatenate features and labels\n",
    "X_train = np.concatenate([X_cls_bal, X_reg_bal], axis=0)\n",
    "y_train = np.concatenate([y_cls_bal, y_reg_bal])   # 1-D vector\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train.astype(np.float32), label=y_train.astype(np.float32))\n",
    "\n",
    "# 4. train\n",
    "params = dict(\n",
    "    max_depth=4,\n",
    "    eta=0.03,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_weight=3,\n",
    "    disable_default_eval_metric=1\n",
    ")\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=600,\n",
    "    obj=custom_obj,\n",
    "    custom_metric=custom_eval, \n",
    "    verbose_eval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p  = xgb.XGBRegressor(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=600,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_weight=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_mu = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=600,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_weight=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_p.fit(X_cls_bal, y_cls_bal)\n",
    "model_mu.fit(X_reg_bal, y_reg_bal)\n",
    "\n",
    "def predict_line(models, candles_df, lengths=(1,3,5)):\n",
    "    seq_df = candles_df.tail(max(lengths))\n",
    "    X_seq, _, _ = make_multi_sequences(seq_df, lengths)\n",
    "    p   = model_p.predict(X_seq[-1:])[0]\n",
    "    mu  = np.exp(model_mu.predict(X_seq[-1:])[0])\n",
    "    return None if p < 0.5 else mu\n",
    "\n",
    "coeff = predict_line([model_p, model_mu], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf96d40",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m coeff \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# <— use df, not df_c\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mpredict_line\u001b[0;34m(bst, candles_df, lengths)\u001b[0m\n\u001b[1;32m      3\u001b[0m dmat \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])              \u001b[38;5;66;03m# last sequence only\u001b[39;00m\n\u001b[1;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m bst\u001b[38;5;241m.\u001b[39mpredict(dmat)[\u001b[38;5;241m0\u001b[39m]             \u001b[38;5;66;03m# shape (2,)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m p, log_mu \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, pred[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m        \u001b[38;5;66;03m# no line\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "coeff = predict_line(bst, df)   # <— use df, not df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1602e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No time gaps found in the data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the same path as in your Flask app\n",
    "file_path = \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles_prop.csv\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'timestamp' column to actual datetime objects\n",
    "# Assuming the column is named 'timestamp' or 'date'. Adjust if needed.\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Sort by date just in case it's not ordered\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Calculate the difference in days between each consecutive row\n",
    "time_diffs = df['timestamp'].diff().dt.days\n",
    "\n",
    "# Find and print the locations where the gap is larger than 1 day\n",
    "gaps = df[time_diffs > 1]\n",
    "\n",
    "if not gaps.empty:\n",
    "    print(\"Found gaps in the data after the following dates:\")\n",
    "    # We print the row *before* the gap occurs\n",
    "    print(df.iloc[gaps.index - 1])\n",
    "else:\n",
    "    print(\"No time gaps found in the data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rllib2.43 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
