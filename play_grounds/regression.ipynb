{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "label = pd.read_csv(\"/home/iatell/projects/meta-learning/data/seq_line_labels.csv\")\n",
    "label[\"seq_len\"] = label[\"endIndex\"] - label[\"startIndex\"]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e027af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "high",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "volume",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "upper_shadow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "body",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lower_shadow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Candle_Color",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "upper_body_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lower_body_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "upper_lower_body_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "276b5b3c-e85b-4fb2-8da6-147dc065c148",
       "rows": [
        [
         "0",
         "2018-01-01",
         "13707.91",
         "13818.55",
         "12750.0",
         "13380.0",
         "8607.1564",
         "0.0760030099500053",
         "-0.2252544015971292",
         "0.4327720197804016",
         "1",
         "0.3374096550882847",
         "1.9212588820103085",
         "0.1756190476190467"
        ],
        [
         "1",
         "2018-01-02",
         "13382.16",
         "15473.49",
         "12890.02",
         "14675.11",
         "20078.1654",
         "0.5400711938112452",
         "0.8746274330998404",
         "0.3329124443526468",
         "2",
         "0.6174871418074936",
         "0.3806334351676392",
         "1.622261957979437"
        ],
        [
         "2",
         "2018-01-03",
         "14690.0",
         "15307.56",
         "14150.0",
         "14919.51",
         "15905.4821",
         "0.263643840163752",
         "0.1559306732534026",
         "0.3668797157284533",
         "2",
         "1.6907760010457011",
         "2.352838656267698",
         "0.7186111111111099"
        ],
        [
         "3",
         "2018-01-04",
         "14919.51",
         "15280.0",
         "13918.04",
         "15059.54",
         "25224.415",
         "0.1500060750668053",
         "0.0952796457026441",
         "0.6814233148741451",
         "2",
         "1.5743769192315795",
         "5.0",
         "0.2201363994927449"
        ],
        [
         "4",
         "2018-01-05",
         "15059.56",
         "17176.24",
         "14600.0",
         "16960.39",
         "23251.352",
         "0.144690479328261",
         "1.27418116201777",
         "0.3080563200375024",
         "2",
         "0.1135556572655114",
         "0.2417680697379563",
         "0.4696883975977073"
        ],
        [
         "5",
         "2018-01-06",
         "16960.39",
         "17143.13",
         "16011.21",
         "17069.79",
         "18571.4216",
         "0.0494002646223289",
         "0.0736895139035021",
         "0.639347466242461",
         "2",
         "0.6703839122486213",
         "5.0",
         "0.0772666933563708"
        ],
        [
         "6",
         "2018-01-07",
         "17069.79",
         "17099.96",
         "15610.0",
         "16150.03",
         "12493.3283",
         "0.0203204052621319",
         "-0.6194861101723416",
         "0.3637264982999586",
         "1",
         "0.0328020353135581",
         "0.5871422979907808",
         "0.0558672666333318"
        ],
        [
         "7",
         "2018-01-08",
         "16218.85",
         "16322.3",
         "12812.0",
         "14902.54",
         "26600.3888",
         "0.0678260053683906",
         "-0.863026091120997",
         "1.370642602830709",
         "1",
         "0.0785909094362262",
         "1.588182115155246",
         "0.0494848221033794"
        ],
        [
         "8",
         "2018-01-09",
         "14902.54",
         "15500.0",
         "14011.05",
         "14400.0",
         "14314.7761",
         "0.391905370614885",
         "-0.3296423609091905",
         "0.2551327183420815",
         "1",
         "1.188880487125399",
         "0.7739682413340233",
         "1.5360843296053426"
        ],
        [
         "9",
         "2018-01-10",
         "14401.0",
         "14955.66",
         "13131.31",
         "14907.09",
         "17411.0159",
         "0.0317347758076065",
         "0.3306702221221266",
         "0.8295929070446817",
         "2",
         "0.0959710723389114",
         "2.508822541445198",
         "0.0382534319400796"
        ],
        [
         "10",
         "2018-01-11",
         "14940.0",
         "14968.68",
         "11400.0",
         "13238.78",
         "33554.2284",
         "0.0182528519860017",
         "-1.0827097927345044",
         "1.1702572934037654",
         "1",
         "0.0168584897896805",
         "1.0808596183915082",
         "0.0155972982085949"
        ],
        [
         "11",
         "2018-01-12",
         "13238.76",
         "14109.78",
         "12500.0",
         "13740.01",
         "16417.08194",
         "0.235217906458799",
         "0.3188548979432428",
         "0.4699396397098256",
         "2",
         "0.7376957605985046",
         "1.473835411471322",
         "0.5005279116357143"
        ],
        [
         "12",
         "2018-01-13",
         "13749.95",
         "14580.0",
         "13706.15",
         "14210.0",
         "12221.5188",
         "0.2374735824307884",
         "0.2952695178304975",
         "0.0281117375958616",
         "2",
         "0.804260406477558",
         "0.0952070427127511",
         "5.0"
        ],
        [
         "13",
         "2018-01-14",
         "14210.0",
         "14339.5",
         "12569.2",
         "13474.99",
         "17017.6324",
         "0.0828899370954448",
         "-0.4704628004982467",
         "0.5797751051867408",
         "1",
         "0.176188079073754",
         "1.2323505802642123",
         "0.1429691208779078"
        ],
        [
         "14",
         "2018-01-15",
         "13477.98",
         "14249.99",
         "13147.79",
         "13539.93",
         "14652.0545",
         "0.4571857780873803",
         "0.0398876981558087",
         "0.2125991776281886",
         "2",
         "5.0",
         "5.0",
         "2.150458826736129"
        ],
        [
         "15",
         "2018-01-16",
         "13500.0",
         "13542.93",
         "9035.0",
         "10900.0",
         "63401.866",
         "0.026628097247025",
         "-1.612696315915788",
         "1.156799472762671",
         "1",
         "0.0165115384615385",
         "0.7173076923076923",
         "0.0230187667560323"
        ],
        [
         "16",
         "2018-01-17",
         "10899.99",
         "11680.99",
         "9037.94",
         "10988.79",
         "72330.098",
         "0.4239281967322686",
         "0.0543843164834238",
         "1.14038644716169",
         "2",
         "5.0",
         "5.0",
         "0.3717408232861626"
        ],
        [
         "17",
         "2018-01-18",
         "10972.59",
         "11878.82",
         "10435.33",
         "10961.97",
         "48464.707",
         "0.556297981065909",
         "-0.0065191889022879",
         "0.3232830172787815",
         "1",
         "5.0",
         "5.0",
         "1.7207770013671588"
        ],
        [
         "18",
         "2018-01-19",
         "10960.0",
         "11795.0",
         "10360.0",
         "11474.98",
         "34129.545",
         "0.1969164478876865",
         "0.3168802960227503",
         "0.3691952650853439",
         "2",
         "0.6214221911530559",
         "1.1650937900501002",
         "0.5333666666666673"
        ],
        [
         "19",
         "2018-01-20",
         "11474.98",
         "13099.0",
         "11412.45",
         "12799.94",
         "28768.4576",
         "0.1838802972519933",
         "0.814666082548658",
         "0.0384472513447701",
         "2",
         "0.2257124743388474",
         "0.0471938775510194",
         "4.7826643211259405"
        ],
        [
         "20",
         "2018-01-21",
         "12799.8",
         "12799.8",
         "10965.0",
         "11530.0",
         "41380.038",
         "0.0",
         "-0.778753121487601",
         "0.3465077284930657",
         "1",
         "0.0",
         "0.4449519609387308",
         "0.0"
        ],
        [
         "21",
         "2018-01-22",
         "11530.0",
         "11926.35",
         "9900.24",
         "10760.05",
         "43752.644",
         "0.2419030445007765",
         "-0.4699211533073619",
         "0.524765123482307",
         "1",
         "0.5147736866030262",
         "1.1167088771998166",
         "0.4609739361021628"
        ],
        [
         "22",
         "2018-01-23",
         "10760.05",
         "11399.0",
         "9905.0",
         "10799.18",
         "37474.2905",
         "0.3667329567204366",
         "0.0239242782776017",
         "0.5227818589640378",
         "2",
         "5.0",
         "5.0",
         "0.7015028360914569"
        ],
        [
         "23",
         "2018-01-24",
         "10799.14",
         "11570.48",
         "10500.0",
         "11349.99",
         "27158.7906",
         "0.1357467103297209",
         "0.3391359036016458",
         "0.1841683111616521",
         "2",
         "0.4002723064355081",
         "0.5430516474539334",
         "0.7370796282677015"
        ],
        [
         "24",
         "2018-01-25",
         "11349.96",
         "11794.05",
         "10950.21",
         "11175.27",
         "20840.207",
         "0.2760609679748225",
         "-0.1085930565775436",
         "0.1399047072719807",
         "1",
         "2.5421603984200782",
         "1.288339343980783",
         "1.973207144761386"
        ],
        [
         "25",
         "2018-01-26",
         "11184.7",
         "11643.0",
         "10311.15",
         "11089.0",
         "33056.907",
         "0.2858782400689901",
         "-0.0596957180331717",
         "0.4852070456854996",
         "1",
         "4.788923719958159",
         "5.0",
         "0.5891881468149374"
        ],
        [
         "26",
         "2018-01-27",
         "11089.0",
         "11650.0",
         "10842.69",
         "11491.0",
         "18860.9225",
         "0.1001755492535675",
         "0.253274030188265",
         "0.1551838964568941",
         "2",
         "0.3955223880597014",
         "0.6127114427860684",
         "0.6455279931793282"
        ],
        [
         "27",
         "2018-01-28",
         "11499.98",
         "12244.0",
         "11408.0",
         "11879.95",
         "16887.593",
         "0.2315560729827031",
         "0.2416820795254447",
         "0.0585044021231945",
         "2",
         "0.9581019554175282",
         "0.2420717425059854",
         "3.957925636007839"
        ],
        [
         "28",
         "2018-01-29",
         "11879.95",
         "11975.02",
         "11139.55",
         "11251.0",
         "14170.438",
         "0.0610418921628765",
         "-0.4038318930876343",
         "0.0715590499795167",
         "1",
         "0.1511566897209628",
         "0.1772000953970913",
         "0.8530282637954159"
        ],
        [
         "29",
         "2018-01-30",
         "11250.11",
         "11308.42",
         "9900.0",
         "10237.51",
         "25554.3345",
         "0.0375110737407957",
         "-0.6514099343153862",
         "0.2171216343381256",
         "1",
         "0.0575844361050755",
         "0.3333102903416947",
         "0.1727652513999569"
        ],
        [
         "30",
         "2018-01-31",
         "10230.0",
         "10425.85",
         "9700.0",
         "10285.1",
         "18015.6952",
         "0.0915207973272331",
         "0.0358280350460431",
         "0.3446253824755492",
         "2",
         "2.5544464609800195",
         "5.0",
         "0.265566037735849"
        ],
        [
         "31",
         "2018-02-01",
         "10285.1",
         "10335.0",
         "8750.99",
         "9224.52",
         "33564.9054",
         "0.0324273606349695",
         "-0.6892146321089428",
         "0.3077220056408268",
         "1",
         "0.0470497275075898",
         "0.4464821135605052",
         "0.1053787510822958"
        ],
        [
         "32",
         "2018-02-02",
         "9224.52",
         "9250.0",
         "8010.02",
         "8873.03",
         "49970.757",
         "0.01662266254104",
         "-0.2293053240404337",
         "0.5630111459789321",
         "1",
         "0.0724913937807607",
         "2.455290335429175",
         "0.0295245709783195"
        ],
        [
         "33",
         "2018-02-03",
         "8873.03",
         "9473.01",
         "8229.0",
         "9199.96",
         "28725.049",
         "0.1788064310925536",
         "0.2140896777772864",
         "0.4217421930655082",
         "2",
         "0.8351940782430561",
         "1.9699324014315105",
         "0.4239709330310712"
        ],
        [
         "34",
         "2018-02-04",
         "9199.96",
         "9368.0",
         "7930.0",
         "8184.81",
         "32014.443",
         "0.1101692897958844",
         "-0.6655460279474611",
         "0.1670568717739181",
         "1",
         "0.1655321873614747",
         "0.2510072403093146",
         "0.6594717632745991"
        ],
        [
         "35",
         "2018-02-05",
         "8179.99",
         "8382.8",
         "6625.0",
         "6939.99",
         "63402.168",
         "0.1325608261183738",
         "-0.8104897410718603",
         "0.2058840028550202",
         "1",
         "0.1635564516129028",
         "0.2540241935483869",
         "0.643861709895551"
        ],
        [
         "36",
         "2018-02-06",
         "6939.63",
         "7878.0",
         "6000.01",
         "7652.14",
         "100203.043",
         "0.1469581448945251",
         "0.4636020004374316",
         "0.6113734707597357",
         "2",
         "0.3169920422169508",
         "1.3187464035592478",
         "0.2403737681190265"
        ],
        [
         "37",
         "2018-02-07",
         "7655.02",
         "8476.0",
         "7150.01",
         "7599.0",
         "60777.498",
         "0.5356492803379135",
         "-0.036550309002083",
         "0.2929440064056612",
         "1",
         "5.0",
         "5.0",
         "1.8285039755896568"
        ],
        [
         "38",
         "2018-02-08",
         "7599.0",
         "8280.286",
         "7572.09",
         "8269.3955",
         "19947.321",
         "0.0071827961920388",
         "0.4421573155098645",
         "0.0177484087532961",
         "2",
         "0.0162448882786347",
         "0.0401404842365437",
         "0.4047008547008391"
        ],
        [
         "39",
         "2018-02-09",
         "8250.868",
         "8775.638",
         "8247.343",
         "8764.747",
         "31033.86",
         "0.0072779672878884",
         "0.3434023094235884",
         "0.0023555995491507",
         "2",
         "0.0211937051329232",
         "0.0068595914602457",
         "3.0896453900716496"
        ],
        [
         "40",
         "2018-02-10",
         "8720.666",
         "9065.78",
         "8120.0",
         "8533.98",
         "49473.936",
         "0.2323340969647822",
         "-0.1256788285203355",
         "0.2786953570747059",
         "1",
         "1.8486335343839493",
         "2.217520328251718",
         "0.8336489685492096"
        ],
        [
         "41",
         "2018-02-11",
         "8533.99",
         "8549.0",
         "7726.53",
         "8063.88",
         "47457.823",
         "0.0101958873475835",
         "-0.3193330180527918",
         "0.2291527379551797",
         "1",
         "0.0319286975388743",
         "0.7175980089766238",
         "0.0444938491181271"
        ],
        [
         "42",
         "2018-02-12",
         "8063.82",
         "8989.0",
         "8053.0",
         "8903.0",
         "41987.984",
         "0.058846111468448",
         "0.574214881652235",
         "0.0074036619312626",
         "2",
         "0.1024809933506517",
         "0.0128935389308607",
         "5.0"
        ],
        [
         "43",
         "2018-02-13",
         "8903.0",
         "8950.0",
         "8351.0",
         "8539.9",
         "35455.1325",
         "0.0325441902283817",
         "-0.2514211802537326",
         "0.1307999475349213",
         "1",
         "0.1294409253649131",
         "0.5202423574772775",
         "0.2488088935944949"
        ],
        [
         "44",
         "2018-02-14",
         "8535.17",
         "9489.6",
         "8533.0",
         "9449.99",
         "40812.275",
         "0.0276135947036951",
         "0.6377548272364056",
         "0.0015127871877561",
         "2",
         "0.043298135152271",
         "0.0023720513325026",
         "5.0"
        ],
        [
         "45",
         "2018-02-15",
         "9449.98",
         "10219.5",
         "9301.5",
         "10000.09",
         "52427.447",
         "0.1540681969828395",
         "0.386283468584978",
         "0.1042616375188549",
         "2",
         "0.3988475032266267",
         "0.2699096544327487",
         "1.477707435344831"
        ],
        [
         "46",
         "2018-02-16",
         "10000.89",
         "10323.37",
         "9666.0",
         "10159.98",
         "38161.297",
         "0.1159802105173432",
         "0.1129279129151356",
         "0.2377171962797767",
         "2",
         "1.0270287258784403",
         "2.105034885913628",
         "0.4878915464779525"
        ],
        [
         "47",
         "2018-02-17",
         "10156.07",
         "11075.07",
         "10050.0",
         "11039.55",
         "41882.537",
         "0.0253514964652467",
         "0.630561376607993",
         "0.0757047643600417",
         "2",
         "0.0402046452664468",
         "0.120059310906868",
         "0.3348731969454184"
        ],
        [
         "48",
         "2018-02-18",
         "11039.55",
         "11274.0",
         "10080.0",
         "10383.43",
         "61137.679",
         "0.1678288690562156",
         "-0.4696774474948333",
         "0.2172075655266683",
         "1",
         "0.3573279278180837",
         "0.4624611351582039",
         "0.77266585373892"
        ],
        [
         "49",
         "2018-02-19",
         "10375.01",
         "11250.0",
         "10270.33",
         "11153.0",
         "40831.479",
         "0.0698538791260975",
         "0.560264117745491",
         "0.0753845780094836",
         "2",
         "0.1246802658131852",
         "0.1345518579930337",
         "0.9266335498662563"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 1604
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>upper_shadow</th>\n",
       "      <th>body</th>\n",
       "      <th>lower_shadow</th>\n",
       "      <th>Candle_Color</th>\n",
       "      <th>upper_body_ratio</th>\n",
       "      <th>lower_body_ratio</th>\n",
       "      <th>upper_lower_body_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>13707.91</td>\n",
       "      <td>13818.55</td>\n",
       "      <td>12750.00</td>\n",
       "      <td>13380.00</td>\n",
       "      <td>8607.15640</td>\n",
       "      <td>0.076003</td>\n",
       "      <td>-0.225254</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>1</td>\n",
       "      <td>0.337410</td>\n",
       "      <td>1.921259</td>\n",
       "      <td>0.175619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>13382.16</td>\n",
       "      <td>15473.49</td>\n",
       "      <td>12890.02</td>\n",
       "      <td>14675.11</td>\n",
       "      <td>20078.16540</td>\n",
       "      <td>0.540071</td>\n",
       "      <td>0.874627</td>\n",
       "      <td>0.332912</td>\n",
       "      <td>2</td>\n",
       "      <td>0.617487</td>\n",
       "      <td>0.380633</td>\n",
       "      <td>1.622262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>14690.00</td>\n",
       "      <td>15307.56</td>\n",
       "      <td>14150.00</td>\n",
       "      <td>14919.51</td>\n",
       "      <td>15905.48210</td>\n",
       "      <td>0.263644</td>\n",
       "      <td>0.155931</td>\n",
       "      <td>0.366880</td>\n",
       "      <td>2</td>\n",
       "      <td>1.690776</td>\n",
       "      <td>2.352839</td>\n",
       "      <td>0.718611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>14919.51</td>\n",
       "      <td>15280.00</td>\n",
       "      <td>13918.04</td>\n",
       "      <td>15059.54</td>\n",
       "      <td>25224.41500</td>\n",
       "      <td>0.150006</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>0.681423</td>\n",
       "      <td>2</td>\n",
       "      <td>1.574377</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.220136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>15059.56</td>\n",
       "      <td>17176.24</td>\n",
       "      <td>14600.00</td>\n",
       "      <td>16960.39</td>\n",
       "      <td>23251.35200</td>\n",
       "      <td>0.144690</td>\n",
       "      <td>1.274181</td>\n",
       "      <td>0.308056</td>\n",
       "      <td>2</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>0.241768</td>\n",
       "      <td>0.469688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>28715.33</td>\n",
       "      <td>30545.18</td>\n",
       "      <td>28691.38</td>\n",
       "      <td>30319.23</td>\n",
       "      <td>67877.36415</td>\n",
       "      <td>0.109006</td>\n",
       "      <td>0.773779</td>\n",
       "      <td>0.011554</td>\n",
       "      <td>2</td>\n",
       "      <td>0.140875</td>\n",
       "      <td>0.014932</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>2022-05-20</td>\n",
       "      <td>30319.22</td>\n",
       "      <td>30777.33</td>\n",
       "      <td>28730.00</td>\n",
       "      <td>29201.01</td>\n",
       "      <td>60517.25325</td>\n",
       "      <td>0.221063</td>\n",
       "      <td>-0.539597</td>\n",
       "      <td>0.227288</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409682</td>\n",
       "      <td>0.421218</td>\n",
       "      <td>0.972612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>29201.01</td>\n",
       "      <td>29656.18</td>\n",
       "      <td>28947.28</td>\n",
       "      <td>29445.06</td>\n",
       "      <td>20987.13124</td>\n",
       "      <td>0.103235</td>\n",
       "      <td>0.119338</td>\n",
       "      <td>0.124071</td>\n",
       "      <td>2</td>\n",
       "      <td>0.865069</td>\n",
       "      <td>1.039664</td>\n",
       "      <td>0.832066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>2022-05-22</td>\n",
       "      <td>29445.07</td>\n",
       "      <td>30487.99</td>\n",
       "      <td>29255.11</td>\n",
       "      <td>30293.94</td>\n",
       "      <td>36158.98748</td>\n",
       "      <td>0.095648</td>\n",
       "      <td>0.418411</td>\n",
       "      <td>0.093632</td>\n",
       "      <td>2</td>\n",
       "      <td>0.228598</td>\n",
       "      <td>0.223780</td>\n",
       "      <td>1.021531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>30293.93</td>\n",
       "      <td>30670.51</td>\n",
       "      <td>30048.77</td>\n",
       "      <td>30472.79</td>\n",
       "      <td>20776.30953</td>\n",
       "      <td>0.098828</td>\n",
       "      <td>0.089401</td>\n",
       "      <td>0.122540</td>\n",
       "      <td>2</td>\n",
       "      <td>1.105446</td>\n",
       "      <td>1.370681</td>\n",
       "      <td>0.806494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1604 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp      open      high       low     close       volume  \\\n",
       "0     2018-01-01  13707.91  13818.55  12750.00  13380.00   8607.15640   \n",
       "1     2018-01-02  13382.16  15473.49  12890.02  14675.11  20078.16540   \n",
       "2     2018-01-03  14690.00  15307.56  14150.00  14919.51  15905.48210   \n",
       "3     2018-01-04  14919.51  15280.00  13918.04  15059.54  25224.41500   \n",
       "4     2018-01-05  15059.56  17176.24  14600.00  16960.39  23251.35200   \n",
       "...          ...       ...       ...       ...       ...          ...   \n",
       "1599  2022-05-19  28715.33  30545.18  28691.38  30319.23  67877.36415   \n",
       "1600  2022-05-20  30319.22  30777.33  28730.00  29201.01  60517.25325   \n",
       "1601  2022-05-21  29201.01  29656.18  28947.28  29445.06  20987.13124   \n",
       "1602  2022-05-22  29445.07  30487.99  29255.11  30293.94  36158.98748   \n",
       "1603  2022-05-23  30293.93  30670.51  30048.77  30472.79  20776.30953   \n",
       "\n",
       "      upper_shadow      body  lower_shadow  Candle_Color  upper_body_ratio  \\\n",
       "0         0.076003 -0.225254      0.432772             1          0.337410   \n",
       "1         0.540071  0.874627      0.332912             2          0.617487   \n",
       "2         0.263644  0.155931      0.366880             2          1.690776   \n",
       "3         0.150006  0.095280      0.681423             2          1.574377   \n",
       "4         0.144690  1.274181      0.308056             2          0.113556   \n",
       "...            ...       ...           ...           ...               ...   \n",
       "1599      0.109006  0.773779      0.011554             2          0.140875   \n",
       "1600      0.221063 -0.539597      0.227288             1          0.409682   \n",
       "1601      0.103235  0.119338      0.124071             2          0.865069   \n",
       "1602      0.095648  0.418411      0.093632             2          0.228598   \n",
       "1603      0.098828  0.089401      0.122540             2          1.105446   \n",
       "\n",
       "      lower_body_ratio  upper_lower_body_ratio  \n",
       "0             1.921259                0.175619  \n",
       "1             0.380633                1.622262  \n",
       "2             2.352839                0.718611  \n",
       "3             5.000000                0.220136  \n",
       "4             0.241768                0.469688  \n",
       "...                ...                     ...  \n",
       "1599          0.014932                5.000000  \n",
       "1600          0.421218                0.972612  \n",
       "1601          1.039664                0.832066  \n",
       "1602          0.223780                1.021531  \n",
       "1603          1.370681                0.806494  \n",
       "\n",
       "[1604 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles_prop.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b8d98",
   "metadata": {},
   "source": [
    "# model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b75a48",
   "metadata": {},
   "source": [
    "## FNNCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c70fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    \"\"\"\n",
    "    raw_params: (B, 3K) tensor from mdn_head\n",
    "    returns:\n",
    "        pi    (B, K) mixture weights\n",
    "        mu    (B, K) means\n",
    "        sigma (B, K) std devs\n",
    "    \"\"\"\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "\n",
    "    pi = raw[..., 0]                 # (B,K)\n",
    "    mu = raw[..., 1]                 # (B,K)\n",
    "    sigma = raw[..., 2]              # (B,K)\n",
    "\n",
    "    pi = F.softmax(pi, dim=-1)       # weights sum to 1\n",
    "    sigma = F.softplus(sigma) + 1e-4 # strictly positive\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def mdn_nll_multitarget(y_line, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for MDN with multiple valid targets per sample.\n",
    "    Args:\n",
    "        y_line : (B, L) padded targets (0 where invalid)\n",
    "        pi, mu, sigma : (B, K) MDN params\n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    B, K = mu.shape\n",
    "    losses = []\n",
    "\n",
    "    for b in range(B):\n",
    "        valid_y = y_line[b][y_line[b] > 0]  # (M,)\n",
    "        if len(valid_y) == 0:\n",
    "            continue\n",
    "\n",
    "        # expand to (M, K)\n",
    "        y_exp = valid_y.unsqueeze(-1).expand(-1, K)\n",
    "\n",
    "        log_prob = -0.5 * ((y_exp - mu[b]) / (sigma[b] + 1e-8))**2 \\\n",
    "                   - torch.log(sigma[b] + 1e-8) \\\n",
    "                   - 0.5 * torch.log(torch.tensor(2.0 * torch.pi, device=y_line.device))\n",
    "\n",
    "        log_mix = torch.log(pi[b] + 1e-8) + log_prob\n",
    "        log_sum = torch.logsumexp(log_mix, dim=-1)  # (M,)\n",
    "\n",
    "        losses.append(-log_sum.mean())\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=y_line.device, requires_grad=True)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "class CNNLSTM_MDN(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, hidden_features=64, out_features=32,\n",
    "                 lr=1e-3, n_components=5, cnn_channels=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Time-distributed feature extractor\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_features)\n",
    "        self.ln1 = nn.LayerNorm(hidden_features) # ADDED: LayerNorm for time-step features\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.ln2 = nn.LayerNorm(out_features) # ADDED: LayerNorm\n",
    "\n",
    "        # CNN feature extractors\n",
    "        self.conv1 = nn.Conv1d(out_features, cnn_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm for convolutional features\n",
    "        self.conv3 = nn.Conv1d(out_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm\n",
    "\n",
    "        # Learnable mixer for CNN outputs\n",
    "        self.mixer = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1, bias=True)\n",
    "\n",
    "        # LSTM for temporal dependency\n",
    "        fused_dim = cnn_channels # Input to LSTM is the mixed CNN output\n",
    "        self.lstm = nn.LSTM(fused_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # MDN Head\n",
    "        self.mdn_head = nn.Linear(hidden_dim, 3 * n_components)\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        # Input shape X[\"main\"]: (B, T, F_in)\n",
    "        x = X[\"main\"] # REMOVED redundant transposes\n",
    "\n",
    "        # 1. Time-distributed feature extraction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.ln1(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.ln2(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "\n",
    "        # 2. CNN feature extraction\n",
    "        x = x.transpose(1, 2)   # Shape: (B, C_in, L=T)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "        x3 = F.relu(self.bn3(self.conv3(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "\n",
    "        # 3. Mix CNN outputs\n",
    "        paired = torch.stack([x1, x3], dim=1) # Shape: (B, 2, C_out, L)\n",
    "        mixed = self.mixer(paired)            # Shape: (B, 1, C_out, L)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        xf = mixed.squeeze(1).transpose(1, 2) # Shape: (B, L, C_out)\n",
    "\n",
    "        # 4. LSTM for sequence summary\n",
    "        if lengths is not None:\n",
    "            packed_input = pack_padded_sequence(\n",
    "                xf, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            _, (h_last, _) = self.lstm(packed_input)\n",
    "        else:\n",
    "            _, (h_last, _) = self.lstm(xf)\n",
    "        \n",
    "        last_h = h_last[-1] # Shape: (B, H)\n",
    "        \n",
    "        # 5. MDN head for distribution parameters\n",
    "        raw = self.mdn_head(last_h)\n",
    "        pi, mu, sigma = mdn_split_params(raw, self.n_components)\n",
    "        return {\"pi\": pi, \"mu\": mu, \"sigma\": sigma}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "    # Log everything to progress bar\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        self.log(\"val/pi_mean\", mdn[\"pi\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/pi_std\", mdn[\"pi\"].std(), prog_bar=True)\n",
    "        self.log(\"val/mu_mean\", mdn[\"mu\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/mu_std\", mdn[\"mu\"].std(), prog_bar=True)\n",
    "        self.log(\"val/sigma_mean\", mdn[\"sigma\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/sigma_std\", mdn[\"sigma\"].std(), prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self): \n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #         optimizer,\n",
    "    #         mode=\"min\",\n",
    "    #         factor=0.2,   # Reduce LR by 80%\n",
    "    #         patience=5,   # After 5 epochs of no val_loss improvement\n",
    "    #         verbose=True\n",
    "    #     )\n",
    "    #     return {\n",
    "    #         \"optimizer\": optimizer,\n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": scheduler,\n",
    "    #             \"monitor\": \"val/loss\",  # Important!\n",
    "    #         },\n",
    "        # }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6484b7d",
   "metadata": {},
   "source": [
    "## CNNlSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2139f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    \"\"\"\n",
    "    raw_params: (B, 3K) tensor from mdn_head\n",
    "    returns:\n",
    "        pi    (B, K) mixture weights\n",
    "        mu    (B, K) means\n",
    "        sigma (B, K) std devs\n",
    "    \"\"\"\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "\n",
    "    pi = raw[..., 0]                 # (B,K)\n",
    "    mu = raw[..., 1]                 # (B,K)\n",
    "    sigma = raw[..., 2]              # (B,K)\n",
    "\n",
    "    pi = F.softmax(pi, dim=-1)       # weights sum to 1\n",
    "    sigma = F.softplus(sigma) + 1e-4 # strictly positive\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def mdn_nll_multitarget(y_line, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for MDN with multiple valid targets per sample.\n",
    "    Args:\n",
    "        y_line : (B, L) padded targets (0 where invalid)\n",
    "        pi, mu, sigma : (B, K) MDN params\n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    B, K = mu.shape\n",
    "    losses = []\n",
    "\n",
    "    for b in range(B):\n",
    "        valid_y = y_line[b][y_line[b] > 0]  # (M,)\n",
    "        if len(valid_y) == 0:\n",
    "            continue\n",
    "\n",
    "        # expand to (M, K)\n",
    "        y_exp = valid_y.unsqueeze(-1).expand(-1, K)\n",
    "\n",
    "        log_prob = -0.5 * ((y_exp - mu[b]) / (sigma[b] + 1e-8))**2 \\\n",
    "                   - torch.log(sigma[b] + 1e-8) \\\n",
    "                   - 0.5 * torch.log(torch.tensor(2.0 * torch.pi, device=y_line.device))\n",
    "\n",
    "        log_mix = torch.log(pi[b] + 1e-8) + log_prob\n",
    "        log_sum = torch.logsumexp(log_mix, dim=-1)  # (M,)\n",
    "\n",
    "        losses.append(-log_sum.mean())\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=y_line.device, requires_grad=True)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "class CNNLSTM_MDN(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, hidden_features=64, out_features=32,\n",
    "                 lr=1e-3, n_components=5, cnn_channels=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Time-distributed feature extractor\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_features)\n",
    "        self.ln1 = nn.LayerNorm(hidden_features) # ADDED: LayerNorm for time-step features\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.ln2 = nn.LayerNorm(out_features) # ADDED: LayerNorm\n",
    "\n",
    "        # CNN feature extractors\n",
    "        self.conv1 = nn.Conv1d(out_features, cnn_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm for convolutional features\n",
    "        self.conv3 = nn.Conv1d(out_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm\n",
    "\n",
    "        # Learnable mixer for CNN outputs\n",
    "        self.mixer = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1, bias=True)\n",
    "\n",
    "        # LSTM for temporal dependency\n",
    "        fused_dim = cnn_channels # Input to LSTM is the mixed CNN output\n",
    "        self.lstm = nn.LSTM(fused_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # MDN Head\n",
    "        self.mdn_head = nn.Linear(hidden_dim, 3 * n_components)\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        # Input shape X[\"main\"]: (B, T, F_in)\n",
    "        x = X[\"main\"] # REMOVED redundant transposes\n",
    "\n",
    "        # 1. Time-distributed feature extraction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.ln1(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.ln2(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "        # 2. CNN feature extraction\n",
    "        x = x.transpose(1, 2)   # Shape: (B, C_in, L=T)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "        x3 = F.relu(self.bn3(self.conv3(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "\n",
    "        # 3. Mix CNN outputs\n",
    "        paired = torch.stack([x1, x3], dim=1) # Shape: (B, 2, C_out, L)\n",
    "        mixed = self.mixer(paired)            # Shape: (B, 1, C_out, L)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        xf = mixed.squeeze(1).transpose(1, 2) # Shape: (B, L, C_out)\n",
    "\n",
    "        # 4. LSTM for sequence summary\n",
    "        if lengths is not None:\n",
    "            packed_input = pack_padded_sequence(\n",
    "                xf, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            _, (h_last, _) = self.lstm(packed_input)\n",
    "        else:\n",
    "            _, (h_last, _) = self.lstm(xf)\n",
    "        \n",
    "        last_h = h_last[-1] # Shape: (B, H)\n",
    "        \n",
    "        # 5. MDN head for distribution parameters\n",
    "        raw = self.mdn_head(last_h)\n",
    "        pi, mu, sigma = mdn_split_params(raw, self.n_components)\n",
    "        return {\"pi\": pi, \"mu\": mu, \"sigma\": sigma}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "    # Log everything to progress bar\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        self.log(\"val/pi_mean\", mdn[\"pi\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/pi_std\", mdn[\"pi\"].std(), prog_bar=True)\n",
    "        self.log(\"val/mu_mean\", mdn[\"mu\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/mu_std\", mdn[\"mu\"].std(), prog_bar=True)\n",
    "        self.log(\"val/sigma_mean\", mdn[\"sigma\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/sigma_std\", mdn[\"sigma\"].std(), prog_bar=True)\n",
    "        \n",
    "    # Inside your CNNLSTM_MDN class\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "    #     # Define the scheduler\n",
    "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #         optimizer,\n",
    "    #         mode='min',      # We want to minimize the validation loss\n",
    "    #         factor=0.5,      # Reduce LR by 50% (1.0 -> 0.2)\n",
    "    #         patience=10,      # Wait 5 validation epochs with no improvement before reducing\n",
    "    #         verbose=True\n",
    "    #     )\n",
    "        \n",
    "    #     return {\n",
    "    #         \"optimizer\": optimizer,\n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": scheduler,\n",
    "    #             \"monitor\": \"val/loss\",  # The metric to watch\n",
    "    #         },\n",
    "    #     }\n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #         optimizer,\n",
    "    #         mode=\"min\",\n",
    "    #         factor=0.2,   # Reduce LR by 80%\n",
    "    #         patience=5,   # After 5 epochs of no val_loss improvement\n",
    "    #         verbose=True\n",
    "    #     )\n",
    "    #     return {\n",
    "    #         \"optimizer\": optimizer,\n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": scheduler,\n",
    "    #             \"monitor\": \"val/loss\",  # Important!\n",
    "    #         },\n",
    "        # }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e8361",
   "metadata": {},
   "source": [
    "## CNNLSTM scalie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a359387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def mdn_split_params(raw_params, n_components, mu_scale=10, mu_bias=.9, sigma_scale=10.0):\n",
    "    \"\"\"\n",
    "    Split raw MDN parameters into (pi, mu, sigma).\n",
    "\n",
    "    Args:\n",
    "        raw_params: (B, 3 * K) from the network\n",
    "        n_components: number of mixture components\n",
    "        mu_scale: scaling factor for mu (default 1.0 = no scaling)\n",
    "        mu_bias: shift/bias applied after scaling\n",
    "        sigma_scale: scaling factor for sigma (default 10.0)\n",
    "    \"\"\"\n",
    "    B = raw_params.size(0)\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "\n",
    "    pi_raw = raw[..., 0]\n",
    "    mu_raw = raw[..., 1]\n",
    "    sigma_raw = raw[..., 2]\n",
    "\n",
    "    pi = F.softmax(pi_raw, dim=-1)\n",
    "    mu = mu_raw / mu_scale + mu_bias\n",
    "    sigma = F.softplus(sigma_raw / sigma_scale) + 1e-4\n",
    "\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def mdn_nll_multitarget(y_line, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for MDN with multiple valid targets per sample.\n",
    "    Args:\n",
    "        y_line : (B, L) padded targets (0 where invalid)\n",
    "        pi, mu, sigma : (B, K) MDN params\n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    B, K = mu.shape\n",
    "    losses = []\n",
    "\n",
    "    for b in range(B):\n",
    "        valid_y = y_line[b][y_line[b] > 0]  # (M,)\n",
    "        if len(valid_y) == 0:\n",
    "            continue\n",
    "\n",
    "        # expand to (M, K)\n",
    "        y_exp = valid_y.unsqueeze(-1).expand(-1, K)\n",
    "\n",
    "        log_prob = -0.5 * ((y_exp - mu[b]) / (sigma[b] + 1e-8))**2 \\\n",
    "                   - torch.log(sigma[b] + 1e-8) \\\n",
    "                   - 0.5 * torch.log(torch.tensor(2.0 * torch.pi, device=y_line.device))\n",
    "\n",
    "        log_mix = torch.log(pi[b] + 1e-8) + log_prob\n",
    "        log_sum = torch.logsumexp(log_mix, dim=-1)  # (M,)\n",
    "\n",
    "        losses.append(-log_sum.mean())\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=y_line.device, requires_grad=True)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "class CNNLSTM_MDN(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, hidden_features=64, out_features=32,\n",
    "                 lr=1e-3, n_components=5, cnn_channels=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Time-distributed feature extractor\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_features)\n",
    "        self.ln1 = nn.LayerNorm(hidden_features) # ADDED: LayerNorm for time-step features\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.ln2 = nn.LayerNorm(out_features) # ADDED: LayerNorm\n",
    "\n",
    "        # CNN feature extractors\n",
    "        self.conv1 = nn.Conv1d(out_features, cnn_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm for convolutional features\n",
    "        self.conv3 = nn.Conv1d(out_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm\n",
    "\n",
    "        # Learnable mixer for CNN outputs\n",
    "        self.mixer = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1, bias=True)\n",
    "\n",
    "        # LSTM for temporal dependency\n",
    "        fused_dim = cnn_channels # Input to LSTM is the mixed CNN output\n",
    "        self.lstm = nn.LSTM(fused_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # MDN Head\n",
    "        self.mdn_head = nn.Linear(hidden_dim, 3 * n_components)\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        # Input shape X[\"main\"]: (B, T, F_in)\n",
    "        x = X[\"main\"] # REMOVED redundant transposes\n",
    "\n",
    "        # --- Debug print first candle ---\n",
    "        # if x.ndim == 3:  # batched: (B, T, F)\n",
    "        #     first_candle = x[0, 0, :]   # first sample, first time step, all features\n",
    "        #     print(\"First candle features:\", first_candle.detach().cpu().numpy())\n",
    "        # elif x.ndim == 2:  # single sequence: (T, F)\n",
    "        #     first_candle = x[0, :]      # first time step, all features\n",
    "        #     print(\"First candle features:\", first_candle.detach().cpu().numpy())\n",
    "        # else:\n",
    "        #     print(\"Unexpected shape for x:\", x.shape)\n",
    "        # 1. Time-distributed feature extraction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.ln1(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.ln2(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "\n",
    "        # 2. CNN feature extraction\n",
    "        x = x.transpose(1, 2)   # Shape: (B, C_in, L=T)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "        x3 = F.relu(self.bn3(self.conv3(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "\n",
    "        # 3. Mix CNN outputs\n",
    "        paired = torch.stack([x1, x3], dim=1) # Shape: (B, 2, C_out, L)\n",
    "        mixed = self.mixer(paired)            # Shape: (B, 1, C_out, L)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        xf = mixed.squeeze(1).transpose(1, 2) # Shape: (B, L, C_out)\n",
    "\n",
    "        # 4. LSTM for sequence summary\n",
    "        if lengths is not None:\n",
    "            packed_input = pack_padded_sequence(\n",
    "                xf, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            _, (h_last, _) = self.lstm(packed_input)\n",
    "        else:\n",
    "            _, (h_last, _) = self.lstm(xf)\n",
    "        \n",
    "        last_h = h_last[-1] # Shape: (B, H)\n",
    "        \n",
    "        # 5. MDN head for distribution parameters\n",
    "        raw = self.mdn_head(last_h)\n",
    "        pi, mu, sigma = mdn_split_params(raw, self.n_components)\n",
    "        return {\"pi\": pi, \"mu\": mu, \"sigma\": sigma}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "    # Log everything to progress bar\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        self.log(\"val/pi_mean\", mdn[\"pi\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/pi_std\", mdn[\"pi\"].std(), prog_bar=True)\n",
    "        self.log(\"val/mu_mean\", mdn[\"mu\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/mu_std\", mdn[\"mu\"].std(), prog_bar=True)\n",
    "        self.log(\"val/sigma_mean\", mdn[\"sigma\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/sigma_std\", mdn[\"sigma\"].std(), prog_bar=True)\n",
    "        \n",
    "    # # Inside your CNNLSTM_MDN class\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "    #     # Define the scheduler\n",
    "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #         optimizer,\n",
    "    #         mode='min',      # We want to minimize the validation loss\n",
    "    #         factor=0.5,      # Reduce LR by 80% (1.0 -> 0.2)\n",
    "    #         patience=10,      # Wait 5 validation epochs with no improvement before reducing\n",
    "    #         verbose=True\n",
    "    #     )\n",
    "        \n",
    "    #     return {\n",
    "    #         \"optimizer\": optimizer,\n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": scheduler,\n",
    "    #             \"monitor\": \"val/loss\",  # The metric to watch\n",
    "    #         },\n",
    "    #     }\n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4dd53",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e281d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name     | Type        | Params | Mode \n",
      "--------------------------------------------------\n",
      "0  | fc1      | Linear      | 1.0 K  | train\n",
      "1  | ln1      | LayerNorm   | 128    | train\n",
      "2  | fc2      | Linear      | 2.1 K  | train\n",
      "3  | ln2      | LayerNorm   | 64     | train\n",
      "4  | conv1    | Conv1d      | 2.1 K  | train\n",
      "5  | bn1      | BatchNorm1d | 128    | train\n",
      "6  | conv3    | Conv1d      | 6.2 K  | train\n",
      "7  | bn3      | BatchNorm1d | 128    | train\n",
      "8  | mixer    | Conv2d      | 3      | train\n",
      "9  | lstm     | LSTM        | 212 K  | train\n",
      "10 | mdn_head | Linear      | 3.0 K  | train\n",
      "--------------------------------------------------\n",
      "227 K     Trainable params\n",
      "0         Non-trainable params\n",
      "227 K     Total params\n",
      "0.911     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Debug batch:\n",
      "  Keys in X_batch: ['main']\n",
      "  y_batch shape: torch.Size([32, 6])\n",
      "  First label in batch: tensor([1.0157, 1.0583, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "Feature group: main\n",
      "  X_batch shape: torch.Size([32, 56, 15])\n",
      "  First sequence in batch (first  steps):\n",
      " tensor([[ 1.3218,  0.0086, -0.8959,  0.9549,  2.4932, -0.3550,  2.5471,  1.1624,\n",
      "          1.2290,  1.1575,  1.1680,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4148, -0.8214, -0.4183, -1.0472, -0.6969, -0.7031, -0.5979,  1.1674,\n",
      "          1.1787,  1.0636,  1.0884,  0.0043, -0.0409, -0.0811, -0.0682],\n",
      "        [ 1.0012, -0.6900, -0.6813, -1.0472, -0.2742, -0.7686,  1.6776,  1.0885,\n",
      "          1.1401,  1.0093,  1.0230, -0.0675, -0.0328, -0.0511, -0.0600],\n",
      "        [ 0.3101,  0.2795, -0.3933,  0.9549, -0.1585, -0.4093, -0.0576,  1.0235,\n",
      "          1.0881,  0.9979,  1.0564, -0.0597, -0.0456, -0.0112,  0.0326],\n",
      "        [ 0.5436, -0.4985, -0.1458, -1.0472, -0.2407, -0.4049, -0.1756,  1.0564,\n",
      "          1.0946,  0.9752,  1.0109,  0.0321,  0.0059, -0.0228, -0.0431],\n",
      "        [-0.2418, -0.1558, -0.2055, -1.0472,  0.1671,  1.0079, -0.5804,  1.0109,\n",
      "          1.0268,  0.9671,  1.0000, -0.0431, -0.0619, -0.0083, -0.0108],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "\n",
      "âœ… Combined df_seq shape: (1792, 15)\n",
      "âœ… Column names in df_seq: ['upper_shadow', 'body', 'lower_shadow', 'Candle_Color', 'upper_body_ratio', 'lower_body_ratio', 'upper_lower_body_ratio', 'open_prop', 'high_prop', 'low_prop', 'close_prop', 'open_dif', 'high_dif', 'low_dif', 'close_dif']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e95cf5d8364188aca0257598f32e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First candle features: [ 2.4509761   0.06757022 -0.425552    0.9548821   2.4932256   0.43385926\n",
      "  1.712948    1.6238451   1.6838083   1.609801    1.6305033   0.\n",
      "  0.          0.          0.        ]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ef6abf212047ef91eb4ef76e480456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First candle features: [ 0.34788373  0.2923229  -0.8116072   0.9548821  -0.16202562 -0.7484566\n",
      "  1.825419    1.502845    1.6000752   1.4908248   1.5525004   0.\n",
      "  0.          0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First candle features: [ 0.34788373  0.2923229  -0.8116072   0.9548821  -0.16202562 -0.7484566\n",
      "  1.825419    1.502845    1.6000752   1.4908248   1.5525004   0.\n",
      "  0.          0.          0.        ]\n",
      "\n",
      "ðŸ“Š Validation Metrics (MDN, last-output):\n",
      "  Regression â†’ MSE: 0.060597, MAE: 0.191716\n",
      "  Validity   â†’ Acc: 1.0000, F1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from preprocess.multi_regression_seq_dif import preprocess_sequences_csv_multilines\n",
    "# from models.LSTM.lstm_multi_line_reg_seq_dif import LSTMMultiRegressor\n",
    "from utils.make_step import make_step\n",
    "from utils.padding_batch_reg import collate_batch\n",
    "from utils.get_init_argumens import get_init_args\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "from add_ons.drop_column import drop_columns\n",
    "from add_ons.normalize_candle_seq import add_label_normalized_candles\n",
    "from add_ons.feature_pipeline3 import FeaturePipeline\n",
    "from add_ons.candle_dif_seq import add_candle_differences\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# ---------------- Evaluation ---------------- #\n",
    "@torch.no_grad()\n",
    "def evaluate_model_mdn(model, val_loader, zero_idx=0, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate CNNâ€“LSTMâ€“MDN model (last-output version).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    model : pl.LightningModule with MDN forward\n",
    "    val_loader : DataLoader yielding (X, y, lengths)\n",
    "    zero_idx : which mixture component is considered \"no-line\" (usually 0)\n",
    "    threshold : if pi[:,zero_idx] > threshold â†’ predict invalid\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with mse, mae, acc, f1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_reg, all_labels_reg = [], []\n",
    "    all_preds_len, all_labels_len = [], []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in val_loader:\n",
    "            if isinstance(X_batch, dict):\n",
    "                X_batch = {k: v.to(device) for k, v in X_batch.items()}\n",
    "            else:\n",
    "                X_batch = X_batch.to(device)\n",
    "\n",
    "            y_batch = y_batch.to(device)\n",
    "            mdn = model(X_batch, lengths)\n",
    "            pi, mu, sigma = mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"]  # (B,K)\n",
    "\n",
    "            # regression expectation\n",
    "            y_pred = (pi * mu).sum(dim=-1)  # (B,)\n",
    "            B = y_batch.size(0)\n",
    "            y_len = (y_batch > 0).sum(dim=1)                # (B,)\n",
    "            idx = torch.clamp(y_len - 1, min=0)             # last valid index\n",
    "            y_true = y_batch[torch.arange(B, device=y_batch.device), idx]  # (B,)\n",
    "            # only last step\n",
    "            # print(\"lengths(features):\", lengths[:10])\n",
    "            # print(\"lengths(labels):\", y_len[:10])\n",
    "\n",
    "            all_preds_reg.append(y_pred.cpu().numpy())\n",
    "            all_labels_reg.append(y_true.cpu().numpy())\n",
    "\n",
    "            # validity classification\n",
    "            pi_zero = pi[:, zero_idx]  # (B,)\n",
    "            pred_valid = (pi_zero < (1 - threshold)).long()\n",
    "            true_valid = torch.ones_like(pred_valid)  # last step always valid\n",
    "\n",
    "            all_preds_len.extend(pred_valid.cpu().numpy().tolist())\n",
    "            all_labels_len.extend(true_valid.cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "        # ----- Regression metrics -----\n",
    "    all_preds_reg = np.concatenate(all_preds_reg)  # (N,)\n",
    "    all_labels_reg = np.concatenate(all_labels_reg)\n",
    "    mse = ((all_preds_reg - all_labels_reg) ** 2).mean()\n",
    "    mae = np.abs(all_preds_reg - all_labels_reg).mean()\n",
    "    # ----- Validity metrics -----\n",
    "    acc = accuracy_score(all_labels_len, all_preds_len)\n",
    "    f1 = f1_score(all_labels_len, all_preds_len, average=\"macro\")\n",
    "\n",
    "    print(\"\\nðŸ“Š Validation Metrics (MDN, last-output):\")\n",
    "    print(f\"  Regression â†’ MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "    print(f\"  Validity   â†’ Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=True,\n",
    "    hidden_dim=200,\n",
    "    num_layers=1,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    max_epochs=1000,\n",
    "    save_model=False,\n",
    "    return_val_accuracy = True,\n",
    "    test_mode = False\n",
    "):\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/lstm_model_multireg_{timestamp}.pt\"\n",
    "    meta_out  = f\"{model_out_dir}/lstm_meta_multireg_{timestamp}.pkl\"\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            make_step(add_label_normalized_candles),\n",
    "            make_step(add_candle_differences),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "            \n",
    "        ],\n",
    "        norm_methods={\n",
    "            \"main\": {\n",
    "                \"upper_shadow\": \"robust\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "                \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "                \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "            }\n",
    "        },\n",
    "        per_window_flags=[True, True, True]\n",
    "    )\n",
    "    # Preprocess: pad linePrices and sequences\n",
    "    if do_validation:\n",
    "        train_ds, val_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline\n",
    "        )\n",
    "    else:\n",
    "        train_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=False,\n",
    "            feature_pipeline=pipeline\n",
    "        )\n",
    "        val_ds = None\n",
    "\n",
    "    sample = train_ds[0][0]  # first sample's features\n",
    "    if isinstance(sample, dict):  # multiple feature groups\n",
    "        input_dim = sample['main'].shape[1]\n",
    "    else:  # single tensor\n",
    "        input_dim = sample.shape[1]\n",
    "\n",
    "    model = CNNLSTM_MDN(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        lr=lr\n",
    "    )\n",
    "    init_args = get_init_args(model, input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, lr=lr)\n",
    "\n",
    "    model_class_info = {\n",
    "        \"module\": model.__class__.__module__,\n",
    "        \"class\": model.__class__.__name__,\n",
    "        \"init_args\": init_args\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_batch) if val_ds else None\n",
    "    \n",
    "    # --- Debug / Test mode --- #\n",
    "    if test_mode:\n",
    "        save_model = False\n",
    "        from itertools import islice\n",
    "\n",
    "        # Try to grab 3rd batch; if not available, take first\n",
    "        try:\n",
    "            batch = next(islice(iter(train_loader), 2, 3))\n",
    "        except StopIteration:\n",
    "            batch = next(iter(train_loader))\n",
    "\n",
    "        X_batch_dict, y_batch, lengths = batch\n",
    "\n",
    "        print(\"ðŸ” Debug batch:\")\n",
    "        if isinstance(X_batch_dict, dict):\n",
    "            print(\"  Keys in X_batch:\", list(X_batch_dict.keys()))\n",
    "        print(\"  y_batch shape:\", y_batch.shape)\n",
    "        print(\"  First label in batch:\", y_batch[0])\n",
    "\n",
    "        # --- Track real column names for each feature group ---\n",
    "        feature_names_dict = {}\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            if name == \"main\":\n",
    "                # Use actual feature columns after preprocessing\n",
    "                feature_names_dict[name] = feature_cols\n",
    "            else:\n",
    "                # For extra feature groups, fallback to generic names\n",
    "                feature_names_dict[name] = [f\"{name}_{i}\" for i in range(X_batch.shape[2])]\n",
    "\n",
    "        dfs = []\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            print(f\"\\nFeature group: {name}\")\n",
    "            print(\"  X_batch shape:\", X_batch.shape)\n",
    "            print(\"  First sequence in batch (first  steps):\\n\", X_batch[0][:])\n",
    "\n",
    "            batch_size_, seq_len, feature_dim = X_batch.shape\n",
    "            df_part = pd.DataFrame(\n",
    "                X_batch.reshape(batch_size_ * seq_len, feature_dim).numpy(),\n",
    "                columns=feature_names_dict[name]\n",
    "            )\n",
    "            dfs.append(df_part)\n",
    "\n",
    "        # Combine all feature groups horizontally\n",
    "        global df_seq\n",
    "        df_seq = pd.concat(dfs, axis=1)\n",
    "        print(\"\\nâœ… Combined df_seq shape:\", df_seq.shape)\n",
    "        print(\"âœ… Column names in df_seq:\", df_seq.columns.tolist())\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        fast_dev_run=test_mode,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\"\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        trainer.save_checkpoint(model_out)\n",
    "        joblib.dump({\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"max_len_y\": max_len_y,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"scalers\": pipeline.scalers,\n",
    "    \"pipeline_config\": pipeline.export_config(),\n",
    "    \"model_class_info\": model_class_info   # âœ… save model class info\n",
    "}, meta_out)\n",
    "        \n",
    "    # --- Evaluation --- #\n",
    "    if do_validation:\n",
    "        mse, mae, acc, f1 = evaluate_model_mdn(model, val_loader)\n",
    "        if return_val_accuracy:\n",
    "            return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles_prop.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/seq_line_labels.csv\",\n",
    "        save_model=True,\n",
    "        do_validation=True,\n",
    "        test_mode = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "399de860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "upper_shadow",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "body",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "lower_shadow",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "Candle_Color",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "upper_body_ratio",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "lower_body_ratio",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "upper_lower_body_ratio",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "open_prop",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "high_prop",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "low_prop",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "close_prop",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "open_dif",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "high_dif",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "low_dif",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "close_dif",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "54850ea1-de31-4003-8807-89cf9890899c",
       "rows": [
        [
         "0",
         "1.3217858",
         "0.008633563",
         "-0.8959257",
         "0.9548821",
         "2.4932256",
         "-0.35504246",
         "2.5470576",
         "1.1623682",
         "1.2289896",
         "1.1575172",
         "1.1679865",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "-0.4148322",
         "-0.82140845",
         "-0.4182826",
         "-1.0472498",
         "-0.69693536",
         "-0.7031182",
         "-0.5978814",
         "1.1673691",
         "1.178731",
         "1.0636079",
         "1.0883837",
         "0.0043024602",
         "-0.040894248",
         "-0.08112992",
         "-0.0681538"
        ],
        [
         "2",
         "1.0011624",
         "-0.6899791",
         "-0.6813451",
         "-1.0472498",
         "-0.27421898",
         "-0.7685983",
         "1.6776013",
         "1.0885317",
         "1.1400876",
         "1.0092804",
         "1.023049",
         "-0.06753419",
         "-0.032783885",
         "-0.05107843",
         "-0.060029127"
        ],
        [
         "3",
         "0.31009448",
         "0.27949253",
         "-0.39333308",
         "0.9548821",
         "-0.1585099",
         "-0.4092994",
         "-0.057558753",
         "1.0235193",
         "1.0881126",
         "0.99793434",
         "1.0564171",
         "-0.059724957",
         "-0.04558858",
         "-0.0112418635",
         "0.032616317"
        ],
        [
         "4",
         "0.5435583",
         "-0.49846163",
         "-0.14581029",
         "-1.0472498",
         "-0.24074703",
         "-0.4049249",
         "-0.17559932",
         "1.0564171",
         "1.0945746",
         "0.9751825",
         "1.0108978",
         "0.032141857",
         "0.005938668",
         "-0.02279892",
         "-0.04308837"
        ],
        [
         "5",
         "-0.24181376",
         "-0.15580483",
         "-0.2055458",
         "-1.0472498",
         "0.16709252",
         "1.0079327",
         "-0.5804288",
         "1.0108978",
         "1.0267987",
         "0.9671324",
         "1.0",
         "-0.04308837",
         "-0.061919775",
         "-0.008254918",
         "-0.010780328"
        ],
        [
         "56",
         "0.448118",
         "1.0001476",
         "-0.4999513",
         "0.9548821",
         "-0.5702011",
         "-0.7748621",
         "0.24845493",
         "1.1979386",
         "1.2667452",
         "1.187625",
         "1.2494111",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "57",
         "-0.35491216",
         "0.0063638682",
         "1.3473517",
         "0.9548821",
         "0.80852634",
         "2.2641795",
         "-0.8235902",
         "1.2494121",
         "1.2582763",
         "1.20204",
         "1.2519922",
         "0.042968415",
         "-0.0066855173",
         "0.012137569",
         "0.0020659657"
        ],
        [
         "58",
         "-0.41764733",
         "-0.291699",
         "-0.0675294",
         "-1.0472498",
         "-0.49507174",
         "0.09639787",
         "-0.71727103",
         "1.251408",
         "1.2567823",
         "1.2207159",
         "1.239495",
         "0.0015974895",
         "-0.0011873964",
         "0.015536846",
         "-0.009981858"
        ],
        [
         "59",
         "-0.0072208024",
         "-1.2704879",
         "-0.8058466",
         "-1.0472498",
         "-0.67053854",
         "-0.8581161",
         "0.93462515",
         "1.2394941",
         "1.2504512",
         "1.175852",
         "1.1799533",
         "-0.009520459",
         "-0.0050374907",
         "-0.03675214",
         "-0.04803708"
        ],
        [
         "60",
         "-0.42783228",
         "0.47275552",
         "0.025624376",
         "0.9548821",
         "-0.6549056",
         "-0.38322914",
         "-0.7397016",
         "1.1799461",
         "1.2102824",
         "1.1593785",
         "1.2050625",
         "-0.048042167",
         "-0.032123487",
         "-0.014009761",
         "0.021279814"
        ],
        [
         "61",
         "-0.51162845",
         "-0.35755244",
         "-0.15767364",
         "-1.0472498",
         "-0.61314285",
         "-0.18991125",
         "-0.7482721",
         "1.2039263",
         "1.2079755",
         "1.1722337",
         "1.1890064",
         "0.020323204",
         "-0.0019060345",
         "0.011087903",
         "-0.013323844"
        ],
        [
         "62",
         "-0.36381763",
         "-1.1460869",
         "-0.45821962",
         "-1.0472498",
         "-0.716326",
         "-0.77117366",
         "-0.53147507",
         "1.1892432",
         "1.195275",
         "1.1256096",
         "1.1364875",
         "-0.012195947",
         "-0.010514006",
         "-0.039773677",
         "-0.04417045"
        ],
        [
         "63",
         "-0.7824593",
         "-1.8615154",
         "-0.36910915",
         "-1.0472498",
         "-0.78822255",
         "-0.80989337",
         "-0.8921186",
         "1.1365553",
         "1.1369852",
         "1.0358452",
         "1.0486022",
         "-0.044303782",
         "-0.048766777",
         "-0.07974732",
         "-0.07733062"
        ],
        [
         "64",
         "0.4707593",
         "0.46947542",
         "-0.2517019",
         "0.9548821",
         "-0.33452213",
         "-0.5194414",
         "-0.117701754",
         "1.049102",
         "1.0913409",
         "1.0340611",
         "1.074014",
         "-0.07694595",
         "-0.040145062",
         "-0.0017223839",
         "0.024233934"
        ],
        [
         "65",
         "0.76136506",
         "0.028503697",
         "1.8585002",
         "0.9548821",
         "2.4932256",
         "2.2641795",
         "-0.6564598",
         "1.0743102",
         "1.0992734",
         "1.0172632",
         "1.077938",
         "0.02402846",
         "0.0072686425",
         "-0.016244577",
         "0.0036536094"
        ],
        [
         "66",
         "0.22409666",
         "-0.29534906",
         "0.12882517",
         "-1.0472498",
         "-0.026886297",
         "0.28631654",
         "-0.4859171",
         "1.0778973",
         "1.0918771",
         "1.0433469",
         "1.0658852",
         "0.003338911",
         "-0.006728341",
         "0.025641026",
         "-0.011181295"
        ],
        [
         "67",
         "0.69369113",
         "0.15138318",
         "-0.28828806",
         "0.9548821",
         "0.60757875",
         "0.047727283",
         "0.068047576",
         "1.0658884",
         "1.0955142",
         "1.0516937",
         "1.0753547",
         "-0.011141064",
         "0.0033310591",
         "0.008",
         "0.008884086"
        ],
        [
         "68",
         "-0.7014894",
         "1.2801466",
         "-0.75894403",
         "0.9548821",
         "-0.77584463",
         "-0.85230327",
         "-0.70495105",
         "1.075291",
         "1.1403781",
         "1.0703278",
         "1.1388695",
         "0.008821413",
         "0.04095238",
         "0.017718254",
         "0.059064034"
        ],
        [
         "69",
         "-0.5373559",
         "-0.3846471",
         "0.06196003",
         "-1.0472498",
         "-0.6413638",
         "-0.078496605",
         "-0.7937791",
         "1.138785",
         "1.1424607",
         "1.1017743",
         "1.1226945",
         "0.059048124",
         "0.0018261665",
         "0.029380227",
         "-0.014202687"
        ],
        [
         "70",
         "-0.4746871",
         "-1.3891238",
         "0.49751076",
         "-1.0472498",
         "-0.7450608",
         "-0.6101429",
         "-0.80930364",
         "1.1226945",
         "1.1272455",
         "1.0285313",
         "1.0582219",
         "-0.01412953",
         "-0.013317857",
         "-0.06647728",
         "-0.057426646"
        ],
        [
         "71",
         "-0.022576427",
         "-0.13570833",
         "0.789432",
         "-1.0472498",
         "0.83212215",
         "2.2641795",
         "-0.7080796",
         "1.0580393",
         "1.0685959",
         "1.0185152",
         "1.0537678",
         "-0.057589278",
         "-0.052029178",
         "-0.009738283",
         "-0.0042089922"
        ],
        [
         "72",
         "-0.3774368",
         "0.5906773",
         "-0.581717",
         "0.9548821",
         "-0.6656716",
         "-0.72660965",
         "-0.4361216",
         "1.05377",
         "1.0897758",
         "1.0454117",
         "1.0839905",
         "-0.0040351767",
         "0.019820347",
         "0.026407499",
         "0.028680539"
        ],
        [
         "73",
         "-0.567156",
         "-0.60632586",
         "0.3041577",
         "-1.0472498",
         "-0.71052355",
         "-0.29214978",
         "-0.8268604",
         "1.0838455",
         "1.087107",
         "1.0318701",
         "1.0573642",
         "0.028540876",
         "-0.0024490186",
         "-0.012953365",
         "-0.024563143"
        ],
        [
         "74",
         "0.69996005",
         "-0.028338213",
         "0.22229622",
         "0.9548821",
         "2.4932256",
         "2.2641795",
         "-0.33651227",
         "1.0573914",
         "1.0780903",
         "1.0336406",
         "1.0582345",
         "-0.024407595",
         "-0.008294128",
         "0.0017158746",
         "0.00082294375"
        ],
        [
         "75",
         "1.039357",
         "0.46627936",
         "-0.98570585",
         "0.9548821",
         "-0.12823433",
         "-0.8886029",
         "2.5470576",
         "1.0582345",
         "1.1063651",
         "1.0577377",
         "1.0821855",
         "0.000797268",
         "0.026226653",
         "0.02331288",
         "0.02263305"
        ],
        [
         "76",
         "-0.6731366",
         "-0.47117525",
         "-0.2748857",
         "-1.0472498",
         "-0.73053074",
         "-0.45183858",
         "-0.8250975",
         "1.0823263",
         "1.0841501",
         "1.0486763",
         "1.062654",
         "0.02276615",
         "-0.020079216",
         "-0.008566838",
         "-0.018048158"
        ],
        [
         "77",
         "0.09818621",
         "-1.0908835",
         "-0.3533824",
         "-1.0472498",
         "-0.631247",
         "-0.7382349",
         "-0.26150343",
         "1.0627209",
         "1.074543",
         "1.0017184",
         "1.0142375",
         "-0.018114252",
         "-0.008861447",
         "-0.044778273",
         "-0.045561876"
        ],
        [
         "78",
         "-0.78991044",
         "-0.55013317",
         "-0.025791513",
         "-1.0472498",
         "-0.7825781",
         "-0.39407107",
         "-0.90381044",
         "1.0142375",
         "1.0145506",
         "0.9723993",
         "0.9910168",
         "-0.04562185",
         "-0.055830665",
         "-0.029268796",
         "-0.022894764"
        ],
        [
         "79",
         "0.09770265",
         "0.1490694",
         "-0.18333735",
         "0.9548821",
         "0.06465955",
         "0.19836013",
         "-0.39602986",
         "0.99107623",
         "1.0116291",
         "0.9755721",
         "1.0",
         "-0.022836128",
         "-0.0028794734",
         "0.0032628756",
         "0.009064646"
        ],
        [
         "112",
         "1.4683312",
         "-3.0287366",
         "-0.41100004",
         "-1.0472498",
         "-0.65107816",
         "-0.8494799",
         "0.8774327",
         "0.8477978",
         "0.86426795",
         "0.764348",
         "0.7707096",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "113",
         "0.46859515",
         "0.6289703",
         "-0.942101",
         "0.9548821",
         "-0.443032",
         "-0.8750118",
         "2.5470576",
         "0.7707096",
         "0.7972882",
         "0.7699767",
         "0.7880773",
         "-0.09092764",
         "-0.07749881",
         "0.0073639923",
         "0.0225347"
        ],
        [
         "114",
         "-0.43468842",
         "-0.20201734",
         "-0.37399918",
         "-1.0472498",
         "-0.3444734",
         "0.16060342",
         "-0.63462764",
         "0.78818434",
         "0.79087514",
         "0.7775941",
         "0.78422946",
         "0.022673605",
         "-0.008043567",
         "0.009893049",
         "-0.004882547"
        ],
        [
         "115",
         "-0.21001214",
         "0.024822405",
         "1.6060137",
         "0.9548821",
         "0.76690495",
         "2.2641795",
         "-0.80655193",
         "0.78424597",
         "0.79032755",
         "0.75704145",
         "0.7860494",
         "-0.0049968134",
         "-0.00069243606",
         "-0.02643103",
         "0.002320679"
        ],
        [
         "116",
         "1.2315007",
         "-0.081794806",
         "-0.019148491",
         "-1.0472498",
         "2.4932256",
         "2.2641795",
         "0.056996744",
         "0.7860432",
         "0.80044633",
         "0.77489096",
         "0.78514767",
         "0.0022917537",
         "0.012803313",
         "0.023577914",
         "-0.0011471765"
        ],
        [
         "117",
         "0.1949852",
         "0.026137983",
         "-0.98636526",
         "0.9548821",
         "1.7641366",
         "-0.81117404",
         "2.5470576",
         "0.785265",
         "0.79406416",
         "0.78500766",
         "0.7870644",
         "-0.0009900368",
         "-0.007973251",
         "0.013055711",
         "0.0024412053"
        ],
        [
         "118",
         "-0.2393031",
         "-0.065068886",
         "-0.035895225",
         "-1.0472498",
         "2.4932256",
         "2.2641795",
         "-0.6374646",
         "0.7868935",
         "0.790834",
         "0.77661616",
         "0.7864323",
         "0.0020737967",
         "-0.0040679285",
         "-0.010689718",
         "-0.0008030343"
        ],
        [
         "119",
         "-0.3565483",
         "-0.15016736",
         "-0.46332312",
         "-1.0472498",
         "0.016560597",
         "0.46774483",
         "-0.5216465",
         "0.78644055",
         "0.7895349",
         "0.7784835",
         "0.78392476",
         "-0.00057558925",
         "-0.0016426687",
         "0.0024044006",
         "-0.00318854"
        ],
        [
         "120",
         "-0.12313073",
         "-0.82906973",
         "-0.08554044",
         "-1.0472498",
         "-0.6295205",
         "-0.5947743",
         "-0.5633807",
         "0.78383625",
         "0.7885055",
         "0.7557074",
         "0.76489156",
         "-0.0033115444",
         "-0.0013037809",
         "-0.029256979",
         "-0.02427943"
        ],
        [
         "121",
         "-0.2246272",
         "1.1572224",
         "-0.75138515",
         "0.9548821",
         "-0.7015881",
         "-0.84562516",
         "0.15337294",
         "0.76505625",
         "0.7982023",
         "0.7624725",
         "0.7942145",
         "-0.02395911",
         "0.01229765",
         "0.0089519955",
         "0.03833607"
        ],
        [
         "122",
         "0.5723421",
         "-0.009771248",
         "-0.081011906",
         "0.9548821",
         "2.4932256",
         "2.2641795",
         "-0.21252215",
         "0.7941836",
         "0.804358",
         "0.78502417",
         "0.79506063",
         "0.038072173",
         "0.0077119498",
         "0.029577028",
         "0.0010653933"
        ],
        [
         "123",
         "0.17280267",
         "0.056089893",
         "-0.5576978",
         "0.9548821",
         "0.9767153",
         "0.24872793",
         "0.11038032",
         "0.79506063",
         "0.8040286",
         "0.7906466",
         "0.79748994",
         "0.0011043192",
         "-0.00040952137",
         "0.0071621705",
         "0.0030555383"
        ],
        [
         "124",
         "1.0064411",
         "-0.12238998",
         "0.36729768",
         "-1.0472498",
         "2.4932256",
         "2.2641795",
         "-0.29253197",
         "0.79748785",
         "0.80950487",
         "0.78232926",
         "0.7956885",
         "0.0030529487",
         "0.006811082",
         "-0.010519737",
         "-0.0022588572"
        ],
        [
         "125",
         "0.08619823",
         "0.63803893",
         "-0.7951952",
         "0.9548821",
         "-0.5501268",
         "-0.819668",
         "1.047206",
         "0.7956864",
         "0.81768847",
         "0.793601",
         "0.8117778",
         "-0.002258863",
         "0.010109359",
         "0.014407895",
         "0.020220498"
        ],
        [
         "126",
         "-0.814239",
         "-0.2821644",
         "-0.2516845",
         "-1.0472498",
         "-0.7914332",
         "-0.065817274",
         "-0.9154531",
         "0.81185806",
         "0.81185806",
         "0.79914725",
         "0.8063797",
         "0.020324046",
         "-0.0071303584",
         "0.00698878",
         "-0.0066496916"
        ],
        [
         "127",
         "-0.1434031",
         "-0.44224223",
         "-0.10564886",
         "-1.0472498",
         "-0.48073027",
         "-0.30807024",
         "-0.5661231",
         "0.8062088",
         "0.8105301",
         "0.7885055",
         "0.79707205",
         "-0.006958409",
         "-0.0016356319",
         "-0.013316365",
         "-0.011542527"
        ],
        [
         "128",
         "-0.08890688",
         "0.053921312",
         "-0.009923686",
         "0.9548821",
         "0.53601795",
         "1.6919239",
         "-0.57383895",
         "0.79707205",
         "0.8039874",
         "0.7876944",
         "0.79936135",
         "-0.011333021",
         "-0.008072177",
         "-0.0010287206",
         "0.0028721902"
        ],
        [
         "129",
         "-0.4586939",
         "-0.19868286",
         "-0.43226466",
         "-1.0472498",
         "-0.36358133",
         "0.08465133",
         "-0.6259339",
         "0.7993531",
         "0.80159104",
         "0.7905643",
         "0.79591703",
         "0.0028618586",
         "-0.002980641",
         "0.0036434347",
         "-0.004308824"
        ],
        [
         "130",
         "0.596382",
         "0.055836413",
         "0.23378837",
         "0.9548821",
         "1.7417713",
         "2.2624712",
         "-0.38109115",
         "0.79591703",
         "0.8070344",
         "0.7844765",
         "0.79820645",
         "-0.0042985664",
         "0.0067907004",
         "-0.007700521",
         "0.002876358"
        ],
        [
         "131",
         "-0.2998843",
         "0.38689524",
         "-0.9001259",
         "0.9548821",
         "-0.5736767",
         "-0.83493847",
         "1.2605801",
         "0.79820645",
         "0.8109913",
         "0.79719347",
         "0.80780846",
         "0.002876358",
         "0.0049030613",
         "0.016210768",
         "0.012029547"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 812
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>upper_shadow</th>\n",
       "      <th>body</th>\n",
       "      <th>lower_shadow</th>\n",
       "      <th>Candle_Color</th>\n",
       "      <th>upper_body_ratio</th>\n",
       "      <th>lower_body_ratio</th>\n",
       "      <th>upper_lower_body_ratio</th>\n",
       "      <th>open_prop</th>\n",
       "      <th>high_prop</th>\n",
       "      <th>low_prop</th>\n",
       "      <th>close_prop</th>\n",
       "      <th>open_dif</th>\n",
       "      <th>high_dif</th>\n",
       "      <th>low_dif</th>\n",
       "      <th>close_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.321786</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>-0.895926</td>\n",
       "      <td>0.954882</td>\n",
       "      <td>2.493226</td>\n",
       "      <td>-0.355042</td>\n",
       "      <td>2.547058</td>\n",
       "      <td>1.162368</td>\n",
       "      <td>1.228990</td>\n",
       "      <td>1.157517</td>\n",
       "      <td>1.167987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.414832</td>\n",
       "      <td>-0.821408</td>\n",
       "      <td>-0.418283</td>\n",
       "      <td>-1.047250</td>\n",
       "      <td>-0.696935</td>\n",
       "      <td>-0.703118</td>\n",
       "      <td>-0.597881</td>\n",
       "      <td>1.167369</td>\n",
       "      <td>1.178731</td>\n",
       "      <td>1.063608</td>\n",
       "      <td>1.088384</td>\n",
       "      <td>0.004302</td>\n",
       "      <td>-0.040894</td>\n",
       "      <td>-0.081130</td>\n",
       "      <td>-0.068154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.001162</td>\n",
       "      <td>-0.689979</td>\n",
       "      <td>-0.681345</td>\n",
       "      <td>-1.047250</td>\n",
       "      <td>-0.274219</td>\n",
       "      <td>-0.768598</td>\n",
       "      <td>1.677601</td>\n",
       "      <td>1.088532</td>\n",
       "      <td>1.140088</td>\n",
       "      <td>1.009280</td>\n",
       "      <td>1.023049</td>\n",
       "      <td>-0.067534</td>\n",
       "      <td>-0.032784</td>\n",
       "      <td>-0.051078</td>\n",
       "      <td>-0.060029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.310094</td>\n",
       "      <td>0.279493</td>\n",
       "      <td>-0.393333</td>\n",
       "      <td>0.954882</td>\n",
       "      <td>-0.158510</td>\n",
       "      <td>-0.409299</td>\n",
       "      <td>-0.057559</td>\n",
       "      <td>1.023519</td>\n",
       "      <td>1.088113</td>\n",
       "      <td>0.997934</td>\n",
       "      <td>1.056417</td>\n",
       "      <td>-0.059725</td>\n",
       "      <td>-0.045589</td>\n",
       "      <td>-0.011242</td>\n",
       "      <td>0.032616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.543558</td>\n",
       "      <td>-0.498462</td>\n",
       "      <td>-0.145810</td>\n",
       "      <td>-1.047250</td>\n",
       "      <td>-0.240747</td>\n",
       "      <td>-0.404925</td>\n",
       "      <td>-0.175599</td>\n",
       "      <td>1.056417</td>\n",
       "      <td>1.094575</td>\n",
       "      <td>0.975182</td>\n",
       "      <td>1.010898</td>\n",
       "      <td>0.032142</td>\n",
       "      <td>0.005939</td>\n",
       "      <td>-0.022799</td>\n",
       "      <td>-0.043088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>-0.059996</td>\n",
       "      <td>-1.183145</td>\n",
       "      <td>-0.249545</td>\n",
       "      <td>-1.047250</td>\n",
       "      <td>-0.669764</td>\n",
       "      <td>-0.727806</td>\n",
       "      <td>-0.448530</td>\n",
       "      <td>1.117452</td>\n",
       "      <td>1.139657</td>\n",
       "      <td>0.964626</td>\n",
       "      <td>0.997559</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>-0.014801</td>\n",
       "      <td>-0.104581</td>\n",
       "      <td>-0.107104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>-0.024688</td>\n",
       "      <td>0.020147</td>\n",
       "      <td>0.508013</td>\n",
       "      <td>0.954882</td>\n",
       "      <td>1.388192</td>\n",
       "      <td>2.264179</td>\n",
       "      <td>-0.670333</td>\n",
       "      <td>0.999270</td>\n",
       "      <td>1.029305</td>\n",
       "      <td>0.934067</td>\n",
       "      <td>1.006226</td>\n",
       "      <td>-0.105760</td>\n",
       "      <td>-0.096829</td>\n",
       "      <td>-0.031680</td>\n",
       "      <td>0.008688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>0.666663</td>\n",
       "      <td>-0.023924</td>\n",
       "      <td>-0.033323</td>\n",
       "      <td>0.954882</td>\n",
       "      <td>2.493226</td>\n",
       "      <td>2.264179</td>\n",
       "      <td>-0.201301</td>\n",
       "      <td>1.006226</td>\n",
       "      <td>1.051484</td>\n",
       "      <td>0.964592</td>\n",
       "      <td>1.008548</td>\n",
       "      <td>0.006961</td>\n",
       "      <td>0.021547</td>\n",
       "      <td>0.032680</td>\n",
       "      <td>0.002308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>-0.437450</td>\n",
       "      <td>-0.562835</td>\n",
       "      <td>-0.713843</td>\n",
       "      <td>-1.047250</td>\n",
       "      <td>-0.657662</td>\n",
       "      <td>-0.752152</td>\n",
       "      <td>-0.318531</td>\n",
       "      <td>1.008548</td>\n",
       "      <td>1.019369</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.955410</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>-0.030542</td>\n",
       "      <td>-0.022533</td>\n",
       "      <td>-0.052688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>-0.270822</td>\n",
       "      <td>0.389472</td>\n",
       "      <td>0.447057</td>\n",
       "      <td>0.954882</td>\n",
       "      <td>-0.562733</td>\n",
       "      <td>-0.033204</td>\n",
       "      <td>-0.739696</td>\n",
       "      <td>0.955314</td>\n",
       "      <td>1.015557</td>\n",
       "      <td>0.894018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052783</td>\n",
       "      <td>-0.003740</td>\n",
       "      <td>-0.051799</td>\n",
       "      <td>0.046671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>812 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      upper_shadow      body  lower_shadow  Candle_Color  upper_body_ratio  \\\n",
       "0         1.321786  0.008634     -0.895926      0.954882          2.493226   \n",
       "1        -0.414832 -0.821408     -0.418283     -1.047250         -0.696935   \n",
       "2         1.001162 -0.689979     -0.681345     -1.047250         -0.274219   \n",
       "3         0.310094  0.279493     -0.393333      0.954882         -0.158510   \n",
       "4         0.543558 -0.498462     -0.145810     -1.047250         -0.240747   \n",
       "...            ...       ...           ...           ...               ...   \n",
       "1745     -0.059996 -1.183145     -0.249545     -1.047250         -0.669764   \n",
       "1746     -0.024688  0.020147      0.508013      0.954882          1.388192   \n",
       "1747      0.666663 -0.023924     -0.033323      0.954882          2.493226   \n",
       "1748     -0.437450 -0.562835     -0.713843     -1.047250         -0.657662   \n",
       "1749     -0.270822  0.389472      0.447057      0.954882         -0.562733   \n",
       "\n",
       "      lower_body_ratio  upper_lower_body_ratio  open_prop  high_prop  \\\n",
       "0            -0.355042                2.547058   1.162368   1.228990   \n",
       "1            -0.703118               -0.597881   1.167369   1.178731   \n",
       "2            -0.768598                1.677601   1.088532   1.140088   \n",
       "3            -0.409299               -0.057559   1.023519   1.088113   \n",
       "4            -0.404925               -0.175599   1.056417   1.094575   \n",
       "...                ...                     ...        ...        ...   \n",
       "1745         -0.727806               -0.448530   1.117452   1.139657   \n",
       "1746          2.264179               -0.670333   0.999270   1.029305   \n",
       "1747          2.264179               -0.201301   1.006226   1.051484   \n",
       "1748         -0.752152               -0.318531   1.008548   1.019369   \n",
       "1749         -0.033204               -0.739696   0.955314   1.015557   \n",
       "\n",
       "      low_prop  close_prop  open_dif  high_dif   low_dif  close_dif  \n",
       "0     1.157517    1.167987  0.000000  0.000000  0.000000   0.000000  \n",
       "1     1.063608    1.088384  0.004302 -0.040894 -0.081130  -0.068154  \n",
       "2     1.009280    1.023049 -0.067534 -0.032784 -0.051078  -0.060029  \n",
       "3     0.997934    1.056417 -0.059725 -0.045589 -0.011242   0.032616  \n",
       "4     0.975182    1.010898  0.032142  0.005939 -0.022799  -0.043088  \n",
       "...        ...         ...       ...       ...       ...        ...  \n",
       "1745  0.964626    0.997559  0.002254 -0.014801 -0.104581  -0.107104  \n",
       "1746  0.934067    1.006226 -0.105760 -0.096829 -0.031680   0.008688  \n",
       "1747  0.964592    1.008548  0.006961  0.021547  0.032680   0.002308  \n",
       "1748  0.942857    0.955410  0.002308 -0.030542 -0.022533  -0.052688  \n",
       "1749  0.894018    1.000000 -0.052783 -0.003740 -0.051799   0.046671  \n",
       "\n",
       "[812 rows x 15 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seq = df_seq.loc[~(df_seq==0).all(axis=1)]\n",
    "df_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc099c49",
   "metadata": {},
   "source": [
    "# server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709f0d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features ['upper_shadow', 'body', 'lower_shadow', 'Candle_Color', 'upper_body_ratio', 'lower_body_ratio', 'upper_lower_body_ratio', 'open_prop', 'high_prop', 'low_prop', 'close_prop']\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [08/Sep/2025 00:59:35] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Sep/2025 00:59:35] \"GET /get_and_add_data?init=1 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning initial candles: [{'time': 1514764800, 'open': 13707.91, 'high': 13818.55, 'low': 12750.0, 'close': 13380.0}, {'time': 1514851200, 'open': 13382.16, 'high': 15473.49, 'low': 12890.02, 'close': 14675.11}, {'time': 1514937600, 'open': 14690.0, 'high': 15307.56, 'low': 14150.0, 'close': 14919.51}, {'time': 1515024000, 'open': 14919.51, 'high': 15280.0, 'low': 13918.04, 'close': 15059.54}, {'time': 1515110400, 'open': 15059.56, 'high': 17176.24, 'low': 14600.0, 'close': 16960.39}, {'time': 1515196800, 'open': 16960.39, 'high': 17143.13, 'low': 16011.21, 'close': 17069.79}, {'time': 1515283200, 'open': 17069.79, 'high': 17099.96, 'low': 15610.0, 'close': 16150.03}, {'time': 1515369600, 'open': 16218.85, 'high': 16322.3, 'low': 12812.0, 'close': 14902.54}, {'time': 1515456000, 'open': 14902.54, 'high': 15500.0, 'low': 14011.05, 'close': 14400.0}, {'time': 1515542400, 'open': 14401.0, 'high': 14955.66, 'low': 13131.31, 'close': 14907.09}, {'time': 1515628800, 'open': 14940.0, 'high': 14968.68, 'low': 11400.0, 'close': 13238.78}, {'time': 1515715200, 'open': 13238.76, 'high': 14109.78, 'low': 12500.0, 'close': 13740.01}, {'time': 1515801600, 'open': 13749.95, 'high': 14580.0, 'low': 13706.15, 'close': 14210.0}, {'time': 1515888000, 'open': 14210.0, 'high': 14339.5, 'low': 12569.2, 'close': 13474.99}, {'time': 1515974400, 'open': 13477.98, 'high': 14249.99, 'low': 13147.79, 'close': 13539.93}, {'time': 1516060800, 'open': 13500.0, 'high': 13542.93, 'low': 9035.0, 'close': 10900.0}, {'time': 1516147200, 'open': 10899.99, 'high': 11680.99, 'low': 9037.94, 'close': 10988.79}, {'time': 1516233600, 'open': 10972.59, 'high': 11878.82, 'low': 10435.33, 'close': 10961.97}, {'time': 1516320000, 'open': 10960.0, 'high': 11795.0, 'low': 10360.0, 'close': 11474.98}, {'time': 1516406400, 'open': 11474.98, 'high': 13099.0, 'low': 11412.45, 'close': 12799.94}, {'time': 1516492800, 'open': 12799.8, 'high': 12799.8, 'low': 10965.0, 'close': 11530.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Sep/2025 00:59:36] \"GET /get_and_add_data?idx=21 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Sep/2025 00:59:36] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Sep/2025 00:59:36] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Sep/2025 00:59:36] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Sep/2025 00:59:36] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Sep/2025 00:59:36] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XT XT XT XT XT tensor([[[ 0.1086,  1.0951, -0.8802,  0.9549, -0.6432, -0.8718,  2.3966,\n",
      "           1.0664,  1.2174,  1.0606,  1.1896],\n",
      "         [-0.8142, -1.1374,  0.1731, -1.0472, -0.7914, -0.6200, -0.9155,\n",
      "           1.1896,  1.1896,  1.0190,  1.0716],\n",
      "         [ 0.3998, -0.7047,  0.7826, -1.0472, -0.4533, -0.1947, -0.5962,\n",
      "           1.0716,  1.1084,  0.9201,  1.0000]]])\n",
      "First candle features: [ 0.10861079  1.0951006  -0.88018584  0.9548821  -0.6431555  -0.87184983\n",
      "  2.3965523   1.0664431   1.2173735   1.0606318   1.1895801 ]\n",
      "tensor([[[ 1.9777, -0.0554,  0.0937, -1.0472,  2.4932,  2.2642,  0.2762,\n",
      "           1.0198,  1.1040,  0.9698,  1.0188],\n",
      "         [ 0.1740,  0.3977,  0.2507,  0.9549, -0.3832, -0.1640, -0.5461,\n",
      "           1.0186,  1.0962,  0.9628,  1.0664],\n",
      "         [ 0.1086,  1.0951, -0.8802,  0.9549, -0.6432, -0.8718,  2.3966,\n",
      "           1.0664,  1.2174,  1.0606,  1.1896],\n",
      "         [-0.8142, -1.1374,  0.1731, -1.0472, -0.7914, -0.6200, -0.9155,\n",
      "           1.1896,  1.1896,  1.0190,  1.0716],\n",
      "         [ 0.3998, -0.7047,  0.7826, -1.0472, -0.4533, -0.1947, -0.5962,\n",
      "           1.0716,  1.1084,  0.9201,  1.0000]]])\n",
      "First candle features: [ 1.9776832  -0.05541882  0.09371395 -1.0472498   2.4932256   2.2641795\n",
      "  0.27618867  1.0197527   1.1039745   0.96982175  1.0187657 ]\n",
      "tensor([[[ 1.4803,  0.0096, -0.2847,  0.9549,  2.4932,  2.2642,  0.5737,\n",
      "           1.2526,  1.3243,  1.2219,  1.2584],\n",
      "         [-0.6806, -2.3057,  2.9436, -1.0472, -0.7806, -0.4475, -0.8995,\n",
      "           1.2546,  1.2586,  0.8397,  1.0130],\n",
      "         [ 1.3134,  0.0299,  2.8875,  0.9549,  2.4932,  2.2642, -0.6580,\n",
      "           1.0130,  1.0856,  0.8400,  1.0213],\n",
      "         [ 1.9777, -0.0554,  0.0937, -1.0472,  2.4932,  2.2642,  0.2762,\n",
      "           1.0198,  1.1040,  0.9698,  1.0188],\n",
      "         [ 0.1740,  0.3977,  0.2507,  0.9549, -0.3832, -0.1640, -0.5461,\n",
      "           1.0186,  1.0962,  0.9628,  1.0664],\n",
      "         [ 0.1086,  1.0951, -0.8802,  0.9549, -0.6432, -0.8718,  2.3966,\n",
      "           1.0664,  1.2174,  1.0606,  1.1896],\n",
      "         [-0.8142, -1.1374,  0.1731, -1.0472, -0.7914, -0.6200, -0.9155,\n",
      "           1.1896,  1.1896,  1.0190,  1.0716],\n",
      "         [ 0.3998, -0.7047,  0.7826, -1.0472, -0.4533, -0.1947, -0.5962,\n",
      "           1.0716,  1.1084,  0.9201,  1.0000]]])\n",
      "First candle features: [ 1.4802635   0.00959943 -0.2847321   0.9548821   2.4932256   2.2641795\n",
      "  0.5737443   1.2525946   1.3243424   1.221908    1.2583519 ]\n",
      "tensor([[[-0.6550,  0.4170,  1.8249,  0.9549, -0.7284,  0.6868, -0.8890,\n",
      "           1.3384,  1.3899,  1.2204,  1.3854],\n",
      "         [-0.7226, -1.5632,  2.9897, -1.0472, -0.7804, -0.2174, -0.9047,\n",
      "           1.3885,  1.3911,  1.0595,  1.2304],\n",
      "         [ 0.3663,  0.4004,  0.5952,  0.9549, -0.3068,  0.0315, -0.5688,\n",
      "           1.2304,  1.3113,  1.1617,  1.2769],\n",
      "         [ 0.3776,  0.3674, -0.9155,  0.9549, -0.2631, -0.8414,  2.5471,\n",
      "           1.2779,  1.3550,  1.2738,  1.3206],\n",
      "         [-0.3982, -0.7054,  0.9707, -1.0472, -0.6757, -0.1214, -0.8164,\n",
      "           1.3206,  1.3327,  1.1681,  1.2523],\n",
      "         [ 1.4803,  0.0096, -0.2847,  0.9549,  2.4932,  2.2642,  0.5737,\n",
      "           1.2526,  1.3243,  1.2219,  1.2584],\n",
      "         [-0.6806, -2.3057,  2.9436, -1.0472, -0.7806, -0.4475, -0.8995,\n",
      "           1.2546,  1.2586,  0.8397,  1.0130],\n",
      "         [ 1.3134,  0.0299,  2.8875,  0.9549,  2.4932,  2.2642, -0.6580,\n",
      "           1.0130,  1.0856,  0.8400,  1.0213],\n",
      "         [ 1.9777, -0.0554,  0.0937, -1.0472,  2.4932,  2.2642,  0.2762,\n",
      "           1.0198,  1.1040,  0.9698,  1.0188],\n",
      "         [ 0.1740,  0.3977,  0.2507,  0.9549, -0.3832, -0.1640, -0.5461,\n",
      "           1.0186,  1.0962,  0.9628,  1.0664],\n",
      "         [ 0.1086,  1.0951, -0.8802,  0.9549, -0.6432, -0.8718,  2.3966,\n",
      "           1.0664,  1.2174,  1.0606,  1.1896],\n",
      "         [-0.8142, -1.1374,  0.1731, -1.0472, -0.7914, -0.6200, -0.9155,\n",
      "           1.1896,  1.1896,  1.0190,  1.0716],\n",
      "         [ 0.3998, -0.7047,  0.7826, -1.0472, -0.4533, -0.1947, -0.5962,\n",
      "           1.0716,  1.1084,  0.9201,  1.0000]]])\n",
      "First candle features: [-0.65497     0.41699952  1.8248698   0.9548821  -0.72838676  0.68680996\n",
      " -0.8889625   1.3383768   1.3899248   1.2203763   1.3854109 ]\n",
      "tensor([[[ 0.5089,  0.1722,  0.2428,  0.9549,  0.3193,  0.5880, -0.4178,\n",
      "           1.3652,  1.4226,  1.3150,  1.3866],\n",
      "         [-0.0614,  0.0872,  1.3183,  0.9549,  0.2428,  2.2642, -0.7630,\n",
      "           1.3866,  1.4201,  1.2935,  1.3996],\n",
      "         [-0.0881,  1.7389,  0.0417,  0.9549, -0.7168, -0.7486, -0.5902,\n",
      "           1.3996,  1.5963,  1.3569,  1.5762],\n",
      "         [-0.5663,  0.0570,  1.1744,  0.9549, -0.3510,  2.2642, -0.8619,\n",
      "           1.5762,  1.5932,  1.4880,  1.5864],\n",
      "         [-0.7123, -0.9142,  0.2320, -1.0472, -0.7699, -0.5300, -0.8768,\n",
      "           1.5864,  1.5892,  1.4507,  1.5009],\n",
      "         [-0.4738, -1.2554,  3.6748, -1.0472, -0.7398,  0.1039, -0.8812,\n",
      "           1.5073,  1.5169,  1.1907,  1.3850],\n",
      "         [ 1.1526, -0.5081, -0.1393, -1.0472, -0.0104, -0.4117,  0.1483,\n",
      "           1.3850,  1.4405,  1.3021,  1.3383],\n",
      "         [-0.6550,  0.4170,  1.8249,  0.9549, -0.7284,  0.6868, -0.8890,\n",
      "           1.3384,  1.3899,  1.2204,  1.3854],\n",
      "         [-0.7226, -1.5632,  2.9897, -1.0472, -0.7804, -0.2174, -0.9047,\n",
      "           1.3885,  1.3911,  1.0595,  1.2304],\n",
      "         [ 0.3663,  0.4004,  0.5952,  0.9549, -0.3068,  0.0315, -0.5688,\n",
      "           1.2304,  1.3113,  1.1617,  1.2769],\n",
      "         [ 0.3776,  0.3674, -0.9155,  0.9549, -0.2631, -0.8414,  2.5471,\n",
      "           1.2779,  1.3550,  1.2738,  1.3206],\n",
      "         [-0.3982, -0.7054,  0.9707, -1.0472, -0.6757, -0.1214, -0.8164,\n",
      "           1.3206,  1.3327,  1.1681,  1.2523],\n",
      "         [ 1.4803,  0.0096, -0.2847,  0.9549,  2.4932,  2.2642,  0.5737,\n",
      "           1.2526,  1.3243,  1.2219,  1.2584],\n",
      "         [-0.6806, -2.3057,  2.9436, -1.0472, -0.7806, -0.4475, -0.8995,\n",
      "           1.2546,  1.2586,  0.8397,  1.0130],\n",
      "         [ 1.3134,  0.0299,  2.8875,  0.9549,  2.4932,  2.2642, -0.6580,\n",
      "           1.0130,  1.0856,  0.8400,  1.0213],\n",
      "         [ 1.9777, -0.0554,  0.0937, -1.0472,  2.4932,  2.2642,  0.2762,\n",
      "           1.0198,  1.1040,  0.9698,  1.0188],\n",
      "         [ 0.1740,  0.3977,  0.2507,  0.9549, -0.3832, -0.1640, -0.5461,\n",
      "           1.0186,  1.0962,  0.9628,  1.0664],\n",
      "         [ 0.1086,  1.0951, -0.8802,  0.9549, -0.6432, -0.8718,  2.3966,\n",
      "           1.0664,  1.2174,  1.0606,  1.1896],\n",
      "         [-0.8142, -1.1374,  0.1731, -1.0472, -0.7914, -0.6200, -0.9155,\n",
      "           1.1896,  1.1896,  1.0190,  1.0716],\n",
      "         [ 0.3998, -0.7047,  0.7826, -1.0472, -0.4533, -0.1947, -0.5962,\n",
      "           1.0716,  1.1084,  0.9201,  1.0000]]])\n",
      "First candle features: [ 0.5089243   0.17218113  0.24277814  0.9548821   0.31929123  0.5880437\n",
      " -0.41781333  1.3652353   1.4226291   1.3150496   1.3865651 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import glob\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from servers.pre_process.multi_reg_dif_seq import ServerPreprocess, import_class, build_pipeline_from_config\n",
    "# from models.LSTM.cnn_lstm_mdn import CNNLSTM_MDN  # <-- your updated \"last-output\" model\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# ---------------- Load model and meta ----------------\n",
    "meta_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_meta_multireg_*.pkl\")[0]\n",
    "state_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_model_multireg*.pt\")[0]\n",
    "\n",
    "meta = joblib.load(meta_path)\n",
    "FEATURES = meta['feature_cols']\n",
    "print(\"features\",FEATURES)\n",
    "# ---------------- Model ----------------\n",
    "# Reconstruct model class\n",
    "#for python file:\n",
    "# model_cls_info = meta[\"model_class_info\"]\n",
    "# ModelClass = import_class(model_cls_info[\"module\"], model_cls_info[\"class\"])\n",
    "model_cls_info = meta[\"model_class_info\"]\n",
    "ModelClass = CNNLSTM_MDN\n",
    "# Initialize model with original args\n",
    "model = ModelClass(**model_cls_info[\"init_args\"])\n",
    "model = CNNLSTM_MDN.load_from_checkpoint(state_path)\n",
    "model.eval()\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "df = pd.read_csv( \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles_prop.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "# ---------------- Setup pipeline ----------------\n",
    "pipeline = build_pipeline_from_config(meta[\"pipeline_config\"])\n",
    "pipeline.scalers = meta[\"scalers\"]\n",
    "\n",
    "# Stateful preprocessing instance\n",
    "preproc = ServerPreprocess(feature_pipeline=pipeline)\n",
    "\n",
    "\n",
    "# ---------------- Routes ----------------\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"sequential.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/get_and_add_data\")\n",
    "def get_and_add_data():\n",
    "    dense = df.set_index('timestamp').asfreq('D').ffill()\n",
    "    initial_seq_len = 21\n",
    "    next_idx = request.args.get(\"idx\", type=int)\n",
    "    if next_idx is None:\n",
    "        # First call â†’ load initial candles\n",
    "        if len(preproc.dataset) == 0:\n",
    "            for _, row in dense.iloc[:initial_seq_len].iterrows():\n",
    "                preproc.add_candle(row)\n",
    "        candles = [\n",
    "            {'time': int(ts.timestamp()),\n",
    "             'open': float(row.open),\n",
    "             'high': float(row.high),\n",
    "             'low': float(row.low),\n",
    "             'close': float(row.close)}\n",
    "            for ts, row in dense.iloc[:initial_seq_len].iterrows()\n",
    "        ]\n",
    "        print(\"Returning initial candles:\", candles)\n",
    "\n",
    "        return jsonify({\n",
    "            \"initial_seq_len\": initial_seq_len,\n",
    "            \"next_idx\": initial_seq_len,\n",
    "            \"candles\": candles\n",
    "        })\n",
    "    else:\n",
    "        # Subsequent calls â†’ 1 candle\n",
    "        if next_idx >= len(dense):\n",
    "            print(\"Reached end of data at index:\", next_idx)\n",
    "            return jsonify({\"error\": \"End of data\"}), 404\n",
    "\n",
    "        row = dense.iloc[next_idx]\n",
    "        candle = {\n",
    "            'time': int(row.name.timestamp()),\n",
    "            'open': float(row.open),\n",
    "            'high': float(row.high),\n",
    "            'low': float(row.low),\n",
    "            'close': float(row.close)\n",
    "        }\n",
    "\n",
    "        # âœ… Add to preproc automatically\n",
    "        preproc.add_candle(row)\n",
    "\n",
    "        return jsonify({\n",
    "            \"next_idx\": next_idx + 1,\n",
    "            \"candle\": candle\n",
    "        })\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    seq_len = data.get(\"seq_len\")\n",
    "\n",
    "    if not seq_len or not isinstance(seq_len, int):\n",
    "        return jsonify({\"error\": \"Provide 'seq_len' as an int\"}), 400\n",
    "\n",
    "    try:\n",
    "        # prepare subsequence from current state\n",
    "        seq_df = preproc.prepare_seq(seq_len)\n",
    "    except ValueError as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "    X_np = seq_df[FEATURES].values.astype(np.float32)\n",
    "    X_t = torch.from_numpy(X_np).unsqueeze(0)\n",
    "    print(\"XT\",X_t)\n",
    "    dict_x = {\"main\": X_t}\n",
    "    with torch.no_grad():\n",
    "        mdn_out = model(dict_x)\n",
    "\n",
    "    pi    = mdn_out['pi'][0].cpu().numpy()\n",
    "    mu    = mdn_out['mu'][0].cpu().numpy()\n",
    "    sigma = mdn_out['sigma'][0].cpu().numpy()\n",
    "    last_close =preproc.dataset.iloc[-1]['close']\n",
    "    return jsonify({\n",
    "        'pred_prices': (last_close * mu).tolist(),\n",
    "        'pred_sigmas': (last_close * sigma).tolist(),\n",
    "        'pi': pi.tolist()\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49722e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rllib2.43 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
