{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "label = pd.read_csv(\"/home/iatell/projects/meta-learning/data/seq_line_labels.csv\")\n",
    "label[\"seq_len\"] = label[\"endIndex\"] - label[\"startIndex\"]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e027af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "high",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "volume",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "upper_shadow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "body",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lower_shadow",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Candle_Color",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "upper_body_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lower_body_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "upper_lower_body_ratio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "276b5b3c-e85b-4fb2-8da6-147dc065c148",
       "rows": [
        [
         "0",
         "2018-01-01",
         "13707.91",
         "13818.55",
         "12750.0",
         "13380.0",
         "8607.1564",
         "0.0760030099500053",
         "-0.2252544015971292",
         "0.4327720197804016",
         "1",
         "0.3374096550882847",
         "1.9212588820103085",
         "0.1756190476190467"
        ],
        [
         "1",
         "2018-01-02",
         "13382.16",
         "15473.49",
         "12890.02",
         "14675.11",
         "20078.1654",
         "0.5400711938112452",
         "0.8746274330998404",
         "0.3329124443526468",
         "2",
         "0.6174871418074936",
         "0.3806334351676392",
         "1.622261957979437"
        ],
        [
         "2",
         "2018-01-03",
         "14690.0",
         "15307.56",
         "14150.0",
         "14919.51",
         "15905.4821",
         "0.263643840163752",
         "0.1559306732534026",
         "0.3668797157284533",
         "2",
         "1.6907760010457011",
         "2.352838656267698",
         "0.7186111111111099"
        ],
        [
         "3",
         "2018-01-04",
         "14919.51",
         "15280.0",
         "13918.04",
         "15059.54",
         "25224.415",
         "0.1500060750668053",
         "0.0952796457026441",
         "0.6814233148741451",
         "2",
         "1.5743769192315795",
         "5.0",
         "0.2201363994927449"
        ],
        [
         "4",
         "2018-01-05",
         "15059.56",
         "17176.24",
         "14600.0",
         "16960.39",
         "23251.352",
         "0.144690479328261",
         "1.27418116201777",
         "0.3080563200375024",
         "2",
         "0.1135556572655114",
         "0.2417680697379563",
         "0.4696883975977073"
        ],
        [
         "5",
         "2018-01-06",
         "16960.39",
         "17143.13",
         "16011.21",
         "17069.79",
         "18571.4216",
         "0.0494002646223289",
         "0.0736895139035021",
         "0.639347466242461",
         "2",
         "0.6703839122486213",
         "5.0",
         "0.0772666933563708"
        ],
        [
         "6",
         "2018-01-07",
         "17069.79",
         "17099.96",
         "15610.0",
         "16150.03",
         "12493.3283",
         "0.0203204052621319",
         "-0.6194861101723416",
         "0.3637264982999586",
         "1",
         "0.0328020353135581",
         "0.5871422979907808",
         "0.0558672666333318"
        ],
        [
         "7",
         "2018-01-08",
         "16218.85",
         "16322.3",
         "12812.0",
         "14902.54",
         "26600.3888",
         "0.0678260053683906",
         "-0.863026091120997",
         "1.370642602830709",
         "1",
         "0.0785909094362262",
         "1.588182115155246",
         "0.0494848221033794"
        ],
        [
         "8",
         "2018-01-09",
         "14902.54",
         "15500.0",
         "14011.05",
         "14400.0",
         "14314.7761",
         "0.391905370614885",
         "-0.3296423609091905",
         "0.2551327183420815",
         "1",
         "1.188880487125399",
         "0.7739682413340233",
         "1.5360843296053426"
        ],
        [
         "9",
         "2018-01-10",
         "14401.0",
         "14955.66",
         "13131.31",
         "14907.09",
         "17411.0159",
         "0.0317347758076065",
         "0.3306702221221266",
         "0.8295929070446817",
         "2",
         "0.0959710723389114",
         "2.508822541445198",
         "0.0382534319400796"
        ],
        [
         "10",
         "2018-01-11",
         "14940.0",
         "14968.68",
         "11400.0",
         "13238.78",
         "33554.2284",
         "0.0182528519860017",
         "-1.0827097927345044",
         "1.1702572934037654",
         "1",
         "0.0168584897896805",
         "1.0808596183915082",
         "0.0155972982085949"
        ],
        [
         "11",
         "2018-01-12",
         "13238.76",
         "14109.78",
         "12500.0",
         "13740.01",
         "16417.08194",
         "0.235217906458799",
         "0.3188548979432428",
         "0.4699396397098256",
         "2",
         "0.7376957605985046",
         "1.473835411471322",
         "0.5005279116357143"
        ],
        [
         "12",
         "2018-01-13",
         "13749.95",
         "14580.0",
         "13706.15",
         "14210.0",
         "12221.5188",
         "0.2374735824307884",
         "0.2952695178304975",
         "0.0281117375958616",
         "2",
         "0.804260406477558",
         "0.0952070427127511",
         "5.0"
        ],
        [
         "13",
         "2018-01-14",
         "14210.0",
         "14339.5",
         "12569.2",
         "13474.99",
         "17017.6324",
         "0.0828899370954448",
         "-0.4704628004982467",
         "0.5797751051867408",
         "1",
         "0.176188079073754",
         "1.2323505802642123",
         "0.1429691208779078"
        ],
        [
         "14",
         "2018-01-15",
         "13477.98",
         "14249.99",
         "13147.79",
         "13539.93",
         "14652.0545",
         "0.4571857780873803",
         "0.0398876981558087",
         "0.2125991776281886",
         "2",
         "5.0",
         "5.0",
         "2.150458826736129"
        ],
        [
         "15",
         "2018-01-16",
         "13500.0",
         "13542.93",
         "9035.0",
         "10900.0",
         "63401.866",
         "0.026628097247025",
         "-1.612696315915788",
         "1.156799472762671",
         "1",
         "0.0165115384615385",
         "0.7173076923076923",
         "0.0230187667560323"
        ],
        [
         "16",
         "2018-01-17",
         "10899.99",
         "11680.99",
         "9037.94",
         "10988.79",
         "72330.098",
         "0.4239281967322686",
         "0.0543843164834238",
         "1.14038644716169",
         "2",
         "5.0",
         "5.0",
         "0.3717408232861626"
        ],
        [
         "17",
         "2018-01-18",
         "10972.59",
         "11878.82",
         "10435.33",
         "10961.97",
         "48464.707",
         "0.556297981065909",
         "-0.0065191889022879",
         "0.3232830172787815",
         "1",
         "5.0",
         "5.0",
         "1.7207770013671588"
        ],
        [
         "18",
         "2018-01-19",
         "10960.0",
         "11795.0",
         "10360.0",
         "11474.98",
         "34129.545",
         "0.1969164478876865",
         "0.3168802960227503",
         "0.3691952650853439",
         "2",
         "0.6214221911530559",
         "1.1650937900501002",
         "0.5333666666666673"
        ],
        [
         "19",
         "2018-01-20",
         "11474.98",
         "13099.0",
         "11412.45",
         "12799.94",
         "28768.4576",
         "0.1838802972519933",
         "0.814666082548658",
         "0.0384472513447701",
         "2",
         "0.2257124743388474",
         "0.0471938775510194",
         "4.7826643211259405"
        ],
        [
         "20",
         "2018-01-21",
         "12799.8",
         "12799.8",
         "10965.0",
         "11530.0",
         "41380.038",
         "0.0",
         "-0.778753121487601",
         "0.3465077284930657",
         "1",
         "0.0",
         "0.4449519609387308",
         "0.0"
        ],
        [
         "21",
         "2018-01-22",
         "11530.0",
         "11926.35",
         "9900.24",
         "10760.05",
         "43752.644",
         "0.2419030445007765",
         "-0.4699211533073619",
         "0.524765123482307",
         "1",
         "0.5147736866030262",
         "1.1167088771998166",
         "0.4609739361021628"
        ],
        [
         "22",
         "2018-01-23",
         "10760.05",
         "11399.0",
         "9905.0",
         "10799.18",
         "37474.2905",
         "0.3667329567204366",
         "0.0239242782776017",
         "0.5227818589640378",
         "2",
         "5.0",
         "5.0",
         "0.7015028360914569"
        ],
        [
         "23",
         "2018-01-24",
         "10799.14",
         "11570.48",
         "10500.0",
         "11349.99",
         "27158.7906",
         "0.1357467103297209",
         "0.3391359036016458",
         "0.1841683111616521",
         "2",
         "0.4002723064355081",
         "0.5430516474539334",
         "0.7370796282677015"
        ],
        [
         "24",
         "2018-01-25",
         "11349.96",
         "11794.05",
         "10950.21",
         "11175.27",
         "20840.207",
         "0.2760609679748225",
         "-0.1085930565775436",
         "0.1399047072719807",
         "1",
         "2.5421603984200782",
         "1.288339343980783",
         "1.973207144761386"
        ],
        [
         "25",
         "2018-01-26",
         "11184.7",
         "11643.0",
         "10311.15",
         "11089.0",
         "33056.907",
         "0.2858782400689901",
         "-0.0596957180331717",
         "0.4852070456854996",
         "1",
         "4.788923719958159",
         "5.0",
         "0.5891881468149374"
        ],
        [
         "26",
         "2018-01-27",
         "11089.0",
         "11650.0",
         "10842.69",
         "11491.0",
         "18860.9225",
         "0.1001755492535675",
         "0.253274030188265",
         "0.1551838964568941",
         "2",
         "0.3955223880597014",
         "0.6127114427860684",
         "0.6455279931793282"
        ],
        [
         "27",
         "2018-01-28",
         "11499.98",
         "12244.0",
         "11408.0",
         "11879.95",
         "16887.593",
         "0.2315560729827031",
         "0.2416820795254447",
         "0.0585044021231945",
         "2",
         "0.9581019554175282",
         "0.2420717425059854",
         "3.957925636007839"
        ],
        [
         "28",
         "2018-01-29",
         "11879.95",
         "11975.02",
         "11139.55",
         "11251.0",
         "14170.438",
         "0.0610418921628765",
         "-0.4038318930876343",
         "0.0715590499795167",
         "1",
         "0.1511566897209628",
         "0.1772000953970913",
         "0.8530282637954159"
        ],
        [
         "29",
         "2018-01-30",
         "11250.11",
         "11308.42",
         "9900.0",
         "10237.51",
         "25554.3345",
         "0.0375110737407957",
         "-0.6514099343153862",
         "0.2171216343381256",
         "1",
         "0.0575844361050755",
         "0.3333102903416947",
         "0.1727652513999569"
        ],
        [
         "30",
         "2018-01-31",
         "10230.0",
         "10425.85",
         "9700.0",
         "10285.1",
         "18015.6952",
         "0.0915207973272331",
         "0.0358280350460431",
         "0.3446253824755492",
         "2",
         "2.5544464609800195",
         "5.0",
         "0.265566037735849"
        ],
        [
         "31",
         "2018-02-01",
         "10285.1",
         "10335.0",
         "8750.99",
         "9224.52",
         "33564.9054",
         "0.0324273606349695",
         "-0.6892146321089428",
         "0.3077220056408268",
         "1",
         "0.0470497275075898",
         "0.4464821135605052",
         "0.1053787510822958"
        ],
        [
         "32",
         "2018-02-02",
         "9224.52",
         "9250.0",
         "8010.02",
         "8873.03",
         "49970.757",
         "0.01662266254104",
         "-0.2293053240404337",
         "0.5630111459789321",
         "1",
         "0.0724913937807607",
         "2.455290335429175",
         "0.0295245709783195"
        ],
        [
         "33",
         "2018-02-03",
         "8873.03",
         "9473.01",
         "8229.0",
         "9199.96",
         "28725.049",
         "0.1788064310925536",
         "0.2140896777772864",
         "0.4217421930655082",
         "2",
         "0.8351940782430561",
         "1.9699324014315105",
         "0.4239709330310712"
        ],
        [
         "34",
         "2018-02-04",
         "9199.96",
         "9368.0",
         "7930.0",
         "8184.81",
         "32014.443",
         "0.1101692897958844",
         "-0.6655460279474611",
         "0.1670568717739181",
         "1",
         "0.1655321873614747",
         "0.2510072403093146",
         "0.6594717632745991"
        ],
        [
         "35",
         "2018-02-05",
         "8179.99",
         "8382.8",
         "6625.0",
         "6939.99",
         "63402.168",
         "0.1325608261183738",
         "-0.8104897410718603",
         "0.2058840028550202",
         "1",
         "0.1635564516129028",
         "0.2540241935483869",
         "0.643861709895551"
        ],
        [
         "36",
         "2018-02-06",
         "6939.63",
         "7878.0",
         "6000.01",
         "7652.14",
         "100203.043",
         "0.1469581448945251",
         "0.4636020004374316",
         "0.6113734707597357",
         "2",
         "0.3169920422169508",
         "1.3187464035592478",
         "0.2403737681190265"
        ],
        [
         "37",
         "2018-02-07",
         "7655.02",
         "8476.0",
         "7150.01",
         "7599.0",
         "60777.498",
         "0.5356492803379135",
         "-0.036550309002083",
         "0.2929440064056612",
         "1",
         "5.0",
         "5.0",
         "1.8285039755896568"
        ],
        [
         "38",
         "2018-02-08",
         "7599.0",
         "8280.286",
         "7572.09",
         "8269.3955",
         "19947.321",
         "0.0071827961920388",
         "0.4421573155098645",
         "0.0177484087532961",
         "2",
         "0.0162448882786347",
         "0.0401404842365437",
         "0.4047008547008391"
        ],
        [
         "39",
         "2018-02-09",
         "8250.868",
         "8775.638",
         "8247.343",
         "8764.747",
         "31033.86",
         "0.0072779672878884",
         "0.3434023094235884",
         "0.0023555995491507",
         "2",
         "0.0211937051329232",
         "0.0068595914602457",
         "3.0896453900716496"
        ],
        [
         "40",
         "2018-02-10",
         "8720.666",
         "9065.78",
         "8120.0",
         "8533.98",
         "49473.936",
         "0.2323340969647822",
         "-0.1256788285203355",
         "0.2786953570747059",
         "1",
         "1.8486335343839493",
         "2.217520328251718",
         "0.8336489685492096"
        ],
        [
         "41",
         "2018-02-11",
         "8533.99",
         "8549.0",
         "7726.53",
         "8063.88",
         "47457.823",
         "0.0101958873475835",
         "-0.3193330180527918",
         "0.2291527379551797",
         "1",
         "0.0319286975388743",
         "0.7175980089766238",
         "0.0444938491181271"
        ],
        [
         "42",
         "2018-02-12",
         "8063.82",
         "8989.0",
         "8053.0",
         "8903.0",
         "41987.984",
         "0.058846111468448",
         "0.574214881652235",
         "0.0074036619312626",
         "2",
         "0.1024809933506517",
         "0.0128935389308607",
         "5.0"
        ],
        [
         "43",
         "2018-02-13",
         "8903.0",
         "8950.0",
         "8351.0",
         "8539.9",
         "35455.1325",
         "0.0325441902283817",
         "-0.2514211802537326",
         "0.1307999475349213",
         "1",
         "0.1294409253649131",
         "0.5202423574772775",
         "0.2488088935944949"
        ],
        [
         "44",
         "2018-02-14",
         "8535.17",
         "9489.6",
         "8533.0",
         "9449.99",
         "40812.275",
         "0.0276135947036951",
         "0.6377548272364056",
         "0.0015127871877561",
         "2",
         "0.043298135152271",
         "0.0023720513325026",
         "5.0"
        ],
        [
         "45",
         "2018-02-15",
         "9449.98",
         "10219.5",
         "9301.5",
         "10000.09",
         "52427.447",
         "0.1540681969828395",
         "0.386283468584978",
         "0.1042616375188549",
         "2",
         "0.3988475032266267",
         "0.2699096544327487",
         "1.477707435344831"
        ],
        [
         "46",
         "2018-02-16",
         "10000.89",
         "10323.37",
         "9666.0",
         "10159.98",
         "38161.297",
         "0.1159802105173432",
         "0.1129279129151356",
         "0.2377171962797767",
         "2",
         "1.0270287258784403",
         "2.105034885913628",
         "0.4878915464779525"
        ],
        [
         "47",
         "2018-02-17",
         "10156.07",
         "11075.07",
         "10050.0",
         "11039.55",
         "41882.537",
         "0.0253514964652467",
         "0.630561376607993",
         "0.0757047643600417",
         "2",
         "0.0402046452664468",
         "0.120059310906868",
         "0.3348731969454184"
        ],
        [
         "48",
         "2018-02-18",
         "11039.55",
         "11274.0",
         "10080.0",
         "10383.43",
         "61137.679",
         "0.1678288690562156",
         "-0.4696774474948333",
         "0.2172075655266683",
         "1",
         "0.3573279278180837",
         "0.4624611351582039",
         "0.77266585373892"
        ],
        [
         "49",
         "2018-02-19",
         "10375.01",
         "11250.0",
         "10270.33",
         "11153.0",
         "40831.479",
         "0.0698538791260975",
         "0.560264117745491",
         "0.0753845780094836",
         "2",
         "0.1246802658131852",
         "0.1345518579930337",
         "0.9266335498662563"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 1604
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>upper_shadow</th>\n",
       "      <th>body</th>\n",
       "      <th>lower_shadow</th>\n",
       "      <th>Candle_Color</th>\n",
       "      <th>upper_body_ratio</th>\n",
       "      <th>lower_body_ratio</th>\n",
       "      <th>upper_lower_body_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>13707.91</td>\n",
       "      <td>13818.55</td>\n",
       "      <td>12750.00</td>\n",
       "      <td>13380.00</td>\n",
       "      <td>8607.15640</td>\n",
       "      <td>0.076003</td>\n",
       "      <td>-0.225254</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>1</td>\n",
       "      <td>0.337410</td>\n",
       "      <td>1.921259</td>\n",
       "      <td>0.175619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>13382.16</td>\n",
       "      <td>15473.49</td>\n",
       "      <td>12890.02</td>\n",
       "      <td>14675.11</td>\n",
       "      <td>20078.16540</td>\n",
       "      <td>0.540071</td>\n",
       "      <td>0.874627</td>\n",
       "      <td>0.332912</td>\n",
       "      <td>2</td>\n",
       "      <td>0.617487</td>\n",
       "      <td>0.380633</td>\n",
       "      <td>1.622262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>14690.00</td>\n",
       "      <td>15307.56</td>\n",
       "      <td>14150.00</td>\n",
       "      <td>14919.51</td>\n",
       "      <td>15905.48210</td>\n",
       "      <td>0.263644</td>\n",
       "      <td>0.155931</td>\n",
       "      <td>0.366880</td>\n",
       "      <td>2</td>\n",
       "      <td>1.690776</td>\n",
       "      <td>2.352839</td>\n",
       "      <td>0.718611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>14919.51</td>\n",
       "      <td>15280.00</td>\n",
       "      <td>13918.04</td>\n",
       "      <td>15059.54</td>\n",
       "      <td>25224.41500</td>\n",
       "      <td>0.150006</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>0.681423</td>\n",
       "      <td>2</td>\n",
       "      <td>1.574377</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.220136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>15059.56</td>\n",
       "      <td>17176.24</td>\n",
       "      <td>14600.00</td>\n",
       "      <td>16960.39</td>\n",
       "      <td>23251.35200</td>\n",
       "      <td>0.144690</td>\n",
       "      <td>1.274181</td>\n",
       "      <td>0.308056</td>\n",
       "      <td>2</td>\n",
       "      <td>0.113556</td>\n",
       "      <td>0.241768</td>\n",
       "      <td>0.469688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>2022-05-19</td>\n",
       "      <td>28715.33</td>\n",
       "      <td>30545.18</td>\n",
       "      <td>28691.38</td>\n",
       "      <td>30319.23</td>\n",
       "      <td>67877.36415</td>\n",
       "      <td>0.109006</td>\n",
       "      <td>0.773779</td>\n",
       "      <td>0.011554</td>\n",
       "      <td>2</td>\n",
       "      <td>0.140875</td>\n",
       "      <td>0.014932</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>2022-05-20</td>\n",
       "      <td>30319.22</td>\n",
       "      <td>30777.33</td>\n",
       "      <td>28730.00</td>\n",
       "      <td>29201.01</td>\n",
       "      <td>60517.25325</td>\n",
       "      <td>0.221063</td>\n",
       "      <td>-0.539597</td>\n",
       "      <td>0.227288</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409682</td>\n",
       "      <td>0.421218</td>\n",
       "      <td>0.972612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>29201.01</td>\n",
       "      <td>29656.18</td>\n",
       "      <td>28947.28</td>\n",
       "      <td>29445.06</td>\n",
       "      <td>20987.13124</td>\n",
       "      <td>0.103235</td>\n",
       "      <td>0.119338</td>\n",
       "      <td>0.124071</td>\n",
       "      <td>2</td>\n",
       "      <td>0.865069</td>\n",
       "      <td>1.039664</td>\n",
       "      <td>0.832066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>2022-05-22</td>\n",
       "      <td>29445.07</td>\n",
       "      <td>30487.99</td>\n",
       "      <td>29255.11</td>\n",
       "      <td>30293.94</td>\n",
       "      <td>36158.98748</td>\n",
       "      <td>0.095648</td>\n",
       "      <td>0.418411</td>\n",
       "      <td>0.093632</td>\n",
       "      <td>2</td>\n",
       "      <td>0.228598</td>\n",
       "      <td>0.223780</td>\n",
       "      <td>1.021531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>30293.93</td>\n",
       "      <td>30670.51</td>\n",
       "      <td>30048.77</td>\n",
       "      <td>30472.79</td>\n",
       "      <td>20776.30953</td>\n",
       "      <td>0.098828</td>\n",
       "      <td>0.089401</td>\n",
       "      <td>0.122540</td>\n",
       "      <td>2</td>\n",
       "      <td>1.105446</td>\n",
       "      <td>1.370681</td>\n",
       "      <td>0.806494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1604 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp      open      high       low     close       volume  \\\n",
       "0     2018-01-01  13707.91  13818.55  12750.00  13380.00   8607.15640   \n",
       "1     2018-01-02  13382.16  15473.49  12890.02  14675.11  20078.16540   \n",
       "2     2018-01-03  14690.00  15307.56  14150.00  14919.51  15905.48210   \n",
       "3     2018-01-04  14919.51  15280.00  13918.04  15059.54  25224.41500   \n",
       "4     2018-01-05  15059.56  17176.24  14600.00  16960.39  23251.35200   \n",
       "...          ...       ...       ...       ...       ...          ...   \n",
       "1599  2022-05-19  28715.33  30545.18  28691.38  30319.23  67877.36415   \n",
       "1600  2022-05-20  30319.22  30777.33  28730.00  29201.01  60517.25325   \n",
       "1601  2022-05-21  29201.01  29656.18  28947.28  29445.06  20987.13124   \n",
       "1602  2022-05-22  29445.07  30487.99  29255.11  30293.94  36158.98748   \n",
       "1603  2022-05-23  30293.93  30670.51  30048.77  30472.79  20776.30953   \n",
       "\n",
       "      upper_shadow      body  lower_shadow  Candle_Color  upper_body_ratio  \\\n",
       "0         0.076003 -0.225254      0.432772             1          0.337410   \n",
       "1         0.540071  0.874627      0.332912             2          0.617487   \n",
       "2         0.263644  0.155931      0.366880             2          1.690776   \n",
       "3         0.150006  0.095280      0.681423             2          1.574377   \n",
       "4         0.144690  1.274181      0.308056             2          0.113556   \n",
       "...            ...       ...           ...           ...               ...   \n",
       "1599      0.109006  0.773779      0.011554             2          0.140875   \n",
       "1600      0.221063 -0.539597      0.227288             1          0.409682   \n",
       "1601      0.103235  0.119338      0.124071             2          0.865069   \n",
       "1602      0.095648  0.418411      0.093632             2          0.228598   \n",
       "1603      0.098828  0.089401      0.122540             2          1.105446   \n",
       "\n",
       "      lower_body_ratio  upper_lower_body_ratio  \n",
       "0             1.921259                0.175619  \n",
       "1             0.380633                1.622262  \n",
       "2             2.352839                0.718611  \n",
       "3             5.000000                0.220136  \n",
       "4             0.241768                0.469688  \n",
       "...                ...                     ...  \n",
       "1599          0.014932                5.000000  \n",
       "1600          0.421218                0.972612  \n",
       "1601          1.039664                0.832066  \n",
       "1602          0.223780                1.021531  \n",
       "1603          1.370681                0.806494  \n",
       "\n",
       "[1604 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles_prop.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b8d98",
   "metadata": {},
   "source": [
    "# model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ccb9c",
   "metadata": {},
   "source": [
    "## Hungarian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159560e2",
   "metadata": {},
   "source": [
    "### Hungarian lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16033bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "class LSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # Predict max_len_y candidate values\n",
    "        self.fc_reg = nn.Linear(hidden_dim, max_len_y)\n",
    "        self.lr = lr\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")  # per-element loss\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x[\"main\"]\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        last_h = hn[-1]\n",
    "\n",
    "        y_pred = self.fc_reg(last_h)  # (B, max_len_y)\n",
    "        return y_pred\n",
    "\n",
    "    def hungarian_loss(self, y_pred, y_true, mask):\n",
    "        \"\"\"\n",
    "        Hungarian matching loss.\n",
    "        y_pred: (B, max_len_y)\n",
    "        y_true: (B, max_len_y)\n",
    "        mask:   (B, max_len_y), 1 where valid target, 0 where padding\n",
    "        \"\"\"\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            gt_vals = y_true[i][mask[i] > 0]  # (L,)\n",
    "            preds = y_pred[i]                 # (max_len_y,)\n",
    "\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            # Build cost matrix (L x max_len_y)\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)  # squared error\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "\n",
    "            # Hungarian assignment\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            # Compute loss only for assigned pairs\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "            loss = self.loss_fn_reg(matched_preds, matched_gts).sum()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_count += len(gt_vals)\n",
    "\n",
    "        return total_loss / max(total_count, 1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "\n",
    "        mask = (y != 0).float()  # assume padding = 0\n",
    "\n",
    "        # Hungarian matching loss\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss_reg, prog_bar=True)\n",
    "        return loss_reg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52405383",
   "metadata": {},
   "source": [
    "### Hungarian lstm order weightening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6785b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "class LSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # Predict max_len_y candidate values\n",
    "        self.fc_reg = nn.Linear(hidden_dim, max_len_y)\n",
    "        self.lr = lr\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")  # per-element loss\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x[\"main\"]\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        last_h = hn[-1]\n",
    "\n",
    "        y_pred = self.fc_reg(last_h)  # (B, max_len_y)\n",
    "        return y_pred\n",
    "\n",
    "    def hungarian_loss(self, y_pred, y_true, mask):\n",
    "        \"\"\"\n",
    "        Hungarian matching loss with position-based weighting.\n",
    "        Earlier ground-truth positions in y_true get higher weight.\n",
    "        \"\"\"\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            # Extract ground-truth values and their positions\n",
    "            gt_vals = y_true[i][mask[i] > 0]  # (L,)\n",
    "            gt_indices = torch.nonzero(mask[i] > 0, as_tuple=False).squeeze(1)  # positions in y_true\n",
    "\n",
    "            preds = y_pred[i]  # (max_len_y,)\n",
    "\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            # Cost matrix (L x max_len_y) using squared error\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "\n",
    "            # Hungarian assignment\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            # Matched pairs\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "\n",
    "            # --- weighting logic ---\n",
    "            # Lower index = higher weight (inverse rank)\n",
    "            gt_pos = gt_indices[row_ind]  # actual positions of matched gts\n",
    "            weights = 1.0 / (1.0 + gt_pos.float())  # e.g. pos=0 -> 1.0, pos=2 -> 0.33\n",
    "\n",
    "            # Compute weighted MSE\n",
    "            loss = (weights * self.loss_fn_reg(matched_preds, matched_gts)).sum()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_count += weights.sum().item()\n",
    "\n",
    "        return total_loss / max(total_count, 1.0)\n",
    "    \n",
    "    def hungarian_loss_unweighted(self, y_pred, y_true, mask):\n",
    "        \"\"\"\n",
    "        Same Hungarian matching but without weights (baseline).\n",
    "        \"\"\"\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            gt_vals = y_true[i][mask[i] > 0]\n",
    "            preds = y_pred[i]\n",
    "\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "\n",
    "            loss = self.loss_fn_reg(matched_preds, matched_gts).sum()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_count += len(gt_vals)\n",
    "\n",
    "        return total_loss / max(total_count, 1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "\n",
    "        mask = (y != 0).float()  # assume padding = 0\n",
    "\n",
    "        # Hungarian matching loss (weighted)\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "\n",
    "        # Log both weighted and unweighted (for comparison/debug)\n",
    "        unweighted_loss = self.hungarian_loss_unweighted(y_pred, y, mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss_reg, prog_bar=True)              # weighted\n",
    "        self.log(\"train_loss_unweighted\", unweighted_loss)           # reference\n",
    "        return loss_reg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d451064",
   "metadata": {},
   "source": [
    "### CNN -attention lstm hungarian - concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d25a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# --- Sinusoidal positional encoding ---\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, d_model)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # nn.Embedding is a perfect layer for this.\n",
    "        # It's a lookup table that stores embeddings of a fixed size.\n",
    "        self.embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, d_model)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Create a tensor of positions [0, 1, 2, ..., T-1]\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0) # (1, T)\n",
    "        \n",
    "        # Look up the embeddings for these positions\n",
    "        positional_encodings = self.embedding(positions) # (1, T, d_model)\n",
    "        \n",
    "        # Add to the input tensor\n",
    "        return x + positional_encodings\n",
    "\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        if seq_len != self.seq_len_cached:\n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            self.cos_cached = emb.cos()[:, None, None, :]\n",
    "            self.sin_cached = emb.sin()[:, None, None, :]\n",
    "        return self.cos_cached, self.sin_cached\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=x1.ndim - 1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # q, k have shape (B, H, T, head_dim)\n",
    "    # cos, sin have shape (T, 1, 1, head_dim)\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "# --- How to use it in your Transformer ---\n",
    "# self.rope = RotaryPositionalEncoding(dim=head_dim)\n",
    "#\n",
    "# def forward(self, x):\n",
    "#     q, k, v = self.to_qkv(x)\n",
    "#     cos, sin = self.rope(q)\n",
    "#     q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "#     # ... proceed with attention calculation using the new q and k\n",
    "\n",
    "# --- CNN + Transformer Regressor ---\n",
    "class CNNAttentionTransformerRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, nhead=4, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # --- Multi-branch CNN ---\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=k, padding=\"same\"),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ) for k in [3, 5, 7, 11]\n",
    "        ])\n",
    "\n",
    "        # --- Conv2d fusion ---\n",
    "        self.fusion_conv2d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=1, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # --- Positional encoding ---\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(d_model=32)\n",
    "\n",
    "        # --- Transformer encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=32,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.3,\n",
    "            activation=\"relu\",\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- Regressor ---\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, max_len_y)\n",
    "        )\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")\n",
    "        self.lr = lr\n",
    "\n",
    "    # --- Forward ---\n",
    "    def forward(self, x, lengths):\n",
    "        # x[\"main\"]: (B, T, input_dim)\n",
    "        x = x[\"main\"].transpose(1, 2)  # (B, input_dim, T)\n",
    "\n",
    "        # Multi-branch CNN\n",
    "        branch_outputs = [branch(x) for branch in self.branches]  # list of (B, 32, T)\n",
    "        stacked = torch.stack(branch_outputs, dim=1)               # (B, 4, 32, T)\n",
    "\n",
    "        # Conv2d fusion\n",
    "        fused = self.fusion_conv2d(stacked)                        # (B, 1, 32, T)\n",
    "        fused = fused.squeeze(1)                                   # (B, 32, T)\n",
    "        fused = fused.transpose(1, 2)                               # (B, T, 32)\n",
    "\n",
    "        # Positional encoding\n",
    "        fused = self.positional_encoding(fused)                    # (B, T, 32)\n",
    "\n",
    "        # Padding mask for transformer\n",
    "        max_len = fused.size(1)\n",
    "        mask = torch.arange(max_len, device=lengths.device)[None, :] >= lengths[:, None]  # True=masked\n",
    "\n",
    "        # Transformer encoder\n",
    "        transformer_out = self.transformer(fused, src_key_padding_mask=mask)  # (B, T, 32)\n",
    "\n",
    "        # Masked mean pooling over sequence\n",
    "        seq_mask = ~mask\n",
    "        pooled = (transformer_out * seq_mask.unsqueeze(-1)).sum(1) / seq_mask.sum(1, keepdim=True)  # (B, 32)\n",
    "\n",
    "        # Regression\n",
    "        y_pred = self.regressor(pooled)  # (B, max_len_y)\n",
    "        return y_pred\n",
    "\n",
    "    # --- Hungarian weighted loss ---\n",
    "    def hungarian_loss(self, y_pred, y_true, mask):\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            gt_vals = y_true[i][mask[i] > 0]\n",
    "            gt_indices = torch.nonzero(mask[i] > 0, as_tuple=False).squeeze(1)\n",
    "            preds = y_pred[i]\n",
    "\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "\n",
    "            gt_pos = gt_indices[row_ind]\n",
    "            weights = 1.0 / (1.0 + gt_pos.float())\n",
    "\n",
    "            loss = (weights * self.loss_fn_reg(matched_preds, matched_gts)).sum()\n",
    "            total_loss += loss\n",
    "            total_count += weights.sum().item()\n",
    "\n",
    "        return total_loss / max(total_count, 1.0)\n",
    "\n",
    "    # --- Hungarian unweighted loss ---\n",
    "    def hungarian_loss_unweighted(self, y_pred, y_true, mask):\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            gt_vals = y_true[i][mask[i] > 0]\n",
    "            preds = y_pred[i]\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "\n",
    "            loss = self.loss_fn_reg(matched_preds, matched_gts).sum()\n",
    "            total_loss += loss\n",
    "            total_count += len(gt_vals)\n",
    "\n",
    "        return total_loss / max(total_count, 1)\n",
    "\n",
    "    # --- Training step ---\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "\n",
    "        mask = (y != 0).float()  # assume padding = 0\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "        unweighted_loss = self.hungarian_loss_unweighted(y_pred, y, mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss_reg, prog_bar=True)\n",
    "        self.log(\"train_loss_unweighted\", unweighted_loss)\n",
    "        return loss_reg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5ef5c",
   "metadata": {},
   "source": [
    "depricated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn.functional as F\n",
    "from utils.load_attention import  load_attention\n",
    "from importlib import import_module\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn_layer = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.v_context = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_outputs, lengths):\n",
    "        energy = torch.tanh(self.attn_layer(lstm_outputs))\n",
    "        attn_scores = self.v_context(energy).squeeze(2)\n",
    "        mask = torch.arange(\n",
    "            lstm_outputs.size(1), device=lstm_outputs.device\n",
    "        )[None, :] < lengths[:, None]\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e10)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(1), lstm_outputs).squeeze(1)\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class CNNAttentionLSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Multi-branch 1D convolutions\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=k, padding=\"same\"),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ) for k in [3, 5, 7, 11]\n",
    "        ])\n",
    "\n",
    "        # Fusion with Conv2d over (branches × seq)\n",
    "        self.fusion_conv2d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=1, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # LSTM takes feature_dim = 32 after fusion\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, max_len_y)\n",
    "        )\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Input: x[\"main\"] → (B, T, input_dim)\n",
    "        x = x[\"main\"].transpose(1, 2)  # (B, input_dim, T)\n",
    "\n",
    "        # Branch outputs\n",
    "        branch_outputs = [branch(x) for branch in self.branches]  # list of (B, 32, T)\n",
    "        stacked = torch.stack(branch_outputs, dim=1)  # (B, 4, 32, T)\n",
    "\n",
    "        # Fusion conv2d\n",
    "        fused = self.fusion_conv2d(stacked)  # (B, 1, 32, T)\n",
    "        fused = fused.squeeze(1)             # (B, 32, T)\n",
    "\n",
    "        # Prepare for LSTM\n",
    "        lstm_input = fused.transpose(1, 2)   # (B, T, 32)\n",
    "\n",
    "        # LSTM with packing\n",
    "        packed_input = pack_padded_sequence(lstm_input, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        lstm_outputs, _ = pad_packed_sequence(packed_output, batch_first=True)  # (B, T, H)\n",
    "\n",
    "        # Attention\n",
    "        context_vector = self.attention(lstm_outputs, lengths)  # (B, H)\n",
    "\n",
    "        # Regression\n",
    "        y_pred = self.regressor(context_vector)  # (B, max_len_y)\n",
    "        return y_pred\n",
    "\n",
    "    def hungarian_loss(self, y_pred, y_true, mask):\n",
    "        \"\"\"\n",
    "        Hungarian matching loss with position-based weighting.\n",
    "        Earlier ground-truth positions in y_true get higher weight.\n",
    "        \"\"\"\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            # Extract ground-truth values and their positions\n",
    "            gt_vals = y_true[i][mask[i] > 0]  # (L,)\n",
    "            gt_indices = torch.nonzero(mask[i] > 0, as_tuple=False).squeeze(1)  # positions in y_true\n",
    "\n",
    "            preds = y_pred[i]  # (max_len_y,)\n",
    "\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            # Cost matrix (L x max_len_y) using squared error\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "\n",
    "            # Hungarian assignment\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            # Matched pairs\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "\n",
    "            # --- weighting logic ---\n",
    "            # Lower index = higher weight (inverse rank)\n",
    "            gt_pos = gt_indices[row_ind]  # actual positions of matched gts\n",
    "            weights = 1.0 / (1.0 + gt_pos.float())  # e.g. pos=0 -> 1.0, pos=2 -> 0.33\n",
    "\n",
    "            # Compute weighted MSE\n",
    "            loss = (weights * self.loss_fn_reg(matched_preds, matched_gts)).sum()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_count += weights.sum().item()\n",
    "\n",
    "        return total_loss / max(total_count, 1.0)\n",
    "    \n",
    "    def hungarian_loss_unweighted(self, y_pred, y_true, mask):\n",
    "        \"\"\"\n",
    "        Same Hungarian matching but without weights (baseline).\n",
    "        \"\"\"\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            gt_vals = y_true[i][mask[i] > 0]\n",
    "            preds = y_pred[i]\n",
    "\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "\n",
    "            loss = self.loss_fn_reg(matched_preds, matched_gts).sum()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_count += len(gt_vals)\n",
    "\n",
    "        return total_loss / max(total_count, 1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "\n",
    "        mask = (y != 0).float()  # assume padding = 0\n",
    "\n",
    "        # Hungarian matching loss (weighted)\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "\n",
    "        # Log both weighted and unweighted (for comparison/debug)\n",
    "        unweighted_loss = self.hungarian_loss_unweighted(y_pred, y, mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss_reg, prog_bar=True)              # weighted\n",
    "        self.log(\"train_loss_unweighted\", unweighted_loss)           # reference\n",
    "        return loss_reg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Import optimizer dynamically\n",
    "        opt_module = import_module(f\"model.optimizer.{self.optimizer_name}\")\n",
    "        optimizer = opt_module.build(self, self.lr)\n",
    "\n",
    "        # No scheduler\n",
    "        if self.scheduler_name is None:\n",
    "            return optimizer\n",
    "\n",
    "        # Import scheduler dynamically\n",
    "        sch_module = import_module(f\"model.schedulers.{self.scheduler_name}\")\n",
    "        # OneCycle needs trainer\n",
    "        if self.scheduler_name == \"onecycle\":\n",
    "            scheduler = sch_module.build(optimizer, self.lr, self.trainer)\n",
    "        else:\n",
    "            scheduler = sch_module.build(optimizer)\n",
    "\n",
    "        # Lightning accepts dict or list depending on scheduler type\n",
    "        if isinstance(scheduler, dict):\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        elif isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler):\n",
    "            return [optimizer], [scheduler]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scheduler return type: {type(scheduler)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541ef27",
   "metadata": {},
   "source": [
    "### CNN simple attention lstm weightening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef21c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn.functional as F\n",
    "from utils.load_attention import  load_attention\n",
    "from utils.load_class import load_class\n",
    "from importlib import import_module\n",
    "from models.losses.hungarian_loss import hungarian_loss_weighted\n",
    "from models.losses.hungarian_loss_unweighted import hungarian_loss_unweighted\n",
    "\n",
    "class CNNAttentionLSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001, attention_name=\"tanh_attention\",optimizer_name=\"adamw\",kernels= [3, 5, 7, 11],\n",
    "    cnn_out_channels=32,first_drop= 0.3, second_drop=0.3, third_drop= 0.3,scheduler_name=None, scheduler_params=None, optimizer_params=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.scheduler_name = scheduler_name or None\n",
    "        self.optimizer_params = optimizer_params or {}\n",
    "        self.scheduler_params = scheduler_params or {}\n",
    "        self.kernels = kernels\n",
    "        # Multi-branch 1D convolutions\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=input_dim[\"candle_shape\"], out_channels=cnn_out_channels, kernel_size=k, padding=\"same\"),\n",
    "                nn.BatchNorm1d(cnn_out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ) for k in self.kernels\n",
    "        ])\n",
    "        # Fusion with Conv2d over (branches × seq)\n",
    "        self.fusion_conv2d = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=1, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # LSTM takes feature_dim = 32 after fusion\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_out_channels,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.attention = load_class(f\"models.attention.{attention_name}\", hidden_dim=hidden_dim)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, max_len_y)\n",
    "        )\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")\n",
    "        self.lr = lr\n",
    "        self.hungarian_loss = hungarian_loss_weighted\n",
    "        self.hungarian_loss_unweighted = hungarian_loss_unweighted\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # Input: x[\"main\"] → (B, T, input_dim)\n",
    "        x = x[\"main\"].transpose(1, 2)  # (B, input_dim, T)\n",
    "        # Branch outputs\n",
    "        branch_outputs = [branch(x) for branch in self.branches]  # list of (B, 32, T)\n",
    "        stacked = torch.stack(branch_outputs, dim=1)  # (B, 4, 32, T)\n",
    "        # Fusion conv2d\n",
    "        fused = self.fusion_conv2d(stacked)  # (B, 1, 32, T)\n",
    "        fused = fused.squeeze(1)             # (B, 32, T)\n",
    "        # Prepare for LSTM\n",
    "        lstm_input = fused.transpose(1, 2)   # (B, T, 32)\n",
    "        # LSTM with packing\n",
    "        packed_input = pack_padded_sequence(lstm_input, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        lstm_outputs, _ = pad_packed_sequence(packed_output, batch_first=True)  # (B, T, H)\n",
    "        # Attention\n",
    "        context_vector = self.attention(lstm_outputs, lengths)  # (B, H)\n",
    "        # Regression\n",
    "        y_pred = self.regressor(context_vector)  # (B, max_len_y)\n",
    "        return y_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "\n",
    "        mask = (y != 0).float()  # assume padding = 0\n",
    "\n",
    "        # Hungarian matching loss (weighted)\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "\n",
    "        # Log both weighted and unweighted (for comparison/debug)\n",
    "        unweighted_loss = self.hungarian_loss_unweighted(y_pred, y, mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss_reg, prog_bar=True)              # weighted\n",
    "        self.log(\"train_loss_unweighted\", unweighted_loss)           # reference\n",
    "        return loss_reg\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "        mask = (y != 0).float()\n",
    "\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "        self.log(\"val_loss\", loss_reg, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss_reg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Import optimizer dynamically\n",
    "        opt_module = import_module(f\"models.optimizer.{self.optimizer_name}\")\n",
    "        optimizer = opt_module.build(self, self.lr)\n",
    "\n",
    "        # No scheduler\n",
    "        if self.scheduler_name is None:\n",
    "            return optimizer\n",
    "\n",
    "        # Import scheduler dynamically\n",
    "        sch_module = import_module(f\"models.schedulers.{self.scheduler_name}\")\n",
    "        # OneCycle needs trainer\n",
    "        if self.scheduler_name == \"onecycle\":\n",
    "            scheduler = sch_module.build(optimizer, self.lr, self.trainer)\n",
    "        else:\n",
    "            scheduler = sch_module.build(optimizer)\n",
    "\n",
    "        # Lightning accepts dict or list depending on scheduler type\n",
    "        if isinstance(scheduler, dict):\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        elif isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler):\n",
    "            return [optimizer], [scheduler]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scheduler return type: {type(scheduler)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c47cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn.functional as F\n",
    "from utils.load_attention import load_attention\n",
    "from utils.load_class import load_class\n",
    "from importlib import import_module\n",
    "from models.losses.hungarian_loss import hungarian_loss_weighted\n",
    "from models.losses.hungarian_loss_unweighted import hungarian_loss_unweighted\n",
    "from typing import Optional\n",
    "\n",
    "class CNNAttentionLSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001, attention_name=\"tanh_attention\",optimizer_name=\"adamw\",kernels= [3, 5, 7, 11],fusion_out_channels = 10,\n",
    "    cnn_out_channels=32,first_drop= 0.3, second_drop=0.3, third_drop= 0.3,scheduler_name=None, scheduler_params=None, optimizer_params=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.scheduler_name = scheduler_name or None\n",
    "        self.optimizer_params = optimizer_params or {}\n",
    "        self.scheduler_params = scheduler_params or {}\n",
    "        self.input_dim = input_dim[\"candle_shape\"]\n",
    "        self.cnn_out_channels = cnn_out_channels\n",
    "        self.kernels = kernels\n",
    "        self.num_branches = len(kernels)\n",
    "        self.fusion_out_channels = fusion_out_channels\n",
    "        self.main_feat_dim = input_dim['main']\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_len_y = max_len_y\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")\n",
    "        self.lr = lr\n",
    "        self.attention = load_class(f\"models.attention.{attention_name}\", hidden_dim=hidden_dim)\n",
    "        self.hungarian_loss = hungarian_loss_weighted\n",
    "        self.hungarian_loss_unweighted = hungarian_loss_unweighted\n",
    "        # ----- Branches: multiple Conv1d with different kernel sizes -----\n",
    "        branches = []\n",
    "        for k in kernels:\n",
    "            pad = (k - 1) // 2  # 'same' padding for odd kernels; for even kernels behavior approximated\n",
    "            branches.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(in_channels=self.input_dim , out_channels=cnn_out_channels, kernel_size=k, padding=pad),\n",
    "                    nn.BatchNorm1d(cnn_out_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(first_drop)\n",
    "                )\n",
    "            )\n",
    "        self.branches = nn.ModuleList(branches)\n",
    "\n",
    "        # ----- Fusion Conv2d: we will stack branch outputs into shape (B, num_branches, C, T)\n",
    "        # in_channels should equal number of branches.\n",
    "        # Kernel height must be cnn_out_channels to cover \"full feature height\".\n",
    "        self.fusion_conv2d = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.num_branches,\n",
    "                out_channels=self.fusion_out_channels,\n",
    "                kernel_size=(self.cnn_out_channels, 1),\n",
    "                padding=(0, 0)\n",
    "            ),\n",
    "            nn.BatchNorm2d(self.fusion_out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(second_drop)\n",
    "        )\n",
    "\n",
    "        # After fusion we will have (B, fusion_out_channels, 1, T) -> squeeze -> (B, fusion_out_channels, T)\n",
    "\n",
    "        # ----- LSTM: input_size should be fusion_out_channels + main_feat_dim -----\n",
    "        lstm_input_size = self.fusion_out_channels + self.main_feat_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=third_drop if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        # Regressor (maps attention context to target length)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(third_drop),\n",
    "            nn.Linear(hidden_dim, max(4, hidden_dim // 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(max(4, hidden_dim // 2), max_len_y)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: dict, lengths):\n",
    "        \"\"\"\n",
    "        x is a dict with at least:\n",
    "          - x[\"candle_shape\"]: Tensor shape (B, input_dim, T)\n",
    "          - x[\"main\"]: Tensor shape (B, main_feat_dim, T)\n",
    "\n",
    "        Returns:\n",
    "          - out: (B, max_len_y)\n",
    "          - optionally attention weights if you want them (we return both)\n",
    "        \"\"\"\n",
    "        candle = x[\"candle_shape\"]\n",
    "        candle = candle.permute(0, 2, 1) \n",
    "        main = x[\"main\"]\n",
    "        main = main.permute(0, 2, 1)\n",
    "        # ---- Validate shapes ----\n",
    "        B, _, T = candle.shape\n",
    "        assert main.shape[0] == B and main.shape[2] == T, \\\n",
    "            f\"main must match batch and time dims, got {main.shape} vs candle {candle.shape}\"\n",
    "        # ---- Branches: each branch returns (B, C, T) ----\n",
    "        branch_feats = [branch(candle) for branch in self.branches]  # list of (B, C, T)\n",
    "        # stack into (B, num_branches, C, T)\n",
    "        stacked = torch.stack(branch_feats, dim=1)\n",
    "\n",
    "        # ---- Fusion Conv2d expects (B, in_channels=num_branches, height=C, width=T) ----\n",
    "        fused = self.fusion_conv2d(stacked)  # -> (B, fusion_out_channels, 1, T)\n",
    "        fused = fused.squeeze(2)  # -> (B, fusion_out_channels, T)\n",
    "\n",
    "        # ---- Concatenate with main features along channel dimension -> (B, fusion_out + m, T) ----\n",
    "        combined = torch.cat([fused, main], dim=1)\n",
    "\n",
    "        # ---- Prepare for LSTM: LSTM batch_first expects (B, T, feat) ----\n",
    "        combined_t = combined.permute(0, 2, 1)  # (B, T, feat_dim)\n",
    "        packed_input = pack_padded_sequence(combined_t, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        # LSTM\n",
    "        packed_output, _ = self.lstm(packed_input)  # lstm_out: (B, T, hidden_dim)\n",
    "        lstm_outputs, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Attention over LSTM outputs\n",
    "        context= self.attention(lstm_outputs,lengths)  # context: (B, hidden_dim)\n",
    "\n",
    "        # Regressor -> (B, max_len_y)\n",
    "        y_pred = self.regressor(context)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    # ---------------------------\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "\n",
    "        mask = (y != 0).float()  # assume padding = 0\n",
    "\n",
    "        # Hungarian matching loss (weighted)\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "\n",
    "        # Log both weighted and unweighted (for comparison/debug)\n",
    "        unweighted_loss = self.hungarian_loss_unweighted(y_pred, y, mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss_reg, prog_bar=True)              # weighted\n",
    "        self.log(\"train_loss_unweighted\", unweighted_loss)           # reference\n",
    "        return loss_reg\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "        mask = (y != 0).float()\n",
    "\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "        self.log(\"val_loss\", loss_reg, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss_reg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Import optimizer dynamically\n",
    "        opt_module = import_module(f\"models.optimizer.{self.optimizer_name}\")\n",
    "        optimizer = opt_module.build(self, self.lr)\n",
    "\n",
    "        # No scheduler\n",
    "        if self.scheduler_name is None:\n",
    "            return optimizer\n",
    "\n",
    "        # Import scheduler dynamically\n",
    "        sch_module = import_module(f\"models.schedulers.{self.scheduler_name}\")\n",
    "        # OneCycle needs trainer\n",
    "        if self.scheduler_name == \"onecycle\":\n",
    "            scheduler = sch_module.build(optimizer, self.lr, self.trainer)\n",
    "        else:\n",
    "            scheduler = sch_module.build(optimizer)\n",
    "\n",
    "        # Lightning accepts dict or list depending on scheduler type\n",
    "        if isinstance(scheduler, dict):\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        elif isinstance(scheduler, torch.optim.lr_scheduler._LRScheduler):\n",
    "            return [optimizer], [scheduler]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scheduler return type: {type(scheduler)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567ad84",
   "metadata": {},
   "source": [
    "### CNN LSTM Hungarian weightening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f191e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "class CNNLSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Conv1d branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=k, padding=k // 2),\n",
    "                nn.BatchNorm1d(32),      # normalize per branch\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)          # regularize per branch\n",
    "            )\n",
    "            for k in [1, 3, 7, 10]\n",
    "        ])\n",
    "\n",
    "        # Conv2d fuse across branch dimension\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=4, out_channels=1, kernel_size=(1, 3), padding=(0, 1)\n",
    "        )\n",
    "        self.bn2d = nn.BatchNorm2d(1)   # normalize conv2d output\n",
    "        self.dropout2d = nn.Dropout(0.3)\n",
    "\n",
    "        self.lstm_input_dim = 32  # after conv2d → (B, 32, T)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0.0   # built-in LSTM dropout\n",
    "        )\n",
    "\n",
    "        self.dropout_fc = nn.Dropout(0.3)\n",
    "        self.fc_reg = nn.Linear(hidden_dim, max_len_y)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x[\"main\"]  # (B, T, F)\n",
    "        B, T, F = x.shape\n",
    "\n",
    "        # Conv1d branches\n",
    "        feats = [branch(x.transpose(1, 2)) for branch in self.branches]  # (B, 32, T) each\n",
    "        fusion = torch.stack(feats, dim=1)                               # (B, 4, 32, T)\n",
    "\n",
    "        # Conv2d fusion\n",
    "        fusion2d = self.conv2d(fusion)                                   # (B, 1, 32, T)\n",
    "        fusion2d = self.bn2d(fusion2d)\n",
    "        fusion2d = F.relu(fusion2d)\n",
    "        fusion2d = self.dropout2d(fusion2d)\n",
    "        fusion2d = fusion2d.squeeze(1)                                   # (B, 32, T)\n",
    "\n",
    "        # LSTM\n",
    "        fusion2d = fusion2d.transpose(1, 2)                              # (B, T, 32)\n",
    "        packed = pack_padded_sequence(fusion2d, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        last_h = hn[-1]\n",
    "\n",
    "        # Fully connected with dropout\n",
    "        last_h = self.dropout_fc(last_h)\n",
    "        y_pred = self.fc_reg(last_h)  # (B, max_len_y)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------- Hungarian Losses (same as your code) -------------------\n",
    "    def hungarian_loss(self, y_pred, y_true, mask):\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            gt_vals = y_true[i][mask[i] > 0]\n",
    "            gt_indices = torch.nonzero(mask[i] > 0, as_tuple=False).squeeze(1)\n",
    "\n",
    "            preds = y_pred[i]\n",
    "\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "\n",
    "            gt_pos = gt_indices[row_ind]\n",
    "            weights = 1.0 / (1.0 + gt_pos.float())\n",
    "\n",
    "            loss = (weights * self.loss_fn_reg(matched_preds, matched_gts)).sum()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_count += weights.sum().item()\n",
    "\n",
    "        return total_loss / max(total_count, 1.0)\n",
    "\n",
    "    def hungarian_loss_unweighted(self, y_pred, y_true, mask):\n",
    "        B, max_len = y_true.shape\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i in range(B):\n",
    "            gt_vals = y_true[i][mask[i] > 0]\n",
    "            preds = y_pred[i]\n",
    "\n",
    "            if len(gt_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "            cost = cost.detach().cpu().numpy()\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "            matched_preds = preds[col_ind]\n",
    "            matched_gts = gt_vals[row_ind]\n",
    "\n",
    "            loss = self.loss_fn_reg(matched_preds, matched_gts).sum()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_count += len(gt_vals)\n",
    "\n",
    "        return total_loss / max(total_count, 1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred = self(X, lengths)\n",
    "        mask = (y != 0).float()\n",
    "\n",
    "        loss_reg = self.hungarian_loss(y_pred, y, mask)\n",
    "        unweighted_loss = self.hungarian_loss_unweighted(y_pred, y, mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss_reg, prog_bar=True)\n",
    "        self.log(\"train_loss_unweighted\", unweighted_loss)\n",
    "        return loss_reg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b0b0f",
   "metadata": {},
   "source": [
    "### transformer Hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31244e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Injects position information into the input sequence.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Sequence Length, Batch Size, Feature Dim)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "class TransformerRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_encoder_layers, max_len_y, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 1. CNN Feature Extractor (same as before)\n",
    "        self.branches = nn.ModuleList([...]) # Your Conv1D branches\n",
    "        self.fusion_conv = nn.Sequential([...]) # Your 1x1 fusion conv\n",
    "        \n",
    "        # We need to ensure the output dim of fusion_conv matches model_dim\n",
    "        # Let's assume fusion_conv outputs `model_dim` channels\n",
    "\n",
    "        # 2. Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model=model_dim)\n",
    "\n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # 4. Final Regressor Head\n",
    "        self.regressor = nn.Linear(model_dim, max_len_y)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x[\"main\"].transpose(1, 2)\n",
    "        \n",
    "        # --- CNN Part ---\n",
    "        branch_outputs = [branch(x) for branch in self.branches]\n",
    "        fused_features = torch.cat(branch_outputs, dim=1)\n",
    "        fused_features = self.fusion_conv(fused_features) # (B, model_dim, T)\n",
    "        \n",
    "        # --- Transformer Part ---\n",
    "        # Reshape for Transformer: (B, T, F)\n",
    "        transformer_input = fused_features.transpose(1, 2)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        # Note: Pytorch's default Transformer expects (T, B, F) or batch_first=True\n",
    "        # We used batch_first=True, so shape is (B, T, F)\n",
    "        transformer_input = self.pos_encoder(transformer_input.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        # Create padding mask for the Transformer\n",
    "        # (B, T) -> True for positions that should be ignored\n",
    "        padding_mask = (torch.arange(x.size(2), device=x.device)[None, :] >= lengths[:, None])\n",
    "\n",
    "        # Pass through Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(transformer_input, src_key_padding_mask=padding_mask) # (B, T, F)\n",
    "\n",
    "        # Aggregate the output sequence into a single vector for prediction.\n",
    "        # Simple averaging is a common and effective method.\n",
    "        aggregated_output = transformer_output.mean(dim=1)\n",
    "        \n",
    "        y_pred = self.regressor(aggregated_output)\n",
    "        return y_pred\n",
    "\n",
    "    # ... (training_step, loss functions, etc. would remain the same) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab1e7f",
   "metadata": {},
   "source": [
    "## two head lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ccb5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class LSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # Main regression output: predict all linePrices up to max_len_y\n",
    "        self.fc_reg = nn.Linear(hidden_dim, max_len_y)\n",
    "        # Length prediction branch: logits per possible line (max_len_y)\n",
    "        self.fc_len = nn.Linear(hidden_dim, max_len_y)\n",
    "        self.lr = lr\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")  # we'll mask padded values\n",
    "        self.loss_fn_len = nn.BCEWithLogitsLoss()        # treat as multi-label classification\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x[\"main\"] \n",
    "        print(\"x\",x)\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        last_h = hn[-1]\n",
    "\n",
    "        y_pred = self.fc_reg(last_h)      # regression outputs\n",
    "        len_logits = self.fc_len(last_h)  # logits per possible line\n",
    "        return y_pred, len_logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred, len_logits = self(X, lengths)\n",
    "\n",
    "        # --- Regression loss with masking ---\n",
    "        mask = (y != 0).float()  # assume padding = 0\n",
    "        loss_reg = (self.loss_fn_reg(y_pred, y) * mask).sum() / mask.sum()\n",
    "\n",
    "        # --- Length loss ---\n",
    "        target_lengths = torch.zeros_like(len_logits, dtype=torch.float32)\n",
    "        for i, l in enumerate(lengths):\n",
    "            target_lengths[i, :l] = 1.0   # first l positions are 1, rest are 0\n",
    "\n",
    "        loss_len = self.loss_fn_len(len_logits, target_lengths)\n",
    "\n",
    "        loss = loss_reg + 0.1 * loss_len\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_loss_reg\", loss_reg, prog_bar=True)\n",
    "        self.log(\"train_loss_len\", loss_len, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def predict_length(self, len_logits):\n",
    "        \"\"\"\n",
    "        Convert logits to predicted number of lines using threshold.\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(len_logits)\n",
    "        pred_len = (probs > self.threshold).sum(dim=1)\n",
    "        return pred_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702411e",
   "metadata": {},
   "source": [
    "## two head lstm greedy match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def match_and_loss(y_pred, y_true, mask, loss_fn):\n",
    "    \"\"\"\n",
    "    y_pred: (B, max_len_y)\n",
    "    y_true: (B, max_len_y)\n",
    "    mask: (B, max_len_y)  1 if real, 0 if padding\n",
    "    loss_fn: pointwise loss, e.g. MSELoss(reduction=\"none\")\n",
    "    for each target find closest line\n",
    "    \"\"\"\n",
    "    B, max_len = y_true.shape\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    for i in range(B):\n",
    "        gt_vals = y_true[i][mask[i] > 0]  # real targets\n",
    "        preds = y_pred[i]\n",
    "\n",
    "        if len(gt_vals) == 0:\n",
    "            continue\n",
    "\n",
    "        # greedy matching: for each gt, find closest prediction\n",
    "        used = set()\n",
    "        for gt in gt_vals:\n",
    "            dists = torch.abs(preds - gt)\n",
    "            for u in used:\n",
    "                dists[u] = float(\"inf\")  # prevent reuse\n",
    "            j = torch.argmin(dists)     # index of closest prediction\n",
    "            used.add(j.item())\n",
    "\n",
    "            total_loss += loss_fn(preds[j], gt)\n",
    "            total_count += 1\n",
    "\n",
    "    return total_loss / max(total_count, 1)\n",
    "\n",
    "class LSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # Main regression output: predict all linePrices up to max_len_y\n",
    "        self.fc_reg = nn.Linear(hidden_dim, max_len_y)\n",
    "        # Length prediction branch: logits per possible line (max_len_y)\n",
    "        self.fc_len = nn.Linear(hidden_dim, max_len_y)\n",
    "        self.lr = lr\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")  # we'll mask padded values\n",
    "        self.loss_fn_len = nn.BCEWithLogitsLoss()        # treat as multi-label classification\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x[\"main\"] \n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        last_h = hn[-1]\n",
    "\n",
    "        y_pred = self.fc_reg(last_h)      # regression outputs\n",
    "        len_logits = self.fc_len(last_h)  # logits per possible line\n",
    "        return y_pred, len_logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred, len_logits = self(X, lengths)\n",
    "\n",
    "        mask = (y != 0).float()\n",
    "\n",
    "        # --- New greedy-matching regression loss ---\n",
    "        loss_reg = match_and_loss(y_pred, y, mask, nn.MSELoss())\n",
    "\n",
    "        # --- Length loss (unchanged) ---\n",
    "        target_lengths = torch.zeros_like(len_logits, dtype=torch.float32)\n",
    "        for i, l in enumerate(lengths):\n",
    "            target_lengths[i, :l] = 1.0\n",
    "\n",
    "        loss_len = self.loss_fn_len(len_logits, target_lengths)\n",
    "\n",
    "        loss = loss_reg + 0.1 * loss_len\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def predict_length(self, len_logits):\n",
    "        \"\"\"\n",
    "        Convert logits to predicted number of lines using threshold.\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(len_logits)\n",
    "        pred_len = (probs > self.threshold).sum(dim=1)\n",
    "        return pred_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb2dd1",
   "metadata": {},
   "source": [
    "## two head lstm sum of logits loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models.losses.two_head_logit_sum import sum_of_logits\n",
    "class LSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc_reg = nn.Linear(hidden_dim, max_len_y)\n",
    "        self.fc_len = nn.Linear(hidden_dim, max_len_y)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")  # masked regression\n",
    "        self.loss_fn_len = nn.BCEWithLogitsLoss()        # multi-label classification\n",
    "        self.compute_loss = sum_of_logits()\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = x[\"main\"] \n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        last_h = hn[-1]\n",
    "\n",
    "        y_pred = self.fc_reg(last_h)\n",
    "        len_logits = self.fc_len(last_h)\n",
    "        return y_pred, len_logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred, len_logits = self(X, lengths)\n",
    "\n",
    "        loss, loss_reg, loss_len = self.compute_loss(y_pred, len_logits, y, lengths)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"loss_reg\", loss_reg, prog_bar=False)\n",
    "        self.log(\"loss_len\", loss_len, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def predict_length(self, len_logits):\n",
    "        \"\"\"\n",
    "        Convert logits to predicted number of lines using threshold.\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(len_logits)\n",
    "        pred_len = (probs > self.threshold).sum(dim=1)\n",
    "        return pred_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b88b10",
   "metadata": {},
   "source": [
    "## two head lstm soft thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "from models.losses.soft_thresholding_two_head import soft_thresholding_loss\n",
    "\n",
    "class LSTMMultiRegressor(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, max_len_y, lr=0.001, threshold=0.5, k_soft=20.0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc_reg = nn.Linear(hidden_dim, max_len_y)\n",
    "        self.fc_len = nn.Linear(hidden_dim, max_len_y)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.threshold = threshold\n",
    "        self.k_soft = k_soft\n",
    "\n",
    "        self.loss_fn_reg = nn.MSELoss(reduction=\"none\")\n",
    "        self.loss_fn_len = nn.BCEWithLogitsLoss()\n",
    "        self.compute_loss = soft_thresholding_loss\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = x[\"main\"]\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        last_h = hn[-1]\n",
    "\n",
    "        y_pred = self.fc_reg(last_h)\n",
    "        len_logits = self.fc_len(last_h)\n",
    "        return y_pred, len_logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        y_pred, len_logits = self(X, lengths)\n",
    "\n",
    "        loss, loss_reg, loss_len = self.compute_loss(y_pred, len_logits, y, lengths)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"loss_reg\", loss_reg, prog_bar=False)\n",
    "        self.log(\"loss_len\", loss_len, prog_bar=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def predict_length(self, len_logits):\n",
    "        \"\"\"\n",
    "        Convert logits to predicted number of lines using threshold.\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(len_logits)\n",
    "        pred_len = (probs > self.threshold).sum(dim=1)\n",
    "        return pred_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b75a48",
   "metadata": {},
   "source": [
    "## FNNCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c70fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    \"\"\"\n",
    "    raw_params: (B, 3K) tensor from mdn_head\n",
    "    returns:\n",
    "        pi    (B, K) mixture weights\n",
    "        mu    (B, K) means\n",
    "        sigma (B, K) std devs\n",
    "    \"\"\"\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "\n",
    "    pi = raw[..., 0]                 # (B,K)\n",
    "    mu = raw[..., 1]                 # (B,K)\n",
    "    sigma = raw[..., 2]              # (B,K)\n",
    "\n",
    "    pi = F.softmax(pi, dim=-1)       # weights sum to 1\n",
    "    sigma = F.softplus(sigma) + 1e-4 # strictly positive\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def mdn_nll_multitarget(y_line, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for MDN with multiple valid targets per sample.\n",
    "    Args:\n",
    "        y_line : (B, L) padded targets (0 where invalid)\n",
    "        pi, mu, sigma : (B, K) MDN params\n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    B, K = mu.shape\n",
    "    losses = []\n",
    "\n",
    "    for b in range(B):\n",
    "        valid_y = y_line[b][y_line[b] > 0]  # (M,)\n",
    "        if len(valid_y) == 0:\n",
    "            continue\n",
    "\n",
    "        # expand to (M, K)\n",
    "        y_exp = valid_y.unsqueeze(-1).expand(-1, K)\n",
    "\n",
    "        log_prob = -0.5 * ((y_exp - mu[b]) / (sigma[b] + 1e-8))**2 \\\n",
    "                   - torch.log(sigma[b] + 1e-8) \\\n",
    "                   - 0.5 * torch.log(torch.tensor(2.0 * torch.pi, device=y_line.device))\n",
    "\n",
    "        log_mix = torch.log(pi[b] + 1e-8) + log_prob\n",
    "        log_sum = torch.logsumexp(log_mix, dim=-1)  # (M,)\n",
    "\n",
    "        losses.append(-log_sum.mean())\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=y_line.device, requires_grad=True)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "class CNNLSTM_MDN(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, hidden_features=64, out_features=32,\n",
    "                 lr=1e-3, n_components=5, cnn_channels=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Time-distributed feature extractor\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_features)\n",
    "        self.ln1 = nn.LayerNorm(hidden_features) # ADDED: LayerNorm for time-step features\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.ln2 = nn.LayerNorm(out_features) # ADDED: LayerNorm\n",
    "\n",
    "        # CNN feature extractors\n",
    "        self.conv1 = nn.Conv1d(out_features, cnn_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm for convolutional features\n",
    "        self.conv3 = nn.Conv1d(out_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm\n",
    "\n",
    "        # Learnable mixer for CNN outputs\n",
    "        self.mixer = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1, bias=True)\n",
    "\n",
    "        # LSTM for temporal dependency\n",
    "        fused_dim = cnn_channels # Input to LSTM is the mixed CNN output\n",
    "        self.lstm = nn.LSTM(fused_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # MDN Head\n",
    "        self.mdn_head = nn.Linear(hidden_dim, 3 * n_components)\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        # Input shape X[\"main\"]: (B, T, F_in)\n",
    "        x = X[\"main\"] # REMOVED redundant transposes\n",
    "\n",
    "        # 1. Time-distributed feature extraction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.ln1(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.ln2(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "\n",
    "        # 2. CNN feature extraction\n",
    "        x = x.transpose(1, 2)   # Shape: (B, C_in, L=T)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "        x3 = F.relu(self.bn3(self.conv3(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "\n",
    "        # 3. Mix CNN outputs\n",
    "        paired = torch.stack([x1, x3], dim=1) # Shape: (B, 2, C_out, L)\n",
    "        mixed = self.mixer(paired)            # Shape: (B, 1, C_out, L)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        xf = mixed.squeeze(1).transpose(1, 2) # Shape: (B, L, C_out)\n",
    "\n",
    "        # 4. LSTM for sequence summary\n",
    "        if lengths is not None:\n",
    "            packed_input = pack_padded_sequence(\n",
    "                xf, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            _, (h_last, _) = self.lstm(packed_input)\n",
    "        else:\n",
    "            _, (h_last, _) = self.lstm(xf)\n",
    "        \n",
    "        last_h = h_last[-1] # Shape: (B, H)\n",
    "        \n",
    "        # 5. MDN head for distribution parameters\n",
    "        raw = self.mdn_head(last_h)\n",
    "        pi, mu, sigma = mdn_split_params(raw, self.n_components)\n",
    "        return {\"pi\": pi, \"mu\": mu, \"sigma\": sigma}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "    # Log everything to progress bar\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        self.log(\"val/pi_mean\", mdn[\"pi\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/pi_std\", mdn[\"pi\"].std(), prog_bar=True)\n",
    "        self.log(\"val/mu_mean\", mdn[\"mu\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/mu_std\", mdn[\"mu\"].std(), prog_bar=True)\n",
    "        self.log(\"val/sigma_mean\", mdn[\"sigma\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/sigma_std\", mdn[\"sigma\"].std(), prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self): \n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #         optimizer,\n",
    "    #         mode=\"min\",\n",
    "    #         factor=0.2,   # Reduce LR by 80%\n",
    "    #         patience=5,   # After 5 epochs of no val_loss improvement\n",
    "    #         verbose=True\n",
    "    #     )\n",
    "    #     return {\n",
    "    #         \"optimizer\": optimizer,\n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": scheduler,\n",
    "    #             \"monitor\": \"val/loss\",  # Important!\n",
    "    #         },\n",
    "        # }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2b1d4",
   "metadata": {},
   "source": [
    "## CNNLSTM weightening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368a03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import pytorch_lightning as pl\n",
    "# Your mdn_split_params function remains the same\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "    pi = F.softmax(raw[..., 0], dim=-1)\n",
    "    mu = raw[..., 1]\n",
    "    sigma = F.softplus(raw[..., 2]) + 1e-4\n",
    "    return pi, mu, sigma\n",
    "\n",
    "def weighted_mdn_nll(y_true, mdn_params, weights):\n",
    "    total_loss = 0.0\n",
    "    num_lines = y_true.shape[1]\n",
    "    B = y_true.shape[0]\n",
    "\n",
    "    # Keep track if any valid lines are found\n",
    "    valid_line_found = False\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        y_target = y_true[:, i:i+1]  # (B,1)\n",
    "        pi, mu, sigma = mdn_params['pi'][i], mdn_params['mu'][i], mdn_params['sigma'][i]\n",
    "\n",
    "        mask = (y_target != 0).squeeze()\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        valid_line_found = True\n",
    "        y_target_masked = y_target[mask]\n",
    "        pi_masked, mu_masked, sigma_masked = pi[mask], mu[mask], sigma[mask]\n",
    "\n",
    "        dist = Normal(loc=mu_masked, scale=sigma_masked)\n",
    "        log_prob = dist.log_prob(y_target_masked.expand_as(mu_masked))\n",
    "        log_mix_prob = torch.log(pi_masked + 1e-8) + log_prob\n",
    "        log_likelihood = torch.logsumexp(log_mix_prob, dim=1)\n",
    "        line_loss = -log_likelihood.mean()\n",
    "        total_loss += weights[i] * line_loss\n",
    "\n",
    "    if not valid_line_found:\n",
    "        # Avoid returning a Python float; create a tensor with requires_grad\n",
    "        total_loss = torch.tensor(0.0, device=y_true.device, requires_grad=True)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "class CNNLSTM_MDN_MultiHead(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, hidden_features=64, out_features=32,\n",
    "                 lr=1e-3, n_components=5, cnn_channels=64, dropout=0.1, num_lines=9):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # --- Your CNN and LSTM base remains the same ---\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_features)\n",
    "        self.ln1 = nn.LayerNorm(hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.ln2 = nn.LayerNorm(out_features)\n",
    "        self.conv1 = nn.Conv1d(out_features, cnn_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels)\n",
    "        self.conv3 = nn.Conv1d(out_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels)\n",
    "        self.mixer = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1, bias=True)\n",
    "        fused_dim = cnn_channels\n",
    "        self.lstm = nn.LSTM(fused_dim, hidden_dim, num_layers=num_layers,\n",
    "                              batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # === MODIFICATION: Create a list of MDN heads ===\n",
    "        self.num_lines = num_lines\n",
    "        self.mdn_heads = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, 3 * n_components) for _ in range(num_lines)]\n",
    "        )\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # === Define importance weights here ===\n",
    "        # Using exponential decay: w_i = 0.9^(i-1)\n",
    "        weights = torch.tensor([0.9**i for i in range(self.num_lines)])\n",
    "        self.register_buffer('loss_weights', weights)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module): # Your init function is fine\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None: nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None: nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        # --- Your forward pass for the base model is the same ---\n",
    "        x = X[\"main\"]\n",
    "        x = F.relu(self.ln1(self.fc1(x)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        x = x.transpose(1, 2)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        x3 = F.relu(self.bn3(self.conv3(x)))\n",
    "        paired = torch.stack([x1, x3], dim=1)\n",
    "        mixed = self.mixer(paired)\n",
    "        xf = mixed.squeeze(1).transpose(1, 2)\n",
    "        \n",
    "        # We'll assume lengths is None for simplicity here, but your implementation is fine\n",
    "        _, (h_last, _) = self.lstm(xf)\n",
    "        last_h = h_last[-1]\n",
    "\n",
    "        # === MODIFICATION: Get parameters from all heads ===\n",
    "        all_params = {'pi': [], 'mu': [], 'sigma': []}\n",
    "        for i in range(self.num_lines):\n",
    "            raw_params = self.mdn_heads[i](last_h)\n",
    "            pi, mu, sigma = mdn_split_params(raw_params, self.n_components)\n",
    "            all_params['pi'].append(pi)\n",
    "            all_params['mu'].append(mu)\n",
    "            all_params['sigma'].append(sigma)\n",
    "\n",
    "        return all_params\n",
    "    \n",
    "\n",
    "    # This would be inside your CNNLSTM_MDN_MultiHead class\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Assuming your batch now provides a y tensor of shape (B, 9)\n",
    "        # where y has your target line values, padded with -1.\n",
    "        X, y, lengths = batch\n",
    "\n",
    "        # Get the dictionary of parameter lists from the forward pass\n",
    "        mdn_params = self(X, lengths)\n",
    "\n",
    "        # Calculate loss using the new weighted function\n",
    "        loss = weighted_mdn_nll(y, mdn_params, self.loss_weights)\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    # NOTE: You'll also need a validation_step that mirrors the training_step logic\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        mdn_params = self(X, lengths)\n",
    "        loss = weighted_mdn_nll(y, mdn_params, self.loss_weights)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a057d",
   "metadata": {},
   "source": [
    "## LSTM weightening with pi order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce6729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    \"\"\"\n",
    "    Splits raw MDN output into mixture weights (pi), means (mu), and stds (sigma)\n",
    "    \"\"\"\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "    pi = F.softmax(raw[..., 0], dim=-1)           # mixture probabilities\n",
    "    mu = raw[..., 1]                              # means\n",
    "    sigma = F.softplus(raw[..., 2]) + 1e-4       # stds\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def weighted_mdn_nll(y_true, mdn_params, weights):\n",
    "    \"\"\"\n",
    "    y_true: (B, num_lines)\n",
    "    mdn_params: dict with 'pi', 'mu', 'sigma' each of shape (B, n_components)\n",
    "    weights: (num_lines,) tensor\n",
    "    \"\"\"\n",
    "    B, num_lines = y_true.shape\n",
    "    pi, mu, sigma = mdn_params['pi'], mdn_params['mu'], mdn_params['sigma']  # (B, n_components)\n",
    "\n",
    "    # Sort components by pi descending\n",
    "    _, idx = torch.sort(pi, descending=True, dim=1)  # (B, n_components)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    valid_line_found = False\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        y_target = y_true[:, i]  # (B,)\n",
    "\n",
    "        # Skip masked/padded targets\n",
    "        mask = (y_target != 0)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        valid_line_found = True\n",
    "\n",
    "        # Select top pi component for this line\n",
    "        top_mu = mu.gather(1, idx[:, i].unsqueeze(1)).squeeze(1)      # (B,)\n",
    "        top_sigma = sigma.gather(1, idx[:, i].unsqueeze(1)).squeeze(1) # (B,)\n",
    "        y_target_masked = y_target[mask]\n",
    "        top_mu_masked = top_mu[mask]\n",
    "        top_sigma_masked = top_sigma[mask]\n",
    "\n",
    "        dist = Normal(top_mu_masked, top_sigma_masked)\n",
    "        line_loss = -dist.log_prob(y_target_masked).mean()\n",
    "        total_loss += weights[i] * line_loss\n",
    "\n",
    "    if not valid_line_found:\n",
    "        total_loss = torch.tensor(0.0, device=y_true.device, requires_grad=True)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "class CNNLSTM_MDN(pl.LightningModule):\n",
    "    def __init__(self, input_dim, feature_eng=15,hidden_dim=32, n_components=9, num_lines=9, lr=1e-3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_lines = num_lines\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # Base network\n",
    "        self.fc1 = nn.Linear(input_dim, feature_eng)\n",
    "        self.ln1 = nn.LayerNorm(feature_eng)\n",
    "        self.lstm = nn.LSTM(feature_eng, hidden_dim, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Single MDN head predicting n_components Gaussians\n",
    "        self.mdn_head = nn.Linear(hidden_dim, 3 * n_components)\n",
    "\n",
    "        # Importance weights for lines\n",
    "        weights = torch.tensor([0.9**i for i in range(num_lines)], dtype=torch.float)\n",
    "        self.register_buffer(\"loss_weights\", weights)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None: nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        \"\"\"\n",
    "        X: (B, T, input_dim)\n",
    "        \"\"\"\n",
    "        x = X[\"main\"]\n",
    "        x = F.relu(self.ln1(self.fc1(x)))\n",
    "        \n",
    "        if lengths is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            _, (h_last, _) = self.lstm(x)\n",
    "        else:\n",
    "            _, (h_last, _) = self.lstm(x)\n",
    "\n",
    "        last_h = h_last[-1]  # (B, hidden_dim)\n",
    "        raw_params = self.mdn_head(last_h)  # (B, 3*n_components)\n",
    "        pi, mu, sigma = mdn_split_params(raw_params, self.n_components)\n",
    "        return {\"pi\": pi, \"mu\": mu, \"sigma\": sigma}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        mdn_params = self(X)\n",
    "        loss = weighted_mdn_nll(y, mdn_params, self.loss_weights)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        mdn_params = self(X)\n",
    "        loss = weighted_mdn_nll(y, mdn_params, self.loss_weights)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f1d12",
   "metadata": {},
   "source": [
    "## CNNLSTM weightening order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb7ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    \"\"\"\n",
    "    Splits raw MDN output into mixture weights (pi), means (mu), and stds (sigma)\n",
    "    \"\"\"\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "    pi = F.softmax(raw[..., 0], dim=-1)           # mixture probabilities\n",
    "    mu = raw[..., 1]                              # means\n",
    "    sigma = F.softplus(raw[..., 2]) + 1e-4       # stds\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def weighted_mdn_nll(y_true, mdn_params, weights):\n",
    "    \"\"\"\n",
    "    y_true: (B, num_lines)\n",
    "    mdn_params: dict with 'pi', 'mu', 'sigma' each of shape (B, n_components)\n",
    "    weights: (num_lines,) tensor\n",
    "    \"\"\"\n",
    "    B, num_lines = y_true.shape\n",
    "    pi, mu, sigma = mdn_params['pi'], mdn_params['mu'], mdn_params['sigma']  # (B, n_components)\n",
    "\n",
    "    # Sort components by pi descending\n",
    "    _, idx = torch.sort(pi, descending=True, dim=1)  # (B, n_components)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    valid_line_found = False\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        y_target = y_true[:, i]  # (B,)\n",
    "\n",
    "        # Skip masked/padded targets\n",
    "        mask = (y_target != 0)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        valid_line_found = True\n",
    "\n",
    "        # Select top pi component for this line\n",
    "        top_mu = mu.gather(1, idx[:, i].unsqueeze(1)).squeeze(1)      # (B,)\n",
    "        top_sigma = sigma.gather(1, idx[:, i].unsqueeze(1)).squeeze(1) # (B,)\n",
    "        y_target_masked = y_target[mask]\n",
    "        top_mu_masked = top_mu[mask]\n",
    "        top_sigma_masked = top_sigma[mask]\n",
    "\n",
    "        dist = Normal(top_mu_masked, top_sigma_masked)\n",
    "        line_loss = -dist.log_prob(y_target_masked).mean()\n",
    "        total_loss += weights[i] * line_loss\n",
    "\n",
    "    if not valid_line_found:\n",
    "        total_loss = torch.tensor(0.0, device=y_true.device, requires_grad=True)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "class cnn_lstm(pl.LightningModule):\n",
    "    def __init__(self, input_dim, feature_eng=15, hidden_dim=32, n_components=9, num_lines=9, lr=1e-3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_lines = num_lines\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # Base feature projection\n",
    "        self.fc1 = nn.Linear(input_dim, feature_eng)\n",
    "        self.ln1 = nn.LayerNorm(feature_eng)\n",
    "\n",
    "        # Parallel conv1d branches\n",
    "        self.k1 = nn.Conv1d(feature_eng, feature_eng, kernel_size=1, padding=0)\n",
    "        self.k3 = nn.Conv1d(feature_eng, feature_eng, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fusion via conv2d\n",
    "        # Input channels = 2 (from k1 + k3), Output = 1, kernel size (1,1) to fuse\n",
    "        self.fusion_conv2d = nn.Conv2d(2, 1, kernel_size=(1, 1))\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(feature_eng, hidden_dim, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Single MDN head predicting n_components Gaussians\n",
    "        self.mdn_head = nn.Linear(hidden_dim, 3 * n_components)\n",
    "\n",
    "        # Importance weights for lines\n",
    "        weights = torch.tensor([0.9**i for i in range(num_lines)], dtype=torch.float)\n",
    "        self.register_buffer(\"loss_weights\", weights)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        \"\"\"\n",
    "        X: dict with key \"main\", value shape (B, T, input_dim)\n",
    "        \"\"\"\n",
    "        x = X[\"main\"]  # (B, T, input_dim)\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # Fully connected projection\n",
    "        x = F.relu(self.ln1(self.fc1(x)))  # (B, T, F)\n",
    "\n",
    "        # Conv1d expects (B, F, T)\n",
    "        x_cnn = x.transpose(1, 2)  # (B, F, T)\n",
    "\n",
    "        # Parallel convs\n",
    "        x1 = self.k1(x_cnn)  # (B, F, T)\n",
    "        x3 = self.k3(x_cnn)  # (B, F, T)\n",
    "\n",
    "        # Stack into 2-channel feature map\n",
    "        stacked = torch.stack([x1, x3], dim=1)  # (B, 2, F, T)\n",
    "\n",
    "        # Fuse with conv2d → (B, 1, F, T)\n",
    "        fused = self.fusion_conv2d(stacked).squeeze(1)  # (B, F, T)\n",
    "\n",
    "        # Back to (B, T, F)\n",
    "        fused = fused.transpose(1, 2)\n",
    "\n",
    "        # LSTM with packed sequence\n",
    "        if lengths is not None:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(fused, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            _, (h_last, _) = self.lstm(packed)\n",
    "        else:\n",
    "            _, (h_last, _) = self.lstm(fused)\n",
    "\n",
    "        last_h = h_last[-1]  # (B, hidden_dim)\n",
    "        raw_params = self.mdn_head(last_h)  # (B, 3 * n_components)\n",
    "\n",
    "        # Assume you have mdn_split_params(pi, mu, sigma)\n",
    "        pi, mu, sigma = mdn_split_params(raw_params, self.n_components)\n",
    "        return {\"pi\": pi, \"mu\": mu, \"sigma\": sigma}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        mdn_params = self(X)\n",
    "        loss = weighted_mdn_nll(y, mdn_params, self.loss_weights)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        mdn_params = self(X)\n",
    "        loss = weighted_mdn_nll(y, mdn_params, self.loss_weights)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e7f3",
   "metadata": {},
   "source": [
    "## CNNLSTM weightening with sigma confidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e3bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import pytorch_lightning as pl\n",
    "# Your mdn_split_params function remains the same\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "    pi = F.softmax(raw[..., 0], dim=-1)\n",
    "    mu = raw[..., 1]\n",
    "    sigma = F.softplus(raw[..., 2]) + 1e-4\n",
    "    return pi, mu, sigma\n",
    "\n",
    "def weighted_mdn_nll_with_sigma_penalty(y_true, mdn_params, weights, lambda_sigma=0.01):\n",
    "    \"\"\"\n",
    "    Calculates weighted MDN NLL and adds a penalty for large sigmas.\n",
    "    \n",
    "    Args:\n",
    "        lambda_sigma (float): The strength of the sigma penalty.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_lines = y_true.shape[1]\n",
    "\n",
    "    for i in range(num_lines):\n",
    "        y_target = y_true[:, i:i+1]\n",
    "        pi, mu, sigma = mdn_params['pi'][i], mdn_params['mu'][i], mdn_params['sigma'][i]\n",
    "        mask = (y_target != -1).squeeze()\n",
    "\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        y_target_masked = y_target[mask]\n",
    "        pi_masked, mu_masked, sigma_masked = pi[mask], mu[mask], sigma[mask]\n",
    "        \n",
    "        # --- 1. NLL Loss Calculation (same as before) ---\n",
    "        dist = Normal(loc=mu_masked, scale=sigma_masked)\n",
    "        log_prob = dist.log_prob(y_target_masked.expand_as(mu_masked))\n",
    "        log_mix_prob = torch.log(pi_masked + 1e-8) + log_prob\n",
    "        log_likelihood = torch.logsumexp(log_mix_prob, dim=1)\n",
    "        line_nll_loss = -log_likelihood.mean()\n",
    "\n",
    "        # --- 2. NEW: Sigma Penalty Calculation ---\n",
    "        # We penalize the mean of the sigmas for the most likely component\n",
    "        # This focuses the penalty on the component the model actually uses\n",
    "        most_likely_idx = torch.argmax(pi_masked, dim=1)\n",
    "        most_likely_sigma = sigma_masked.gather(1, most_likely_idx.unsqueeze(1)).squeeze()\n",
    "        sigma_penalty = torch.mean(most_likely_sigma)\n",
    "        \n",
    "        # --- 3. Combine and Weight ---\n",
    "        combined_line_loss = line_nll_loss + (lambda_sigma * sigma_penalty)\n",
    "        total_loss += weights[i] * combined_line_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# In your training_step, you would call this new function:\n",
    "# loss = weighted_mdn_nll_with_sigma_penalty(y, mdn_params, self.loss_weights, lambda_sigma=0.01)\n",
    "\n",
    "class CNNLSTM_MDN_MultiHead(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, hidden_features=64, out_features=32,\n",
    "                 lr=1e-3, n_components=5, cnn_channels=64, dropout=0.1, num_lines=9):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # --- Your CNN and LSTM base remains the same ---\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_features)\n",
    "        self.ln1 = nn.LayerNorm(hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.ln2 = nn.LayerNorm(out_features)\n",
    "        self.conv1 = nn.Conv1d(out_features, cnn_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels)\n",
    "        self.conv3 = nn.Conv1d(out_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels)\n",
    "        self.mixer = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1, bias=True)\n",
    "        fused_dim = cnn_channels\n",
    "        self.lstm = nn.LSTM(fused_dim, hidden_dim, num_layers=num_layers,\n",
    "                              batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # === MODIFICATION: Create a list of MDN heads ===\n",
    "        self.num_lines = num_lines\n",
    "        self.mdn_heads = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, 3 * n_components) for _ in range(num_lines)]\n",
    "        )\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # === Define importance weights here ===\n",
    "        # Using exponential decay: w_i = 0.9^(i-1)\n",
    "        weights = torch.tensor([0.9**i for i in range(self.num_lines)])\n",
    "        self.register_buffer('loss_weights', weights)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module): # Your init function is fine\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None: nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None: nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        # --- Your forward pass for the base model is the same ---\n",
    "        x = X[\"main\"]\n",
    "        x = F.relu(self.ln1(self.fc1(x)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        x = x.transpose(1, 2)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        x3 = F.relu(self.bn3(self.conv3(x)))\n",
    "        paired = torch.stack([x1, x3], dim=1)\n",
    "        mixed = self.mixer(paired)\n",
    "        xf = mixed.squeeze(1).transpose(1, 2)\n",
    "        \n",
    "        # We'll assume lengths is None for simplicity here, but your implementation is fine\n",
    "        _, (h_last, _) = self.lstm(xf)\n",
    "        last_h = h_last[-1]\n",
    "\n",
    "        # === MODIFICATION: Get parameters from all heads ===\n",
    "        all_params = {'pi': [], 'mu': [], 'sigma': []}\n",
    "        for i in range(self.num_lines):\n",
    "            raw_params = self.mdn_heads[i](last_h)\n",
    "            pi, mu, sigma = mdn_split_params(raw_params, self.n_components)\n",
    "            all_params['pi'].append(pi)\n",
    "            all_params['mu'].append(mu)\n",
    "            all_params['sigma'].append(sigma)\n",
    "\n",
    "        return all_params\n",
    "    \n",
    "\n",
    "    # This would be inside your CNNLSTM_MDN_MultiHead class\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Assuming your batch now provides a y tensor of shape (B, 9)\n",
    "        # where y has your target line values, padded with -1.\n",
    "        X, y, lengths = batch\n",
    "\n",
    "        # Get the dictionary of parameter lists from the forward pass\n",
    "        mdn_params = self(X, lengths)\n",
    "\n",
    "        # Calculate loss using the new weighted function\n",
    "        loss = weighted_mdn_nll_with_sigma_penalty(y, mdn_params, self.loss_weights)\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    # NOTE: You'll also need a validation_step that mirrors the training_step logic\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        mdn_params = self(X, lengths)\n",
    "        loss = weighted_mdn_nll_with_sigma_penalty(y, mdn_params, self.loss_weights)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6484b7d",
   "metadata": {},
   "source": [
    "## CNNlSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2139f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    \"\"\"\n",
    "    raw_params: (B, 3K) tensor from mdn_head\n",
    "    returns:\n",
    "        pi    (B, K) mixture weights\n",
    "        mu    (B, K) means\n",
    "        sigma (B, K) std devs\n",
    "    \"\"\"\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "\n",
    "    pi = raw[..., 0]                 # (B,K)\n",
    "    mu = raw[..., 1]                 # (B,K)\n",
    "    sigma = raw[..., 2]              # (B,K)\n",
    "\n",
    "    pi = F.softmax(pi, dim=-1)       # weights sum to 1\n",
    "    sigma = F.softplus(sigma) + 1e-4 # strictly positive\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def mdn_nll_multitarget(y_line, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for MDN with multiple valid targets per sample.\n",
    "    Args:\n",
    "        y_line : (B, L) padded targets (0 where invalid)\n",
    "        pi, mu, sigma : (B, K) MDN params\n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    B, K = mu.shape\n",
    "    losses = []\n",
    "\n",
    "    for b in range(B):\n",
    "        valid_y = y_line[b][y_line[b] > 0]  # (M,)\n",
    "        if len(valid_y) == 0:\n",
    "            continue\n",
    "\n",
    "        # expand to (M, K)\n",
    "        y_exp = valid_y.unsqueeze(-1).expand(-1, K)\n",
    "\n",
    "        log_prob = -0.5 * ((y_exp - mu[b]) / (sigma[b] + 1e-8))**2 \\\n",
    "                   - torch.log(sigma[b] + 1e-8) \\\n",
    "                   - 0.5 * torch.log(torch.tensor(2.0 * torch.pi, device=y_line.device))\n",
    "\n",
    "        log_mix = torch.log(pi[b] + 1e-8) + log_prob\n",
    "        log_sum = torch.logsumexp(log_mix, dim=-1)  # (M,)\n",
    "\n",
    "        losses.append(-log_sum.mean())\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=y_line.device, requires_grad=True)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "class CNNLSTM_MDN(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, hidden_features=64, out_features=32,\n",
    "                 lr=1e-3, n_components=5, cnn_channels=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Time-distributed feature extractor\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_features)\n",
    "        self.ln1 = nn.LayerNorm(hidden_features) # ADDED: LayerNorm for time-step features\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.ln2 = nn.LayerNorm(out_features) # ADDED: LayerNorm\n",
    "\n",
    "        # CNN feature extractors\n",
    "        self.conv1 = nn.Conv1d(out_features, cnn_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm for convolutional features\n",
    "        self.conv3 = nn.Conv1d(out_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm\n",
    "\n",
    "        # Learnable mixer for CNN outputs\n",
    "        self.mixer = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1, bias=True)\n",
    "\n",
    "        # LSTM for temporal dependency\n",
    "        fused_dim = cnn_channels # Input to LSTM is the mixed CNN output\n",
    "        self.lstm = nn.LSTM(fused_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # MDN Head\n",
    "        self.mdn_head = nn.Linear(hidden_dim, 3 * n_components)\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        # Input shape X[\"main\"]: (B, T, F_in)\n",
    "        x = X[\"main\"] # REMOVED redundant transposes\n",
    "\n",
    "        # 1. Time-distributed feature extraction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.ln1(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.ln2(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "        # 2. CNN feature extraction\n",
    "        x = x.transpose(1, 2)   # Shape: (B, C_in, L=T)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "        x3 = F.relu(self.bn3(self.conv3(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "\n",
    "        # 3. Mix CNN outputs\n",
    "        paired = torch.stack([x1, x3], dim=1) # Shape: (B, 2, C_out, L)\n",
    "        mixed = self.mixer(paired)            # Shape: (B, 1, C_out, L)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        xf = mixed.squeeze(1).transpose(1, 2) # Shape: (B, L, C_out)\n",
    "\n",
    "        # 4. LSTM for sequence summary\n",
    "        if lengths is not None:\n",
    "            packed_input = pack_padded_sequence(\n",
    "                xf, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            _, (h_last, _) = self.lstm(packed_input)\n",
    "        else:\n",
    "            _, (h_last, _) = self.lstm(xf)\n",
    "        \n",
    "        last_h = h_last[-1] # Shape: (B, H)\n",
    "        \n",
    "        # 5. MDN head for distribution parameters\n",
    "        raw = self.mdn_head(last_h)\n",
    "        pi, mu, sigma = mdn_split_params(raw, self.n_components)\n",
    "        return {\"pi\": pi, \"mu\": mu, \"sigma\": sigma}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "    # Log everything to progress bar\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        self.log(\"val/pi_mean\", mdn[\"pi\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/pi_std\", mdn[\"pi\"].std(), prog_bar=True)\n",
    "        self.log(\"val/mu_mean\", mdn[\"mu\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/mu_std\", mdn[\"mu\"].std(), prog_bar=True)\n",
    "        self.log(\"val/sigma_mean\", mdn[\"sigma\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/sigma_std\", mdn[\"sigma\"].std(), prog_bar=True)\n",
    "        \n",
    "    # Inside your CNNLSTM_MDN class\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "    #     # Define the scheduler\n",
    "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #         optimizer,\n",
    "    #         mode='min',      # We want to minimize the validation loss\n",
    "    #         factor=0.5,      # Reduce LR by 50% (1.0 -> 0.2)\n",
    "    #         patience=10,      # Wait 5 validation epochs with no improvement before reducing\n",
    "    #         verbose=True\n",
    "    #     )\n",
    "        \n",
    "    #     return {\n",
    "    #         \"optimizer\": optimizer,\n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": scheduler,\n",
    "    #             \"monitor\": \"val/loss\",  # The metric to watch\n",
    "    #         },\n",
    "    #     }\n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #         optimizer,\n",
    "    #         mode=\"min\",\n",
    "    #         factor=0.2,   # Reduce LR by 80%\n",
    "    #         patience=5,   # After 5 epochs of no val_loss improvement\n",
    "    #         verbose=True\n",
    "    #     )\n",
    "    #     return {\n",
    "    #         \"optimizer\": optimizer,\n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": scheduler,\n",
    "    #             \"monitor\": \"val/loss\",  # Important!\n",
    "    #         },\n",
    "        # }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e8361",
   "metadata": {},
   "source": [
    "## CNNLSTM scalie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a359387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def mdn_split_params(raw_params, n_components, mu_scale=10, mu_bias=.9, sigma_scale=10.0):\n",
    "    \"\"\"\n",
    "    Split raw MDN parameters into (pi, mu, sigma).\n",
    "\n",
    "    Args:\n",
    "        raw_params: (B, 3 * K) from the network\n",
    "        n_components: number of mixture components\n",
    "        mu_scale: scaling factor for mu (default 1.0 = no scaling)\n",
    "        mu_bias: shift/bias applied after scaling\n",
    "        sigma_scale: scaling factor for sigma (default 10.0)\n",
    "    \"\"\"\n",
    "    B = raw_params.size(0)\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "\n",
    "    pi_raw = raw[..., 0]\n",
    "    mu_raw = raw[..., 1]\n",
    "    sigma_raw = raw[..., 2]\n",
    "\n",
    "    pi = F.softmax(pi_raw, dim=-1)\n",
    "    mu = mu_raw / mu_scale + mu_bias\n",
    "    sigma = F.softplus(sigma_raw / sigma_scale) + 1e-4\n",
    "\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def mdn_nll_multitarget(y_line, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for MDN with multiple valid targets per sample.\n",
    "    Args:\n",
    "        y_line : (B, L) padded targets (0 where invalid)\n",
    "        pi, mu, sigma : (B, K) MDN params\n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    B, K = mu.shape\n",
    "    losses = []\n",
    "\n",
    "    for b in range(B):\n",
    "        valid_y = y_line[b][y_line[b] > 0]  # (M,)\n",
    "        if len(valid_y) == 0:\n",
    "            continue\n",
    "\n",
    "        # expand to (M, K)\n",
    "        y_exp = valid_y.unsqueeze(-1).expand(-1, K)\n",
    "\n",
    "        log_prob = -0.5 * ((y_exp - mu[b]) / (sigma[b] + 1e-8))**2 \\\n",
    "                   - torch.log(sigma[b] + 1e-8) \\\n",
    "                   - 0.5 * torch.log(torch.tensor(2.0 * torch.pi, device=y_line.device))\n",
    "\n",
    "        log_mix = torch.log(pi[b] + 1e-8) + log_prob\n",
    "        log_sum = torch.logsumexp(log_mix, dim=-1)  # (M,)\n",
    "\n",
    "        losses.append(-log_sum.mean())\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=y_line.device, requires_grad=True)\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "class CNNLSTM_MDN(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=1, hidden_features=64, out_features=32,\n",
    "                 lr=1e-3, n_components=5, cnn_channels=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Time-distributed feature extractor\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_features)\n",
    "        self.ln1 = nn.LayerNorm(hidden_features) # ADDED: LayerNorm for time-step features\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.ln2 = nn.LayerNorm(out_features) # ADDED: LayerNorm\n",
    "\n",
    "        # CNN feature extractors\n",
    "        self.conv1 = nn.Conv1d(out_features, cnn_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm for convolutional features\n",
    "        self.conv3 = nn.Conv1d(out_features, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_channels) # ADDED: BatchNorm\n",
    "\n",
    "        # Learnable mixer for CNN outputs\n",
    "        self.mixer = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=1, bias=True)\n",
    "\n",
    "        # LSTM for temporal dependency\n",
    "        fused_dim = cnn_channels # Input to LSTM is the mixed CNN output\n",
    "        self.lstm = nn.LSTM(fused_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "        # MDN Head\n",
    "        self.mdn_head = nn.Linear(hidden_dim, 3 * n_components)\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, (nn.Conv1d, nn.Conv2d)):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, X, lengths=None):\n",
    "        # Input shape X[\"main\"]: (B, T, F_in)\n",
    "        x = X[\"main\"] \n",
    "\n",
    "        # --- Debug print first candle ---\n",
    "        # if x.ndim == 3:  # batched: (B, T, F)\n",
    "        #     first_candle = x[0, 0, :]   # first sample, first time step, all features\n",
    "        #     print(\"First candle features:\", first_candle.detach().cpu().numpy())\n",
    "        # elif x.ndim == 2:  # single sequence: (T, F)\n",
    "        #     first_candle = x[0, :]      # first time step, all features\n",
    "        #     print(\"First candle features:\", first_candle.detach().cpu().numpy())\n",
    "        # else:\n",
    "        #     print(\"Unexpected shape for x:\", x.shape)\n",
    "        # 1. Time-distributed feature extraction\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.ln1(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.ln2(x)) # CHANGED: Apply LayerNorm before ReLU\n",
    "\n",
    "        # 2. CNN feature extraction\n",
    "        x = x.transpose(1, 2)   # Shape: (B, C_in, L=T)\n",
    "        x1 = F.relu(self.bn1(self.conv1(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "        x3 = F.relu(self.bn3(self.conv3(x))) # CHANGED: Apply BatchNorm before ReLU\n",
    "\n",
    "        # 3. Mix CNN outputs\n",
    "        paired = torch.stack([x1, x3], dim=1) # Shape: (B, 2, C_out, L)\n",
    "        mixed = self.mixer(paired)            # Shape: (B, 1, C_out, L)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        xf = mixed.squeeze(1).transpose(1, 2) # Shape: (B, L, C_out)\n",
    "\n",
    "        # 4. LSTM for sequence summary\n",
    "        if lengths is not None:\n",
    "            packed_input = pack_padded_sequence(\n",
    "                xf, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            _, (h_last, _) = self.lstm(packed_input)\n",
    "        else:\n",
    "            _, (h_last, _) = self.lstm(xf)\n",
    "        \n",
    "        last_h = h_last[-1] # Shape: (B, H)\n",
    "        \n",
    "        # 5. MDN head for distribution parameters\n",
    "        raw = self.mdn_head(last_h)\n",
    "        pi, mu, sigma = mdn_split_params(raw, self.n_components)\n",
    "        return {\"pi\": pi, \"mu\": mu, \"sigma\": sigma}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y_line, lengths = batch\n",
    "        mdn = self(X, lengths)\n",
    "        loss = mdn_nll_multitarget(y_line, mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"])\n",
    "    # Log everything to progress bar\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        self.log(\"val/pi_mean\", mdn[\"pi\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/pi_std\", mdn[\"pi\"].std(), prog_bar=True)\n",
    "        self.log(\"val/mu_mean\", mdn[\"mu\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/mu_std\", mdn[\"mu\"].std(), prog_bar=True)\n",
    "        self.log(\"val/sigma_mean\", mdn[\"sigma\"].mean(), prog_bar=True)\n",
    "        self.log(\"val/sigma_std\", mdn[\"sigma\"].std(), prog_bar=True)\n",
    "        \n",
    "    # # Inside your CNNLSTM_MDN class\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "    # def configure_optimizers(self):\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "    #     # Define the scheduler\n",
    "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    #         optimizer,\n",
    "    #         mode='min',      # We want to minimize the validation loss\n",
    "    #         factor=0.5,      # Reduce LR by 80% (1.0 -> 0.2)\n",
    "    #         patience=10,      # Wait 5 validation epochs with no improvement before reducing\n",
    "    #         verbose=True\n",
    "    #     )\n",
    "        \n",
    "    #     return {\n",
    "    #         \"optimizer\": optimizer,\n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": scheduler,\n",
    "    #             \"monitor\": \"val/loss\",  # The metric to watch\n",
    "    #         },\n",
    "    #     }\n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bccc1c2",
   "metadata": {},
   "source": [
    "## CNNtransformer wheightening order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9609ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "\n",
    "# --- Helper Functions and Modules ---\n",
    "\n",
    "def mdn_split_params(raw_params, n_components):\n",
    "    \"\"\"\n",
    "    Splits raw MDN output into mixture weights (pi), means (mu), and stds (sigma).\n",
    "    This function is used by each individual MDN head.\n",
    "    \"\"\"\n",
    "    B, threeK = raw_params.shape\n",
    "    assert threeK == 3 * n_components\n",
    "    raw = raw_params.view(B, n_components, 3)\n",
    "    pi = F.softmax(raw[..., 0], dim=-1)\n",
    "    mu = raw[..., 1]\n",
    "    sigma = F.softplus(raw[..., 2]) + 1e-6 # Added a small epsilon for stability\n",
    "    return pi, mu, sigma\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects positional information into the input sequence for the Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- Weighted Loss Function for Multi-Head Architecture ---\n",
    "\n",
    "def weighted_mdn_nll_multihead(y_true, mdn_params_list, weights, padding_value=-1):\n",
    "    \"\"\"\n",
    "    Calculates the weighted negative log-likelihood for a multi-headed MDN.\n",
    "    This version correctly handles multiple heads and calculates the full NLL for each.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensor): Padded target values, shape (B, num_lines).\n",
    "        mdn_params_list (list): A list of dicts, one for each head.\n",
    "        weights (Tensor): A 1D tensor of importance weights, shape (num_lines,).\n",
    "        padding_value (int): Value used for padding in y_true.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_lines = y_true.shape[1]\n",
    "    \n",
    "    for i in range(num_lines):\n",
    "        y_target = y_true[:, i:i+1]\n",
    "        pi, mu, sigma = mdn_params_list[i]['pi'], mdn_params_list[i]['mu'], mdn_params_list[i]['sigma']\n",
    "\n",
    "        # Create a mask for valid (non-padded) targets for this line\n",
    "        mask = (y_target != padding_value).squeeze()\n",
    "\n",
    "        if mask.sum() == 0:  # Skip if no valid targets for this line in the batch\n",
    "            continue\n",
    "\n",
    "        # Select only the valid data for this line's loss calculation\n",
    "        y_target_masked = y_target[mask]\n",
    "        pi_masked, mu_masked, sigma_masked = pi[mask], mu[mask], sigma[mask]\n",
    "        \n",
    "        # Use torch.distributions for a clean and stable calculation\n",
    "        dist = Normal(loc=mu_masked, scale=sigma_masked)\n",
    "        \n",
    "        # Calculate log probabilities of the target values in each Gaussian component\n",
    "        log_prob = dist.log_prob(y_target_masked.expand_as(mu_masked))\n",
    "        \n",
    "        # Mix the probabilities using the mixture weights (pi)\n",
    "        log_mix_prob = torch.log(pi_masked + 1e-8) + log_prob\n",
    "        \n",
    "        # Use logsumexp for numerical stability to get the log-likelihood\n",
    "        log_likelihood = torch.logsumexp(log_mix_prob, dim=1)\n",
    "        \n",
    "        # Calculate the mean negative log-likelihood for this line\n",
    "        line_loss = -log_likelihood.mean()\n",
    "\n",
    "        # Apply the importance weight and add to total loss\n",
    "        total_loss += weights[i] * line_loss\n",
    "\n",
    "    # If no valid lines were found in the entire batch, return a zero tensor\n",
    "    if not isinstance(total_loss, torch.Tensor):\n",
    "        return torch.tensor(0.0, device=y_true.device, requires_grad=True)\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "# --- The CNN-Transformer Model ---\n",
    "\n",
    "class cnn_transformer(pl.LightningModule):\n",
    "    def __init__(self, input_dim, cnn_out_channels=64, d_model=128, nhead=4, num_encoder_layers=2,\n",
    "                 n_components=9, num_lines=9, lr=1e-4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_lines = num_lines\n",
    "        self.n_components = n_components\n",
    "        self.lr = lr\n",
    "        \n",
    "        # 1. CNN Feature Extractor Block\n",
    "        self.cnn_extractor = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, cnn_out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(cnn_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(cnn_out_channels, d_model, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 2. Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # 4. Multi-Head MDN Output\n",
    "        self.mdn_heads = nn.ModuleList([\n",
    "            nn.Linear(d_model, 3 * n_components) for _ in range(num_lines)\n",
    "        ])\n",
    "        \n",
    "        # Importance weights for lines (exponential decay)\n",
    "        weights = torch.tensor([0.9**i for i in range(num_lines)], dtype=torch.float)\n",
    "        self.register_buffer(\"loss_weights\", weights)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            if module.bias is not None: nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "    def forward(self, X, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        X: (B, T, input_dim)\n",
    "        src_key_padding_mask: (B, T) boolean mask for padded elements in X\n",
    "        \"\"\"\n",
    "        x = X[\"main\"]\n",
    "        \n",
    "        # 1. CNN Feature Extraction\n",
    "        # Input for Conv1d needs to be (B, C_in, L), so we permute\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn_extractor(x)\n",
    "        # Permute back to (B, T, C_out) for Transformer\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # 2. Add Positional Encoding\n",
    "        # Transformer expects (T, B, C), so permute again\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.pos_encoder(x)\n",
    "        # Permute back to (B, T, C) for batch_first=True\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # 3. Transformer Encoder\n",
    "        # The mask should indicate which key values are NOT to be attended to\n",
    "        encoded_seq = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # We use the representation of the last valid timestep for prediction\n",
    "        # (A common strategy, alternatively you could use mean pooling)\n",
    "        # For simplicity, we'll take the last hidden state of the sequence.\n",
    "        sequence_summary = encoded_seq[:, -1, :] # (B, d_model)\n",
    "        \n",
    "        # 4. Get parameters from all MDN heads\n",
    "        mdn_params_list = []\n",
    "        for i in range(self.num_lines):\n",
    "            raw_params = self.mdn_heads[i](sequence_summary)\n",
    "            pi, mu, sigma = mdn_split_params(raw_params, self.n_components)\n",
    "            mdn_params_list.append({\"pi\": pi, \"mu\": mu, \"sigma\": sigma})\n",
    "\n",
    "        return mdn_params_list\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        # Create the padding mask for the transformer\n",
    "        # True values indicate positions that should be ignored.\n",
    "        max_len = X['main'].shape[1]\n",
    "        mask = torch.arange(max_len, device=self.device)[None, :] >= lengths[:, None]\n",
    "\n",
    "        mdn_params = self(X, src_key_padding_mask=mask)\n",
    "        # Use a padding value of -1 for the loss function\n",
    "        loss = weighted_mdn_nll_multihead(y, mdn_params, self.loss_weights, padding_value=-1)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y, lengths = batch\n",
    "        max_len = X['main'].shape[1]\n",
    "        mask = torch.arange(max_len, device=self.device)[None, :] >= lengths[:, None]\n",
    "        \n",
    "        mdn_params = self(X, src_key_padding_mask=mask)\n",
    "        loss = weighted_mdn_nll_multihead(y, mdn_params, self.loss_weights, padding_value=-1)\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeefa26",
   "metadata": {},
   "source": [
    "# data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95badad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "startTime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "endTime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "startIndex",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "endIndex",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "linePrice_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_9",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6d37502b-8844-4574-b3cd-d3f9d9cfeb6c",
       "rows": [
        [
         "0",
         "1514764800",
         "1515110400",
         "0",
         "4",
         null,
         "0.878016",
         "0.788209",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "1514764800",
         "1515283200",
         "0",
         "6",
         null,
         "1.05529",
         "0.923251",
         "0.828937",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "1515024000",
         "1515369600",
         "3",
         "7",
         "1.143628",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "1515456000",
         "1514937600",
         "2",
         "8",
         "1.139775",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "1515110400",
         "1515542400",
         "4",
         "9",
         "1.143279",
         "0.964469",
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "1515196800",
         "1515628800",
         "5",
         "10",
         "1.290228",
         "1.126277",
         "1.086008",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "1515283200",
         "1515888000",
         "6",
         "13",
         "1.105121",
         "1.041538",
         "0.982194",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "1515369600",
         "1516060800",
         "7",
         "15",
         "1.236932",
         "1.364445",
         "1.299815",
         null,
         "1.177543",
         "1.053524",
         null,
         null,
         null
        ],
        [
         "8",
         "1515801600",
         "1516320000",
         "12",
         "18",
         "0.954276",
         "1.173294",
         "0.785035",
         "1.238004",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "1516492800",
         "1516147200",
         "16",
         "20",
         "0.996497",
         null,
         "1.16283",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "1516060800",
         "1516924800",
         "15",
         "25",
         null,
         "0.989209",
         "1.026983",
         "0.922247",
         "1.154039",
         null,
         null,
         null,
         null
        ],
        [
         "11",
         "1515974400",
         "1517443200",
         "14",
         "31",
         "1.259327",
         null,
         "1.143742",
         "1.218046",
         "1.042605",
         "1.383168",
         null,
         null,
         null
        ],
        [
         "12",
         "1516838400",
         "1517788800",
         "24",
         "35",
         null,
         "1.67662",
         "1.476347",
         "1.322714",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "1517443200",
         "1518134400",
         "31",
         "39",
         "0.866167",
         "1.044538",
         "0.790359",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "1517702400",
         "1518048000",
         "34",
         "38",
         "0.913325",
         "0.840066",
         "0.77626",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "1517875200",
         "1518566400",
         "36",
         "44",
         "0.908825",
         "0.803359",
         null,
         "0.962592",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "1518134400",
         "1518825600",
         "39",
         "47",
         "0.772655",
         null,
         "0.723089",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "1518307200",
         "1518912000",
         "41",
         "48",
         "0.82336",
         null,
         "0.776309",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "1518480000",
         "1518912000",
         "43",
         "48",
         null,
         null,
         "0.819596",
         "1.0605",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "1518480000",
         "1519171200",
         "43",
         "51",
         "1.068102",
         "0.991338",
         null,
         "0.817215",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "20",
         "1518652800",
         "1519344000",
         "45",
         "53",
         "1.106209",
         "1.015549",
         null,
         "0.965396",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "21",
         "1518912000",
         "1519689600",
         "48",
         "57",
         "1.058517",
         "0.977161",
         null,
         "0.919841",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "22",
         "1518825600",
         "1518998400",
         "47",
         "49",
         null,
         "0.929502",
         "0.989077",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "1517875200",
         "1518048000",
         "36",
         "38",
         "0.918052",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "1518134400",
         "1518566400",
         "39",
         "44",
         "0.902621",
         null,
         "0.855058",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "25",
         "1519084800",
         "1519516800",
         "50",
         "55",
         "1.168618",
         null,
         "1.060616",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "26",
         "1519516800",
         "1519948800",
         "55",
         "60",
         "0.93202",
         "0.866519",
         null,
         "0.988669",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "27",
         "1519776000",
         "1520121600",
         "58",
         "62",
         null,
         null,
         "0.898584",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         "1520121600",
         "1519862400",
         "59",
         "62",
         null,
         null,
         null,
         "0.999443",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         "1518566400",
         "1518825600",
         "44",
         "47",
         null,
         null,
         null,
         null,
         "0.90719",
         null,
         null,
         null,
         null
        ],
        [
         "30",
         "1520035200",
         "1520640000",
         "61",
         "68",
         "1.311277",
         null,
         "1.055028",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         "1520380800",
         "1520726400",
         "65",
         "69",
         null,
         "0.923406",
         null,
         "0.970552",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "32",
         "1520467200",
         "1520985600",
         "66",
         "72",
         "1.137321",
         null,
         "1.070346",
         null,
         "1.18516",
         "1.022507",
         null,
         null,
         null
        ],
        [
         "33",
         "1520640000",
         "1521244800",
         "68",
         "75",
         "1.17662",
         "1.057056",
         "1.111053",
         "1.236402",
         "0.97799",
         "0.933635",
         null,
         null,
         null
        ],
        [
         "34",
         "1521158400",
         "1521504000",
         "74",
         "78",
         "0.924926",
         "0.869038",
         null,
         "0.830086",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         "1521244800",
         "1521504000",
         "75",
         "78",
         null,
         null,
         "0.875812",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "1521331200",
         "1521676800",
         "76",
         "80",
         "1.022609",
         null,
         null,
         "1.048557",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         "1521590400",
         "1521849600",
         "79",
         "82",
         "1.04014",
         null,
         null,
         "0.987174",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         "1521676800",
         "1522022400",
         "80",
         "84",
         null,
         "1.092904",
         null,
         "1.039106",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         "1521763200",
         "1522108800",
         "81",
         "85",
         null,
         null,
         "1.086192",
         "1.140392",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "41",
         "1521936000",
         "1522281600",
         "83",
         "87",
         null,
         null,
         "1.121892",
         "1.198509",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "42",
         "1522108800",
         "1522368000",
         "85",
         "88",
         "1.158468",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "43",
         "1522195200",
         "1522540800",
         "86",
         "90",
         null,
         null,
         "0.999198",
         "1.167526",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "44",
         "1522281600",
         "1522627200",
         "87",
         "91",
         null,
         "0.981897",
         null,
         "0.93271",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "45",
         "1522368000",
         "1522800000",
         "88",
         "93",
         null,
         null,
         null,
         null,
         "1.010566",
         "1.088278",
         null,
         null,
         null
        ],
        [
         "46",
         "1522454400",
         "1523059200",
         "89",
         "96",
         null,
         null,
         "0.98939",
         "1.074732",
         "0.93906",
         null,
         null,
         null,
         null
        ],
        [
         "47",
         "1522713600",
         "1523059200",
         "92",
         "96",
         null,
         null,
         "0.982825",
         "1.074732",
         "0.95219",
         null,
         null,
         null,
         null
        ],
        [
         "48",
         "1522281600",
         "1523404800",
         "87",
         "100",
         null,
         "0.987649",
         "1.044069",
         null,
         "0.937739",
         "1.078789",
         null,
         null,
         null
        ],
        [
         "49",
         "1522886400",
         "1523059200",
         "94",
         "96",
         null,
         null,
         "0.971884",
         null,
         "0.947813",
         null,
         null,
         null,
         null
        ],
        [
         "50",
         "1522972800",
         "1523404800",
         "95",
         "100",
         null,
         null,
         "0.991989",
         null,
         null,
         "1.024539",
         "0.948589",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 320
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>startIndex</th>\n",
       "      <th>endIndex</th>\n",
       "      <th>linePrice_1</th>\n",
       "      <th>linePrice_2</th>\n",
       "      <th>linePrice_3</th>\n",
       "      <th>linePrice_4</th>\n",
       "      <th>linePrice_5</th>\n",
       "      <th>linePrice_6</th>\n",
       "      <th>linePrice_7</th>\n",
       "      <th>linePrice_8</th>\n",
       "      <th>linePrice_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514764800</td>\n",
       "      <td>1515110400</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878016</td>\n",
       "      <td>0.788209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514764800</td>\n",
       "      <td>1515283200</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.055290</td>\n",
       "      <td>0.923251</td>\n",
       "      <td>0.828937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1515024000</td>\n",
       "      <td>1515369600</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.143628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1515456000</td>\n",
       "      <td>1514937600</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.139775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1515110400</td>\n",
       "      <td>1515542400</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.143279</td>\n",
       "      <td>0.964469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1651795200</td>\n",
       "      <td>1649116800</td>\n",
       "      <td>1555</td>\n",
       "      <td>1586</td>\n",
       "      <td>0.873150</td>\n",
       "      <td>0.825739</td>\n",
       "      <td>0.905267</td>\n",
       "      <td>0.938913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955736</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1652054400</td>\n",
       "      <td>1652227200</td>\n",
       "      <td>1589</td>\n",
       "      <td>1591</td>\n",
       "      <td>1.063729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.023085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1652572800</td>\n",
       "      <td>1651881600</td>\n",
       "      <td>1587</td>\n",
       "      <td>1595</td>\n",
       "      <td>0.813907</td>\n",
       "      <td>0.870793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.788406</td>\n",
       "      <td>0.904141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1653264000</td>\n",
       "      <td>1652227200</td>\n",
       "      <td>1591</td>\n",
       "      <td>1603</td>\n",
       "      <td>1.042211</td>\n",
       "      <td>1.075683</td>\n",
       "      <td>0.992004</td>\n",
       "      <td>0.958532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1648598400</td>\n",
       "      <td>1640304000</td>\n",
       "      <td>1453</td>\n",
       "      <td>1549</td>\n",
       "      <td>0.781703</td>\n",
       "      <td>0.741996</td>\n",
       "      <td>0.993930</td>\n",
       "      <td>0.847425</td>\n",
       "      <td>0.884394</td>\n",
       "      <td>0.71872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.647521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      startTime     endTime  startIndex  endIndex  linePrice_1  linePrice_2  \\\n",
       "0    1514764800  1515110400           0         4          NaN     0.878016   \n",
       "1    1514764800  1515283200           0         6          NaN     1.055290   \n",
       "2    1515024000  1515369600           3         7     1.143628          NaN   \n",
       "3    1515456000  1514937600           2         8     1.139775          NaN   \n",
       "4    1515110400  1515542400           4         9     1.143279     0.964469   \n",
       "..          ...         ...         ...       ...          ...          ...   \n",
       "328  1651795200  1649116800        1555      1586     0.873150     0.825739   \n",
       "330  1652054400  1652227200        1589      1591     1.063729          NaN   \n",
       "331  1652572800  1651881600        1587      1595     0.813907     0.870793   \n",
       "332  1653264000  1652227200        1591      1603     1.042211     1.075683   \n",
       "333  1648598400  1640304000        1453      1549     0.781703     0.741996   \n",
       "\n",
       "     linePrice_3  linePrice_4  linePrice_5  linePrice_6  linePrice_7  \\\n",
       "0       0.788209          NaN          NaN          NaN          NaN   \n",
       "1       0.923251     0.828937          NaN          NaN          NaN   \n",
       "2            NaN          NaN          NaN          NaN          NaN   \n",
       "3            NaN          NaN          NaN          NaN          NaN   \n",
       "4            NaN          NaN          NaN          NaN          NaN   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "328     0.905267     0.938913          NaN          NaN          NaN   \n",
       "330          NaN     1.023085          NaN          NaN          NaN   \n",
       "331          NaN          NaN          NaN          NaN          NaN   \n",
       "332     0.992004     0.958532          NaN          NaN          NaN   \n",
       "333     0.993930     0.847425     0.884394      0.71872          NaN   \n",
       "\n",
       "     linePrice_8  linePrice_9  \n",
       "0            NaN          NaN  \n",
       "1            NaN          NaN  \n",
       "2            NaN          NaN  \n",
       "3            NaN          NaN  \n",
       "4            NaN          NaN  \n",
       "..           ...          ...  \n",
       "328     0.955736          NaN  \n",
       "330          NaN          NaN  \n",
       "331     0.788406     0.904141  \n",
       "332          NaN          NaN  \n",
       "333          NaN     0.647521  \n",
       "\n",
       "[320 rows x 13 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_labels = pd.read_csv(\"/home/iatell/projects/meta-learning/data/line_seq_ordered.csv\")\n",
    "cols = [f'price_line{i}' for i in range(1, 10)]\n",
    "df_labels = df_labels.dropna(subset=cols, how='all')\n",
    "df_labels = df_labels.rename(columns={c: c.replace('price_line', 'linePrice_') \n",
    "                        for c in df_labels.columns if c.startswith('price_line')})\n",
    "df_labels.to_csv(\"/home/iatell/projects/meta-learning/data/line_seq_ordered.csv\", index=False)      \n",
    "#     # overwrites the old file\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4dd53",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677dbff",
   "metadata": {},
   "source": [
    "## simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e281d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from preprocess.multi_regression_seq_dif import preprocess_sequences_csv_multilines\n",
    "# from models.LSTM.lstm_multi_line_reg_seq_dif import LSTMMultiRegressor\n",
    "from utils.make_step import make_step\n",
    "from utils.padding_batch_reg import collate_batch\n",
    "from utils.get_init_argumens import get_init_args\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "from add_ons.drop_column import drop_columns\n",
    "from add_ons.normalize_candle_seq import add_label_normalized_candles\n",
    "from add_ons.feature_pipeline3 import FeaturePipeline\n",
    "from add_ons.candle_dif_rate_of_change_percentage import add_candle_rocp\n",
    "from add_ons.candle_rate_of_change import add_candle_ratios\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# ---------------- Evaluation ---------------- #\n",
    "@torch.no_grad()\n",
    "def evaluate_model_mdn(model, val_loader, zero_idx=0, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate CNN–LSTM–MDN model (last-output version).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    model : pl.LightningModule with MDN forward\n",
    "    val_loader : DataLoader yielding (X, y, lengths)\n",
    "    zero_idx : which mixture component is considered \"no-line\" (usually 0)\n",
    "    threshold : if pi[:,zero_idx] > threshold → predict invalid\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with mse, mae, acc, f1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_reg, all_labels_reg = [], []\n",
    "    all_preds_len, all_labels_len = [], []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in val_loader:\n",
    "            if isinstance(X_batch, dict):\n",
    "                X_batch = {k: v.to(device) for k, v in X_batch.items()}\n",
    "            else:\n",
    "                X_batch = X_batch.to(device)\n",
    "\n",
    "            y_batch = y_batch.to(device)\n",
    "            mdn = model(X_batch, lengths)\n",
    "            pi, mu, sigma = mdn[\"pi\"], mdn[\"mu\"], mdn[\"sigma\"]  # (B,K)\n",
    "\n",
    "            # regression expectation\n",
    "            y_pred = (pi * mu).sum(dim=-1)  # (B,)\n",
    "            B = y_batch.size(0)\n",
    "            y_len = (y_batch > 0).sum(dim=1)                # (B,)\n",
    "            idx = torch.clamp(y_len - 1, min=0)             # last valid index\n",
    "            y_true = y_batch[torch.arange(B, device=y_batch.device), idx]  # (B,)\n",
    "            # only last step\n",
    "            # print(\"lengths(features):\", lengths[:10])\n",
    "            # print(\"lengths(labels):\", y_len[:10])\n",
    "\n",
    "            all_preds_reg.append(y_pred.cpu().numpy())\n",
    "            all_labels_reg.append(y_true.cpu().numpy())\n",
    "\n",
    "            # validity classification\n",
    "            pi_zero = pi[:, zero_idx]  # (B,)\n",
    "            pred_valid = (pi_zero < (1 - threshold)).long()\n",
    "            true_valid = torch.ones_like(pred_valid)  # last step always valid\n",
    "\n",
    "            all_preds_len.extend(pred_valid.cpu().numpy().tolist())\n",
    "            all_labels_len.extend(true_valid.cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "        # ----- Regression metrics -----\n",
    "    all_preds_reg = np.concatenate(all_preds_reg)  # (N,)\n",
    "    all_labels_reg = np.concatenate(all_labels_reg)\n",
    "    mse = ((all_preds_reg - all_labels_reg) ** 2).mean()\n",
    "    mae = np.abs(all_preds_reg - all_labels_reg).mean()\n",
    "    # ----- Validity metrics -----\n",
    "    acc = accuracy_score(all_labels_len, all_preds_len)\n",
    "    f1 = f1_score(all_labels_len, all_preds_len, average=\"macro\")\n",
    "\n",
    "    print(\"\\n📊 Validation Metrics (MDN, last-output):\")\n",
    "    print(f\"  Regression → MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "    print(f\"  Validity   → Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=True,\n",
    "    hidden_dim=200,\n",
    "    num_layers=1,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    max_epochs=1000,\n",
    "    save_model=False,\n",
    "    return_val_accuracy = True,\n",
    "    test_mode = False,\n",
    "    early_stop = False\n",
    "):\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/lstm_model_multireg_{timestamp}.pt\"\n",
    "    meta_out  = f\"{model_out_dir}/lstm_meta_multireg_{timestamp}.pkl\"\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            # make_step(add_label_normalized_candles),\n",
    "            make_step(add_candle_rocp),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "            \n",
    "        ],\n",
    "        # norm_methods={\n",
    "        #     \"main\": {\n",
    "        #         \"upper_shadow\": \"robust\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "        #         \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "        #         \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "        #     }\n",
    "        # },\n",
    "        per_window_flags=[\n",
    "            False, \n",
    "          False, \n",
    "        #   True\n",
    "                ]\n",
    "    )\n",
    "    # Preprocess: pad linePrices and sequences\n",
    "    if do_validation:\n",
    "        train_ds, val_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline\n",
    "        )\n",
    "    else:\n",
    "        train_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=False,\n",
    "            feature_pipeline=pipeline\n",
    "        )\n",
    "        val_ds = None\n",
    "\n",
    "    sample = train_ds[0][0]  # first sample's features\n",
    "    if isinstance(sample, dict):  # multiple feature groups\n",
    "        input_dim = sample['main'].shape[1]\n",
    "    else:  # single tensor\n",
    "        input_dim = sample.shape[1]\n",
    "\n",
    "    model = CNNLSTM_MDN(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        lr=lr\n",
    "    )\n",
    "    init_args = get_init_args(model, input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, lr=lr)\n",
    "\n",
    "    model_class_info = {\n",
    "        \"module\": model.__class__.__module__,\n",
    "        \"class\": model.__class__.__name__,\n",
    "        \"init_args\": init_args\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_batch) if val_ds else None\n",
    "    \n",
    "    # --- Debug / Test mode --- #\n",
    "    if test_mode:\n",
    "        save_model = False\n",
    "        from itertools import islice\n",
    "\n",
    "        # Try to grab 3rd batch; if not available, take first\n",
    "        try:\n",
    "            batch = next(islice(iter(train_loader), 2, 3))\n",
    "        except StopIteration:\n",
    "            batch = next(iter(train_loader))\n",
    "\n",
    "        X_batch_dict, y_batch, lengths = batch\n",
    "\n",
    "        print(\"🔍 Debug batch:\")\n",
    "        if isinstance(X_batch_dict, dict):\n",
    "            print(\"  Keys in X_batch:\", list(X_batch_dict.keys()))\n",
    "        print(\"  y_batch shape:\", y_batch.shape)\n",
    "        print(\"  First label in batch:\", y_batch[0])\n",
    "\n",
    "        # --- Track real column names for each feature group ---\n",
    "        feature_names_dict = {}\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            if name == \"main\":\n",
    "                # Use actual feature columns after preprocessing\n",
    "                feature_names_dict[name] = feature_cols\n",
    "            else:\n",
    "                # For extra feature groups, fallback to generic names\n",
    "                feature_names_dict[name] = [f\"{name}_{i}\" for i in range(X_batch.shape[2])]\n",
    "\n",
    "        dfs = []\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            print(f\"\\nFeature group: {name}\")\n",
    "            print(\"  X_batch shape:\", X_batch.shape)\n",
    "            print(\"  First sequence in batch (first  steps):\\n\", X_batch[0][:])\n",
    "\n",
    "            batch_size_, seq_len, feature_dim = X_batch.shape\n",
    "            df_part = pd.DataFrame(\n",
    "                X_batch.reshape(batch_size_ * seq_len, feature_dim).numpy(),\n",
    "                columns=feature_names_dict[name]\n",
    "            )\n",
    "            dfs.append(df_part)\n",
    "\n",
    "        # Combine all feature groups horizontally\n",
    "        global df_seq\n",
    "        df_seq = pd.concat(dfs, axis=1)\n",
    "        print(\"\\n✅ Combined df_seq shape:\", df_seq.shape)\n",
    "        print(\"✅ Column names in df_seq:\", df_seq.columns.tolist())\n",
    "\n",
    "    # --- Early stopping --- #\n",
    "    if early_stop == True:\n",
    "        from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",   # metric to monitor (must be logged in your LightningModule)\n",
    "            patience=10,          # number of epochs with no improvement before stopping\n",
    "            min_delta=0.001,      # minimum improvement to qualify as \"better\"\n",
    "            mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=model_out_dir,\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks=[early_stop_callback,checkpoint_callback]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        fast_dev_run=test_mode,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        callbacks= callbacks if early_stop else None\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        trainer.save_checkpoint(model_out)\n",
    "        joblib.dump({\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"max_len_y\": max_len_y,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"scalers\": pipeline.scalers,\n",
    "    \"pipeline_config\": pipeline.export_config(),\n",
    "    \"model_class_info\": model_class_info   # ✅ save model class info\n",
    "}, meta_out)\n",
    "        \n",
    "    # --- Evaluation --- #\n",
    "    if do_validation:\n",
    "        mse, mae, acc, f1 = evaluate_model_mdn(model, val_loader)\n",
    "        if return_val_accuracy:\n",
    "            return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/seq_line_labels.csv\",\n",
    "        save_model=True,\n",
    "        do_validation=True,\n",
    "        test_mode = False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44de2e9",
   "metadata": {},
   "source": [
    "### hungarian lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "from preprocess.multi_regression_seq_dif3 import preprocess_sequences_csv_multilines\n",
    "# from models.LSTM.lstm_multi_line_reg_seq_dif import LSTMMultiRegressor\n",
    "from utils.print_batch import print_batch\n",
    "from utils.to_address import to_address\n",
    "from utils.json_to_csv import json_to_csv_in_memory\n",
    "from utils.padding_batch_reg import collate_batch\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from add_ons.feature_pipeline5 import FeaturePipeline\n",
    "from add_ons.drop_column import drop_columns\n",
    "from add_ons.candle_dif_rate_of_change_percentage2 import add_candle_rocp\n",
    "from add_ons.candle_proportion import add_candle_proportions\n",
    "from add_ons.candle_rate_of_change import add_candle_ratios\n",
    "from utils.make_step import make_step\n",
    "\n",
    "# ---------------- Evaluation ---------------- #\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds_reg, all_labels_reg = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in val_loader:\n",
    "            device = next(model.parameters()).device\n",
    "            X_batch = {k: v.to(device) for k, v in X_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Forward pass: regression only\n",
    "            y_pred = model(X_batch, lengths)\n",
    "\n",
    "            mask = (y_batch != 0).float()\n",
    "\n",
    "            # --- Hungarian assignment per batch ---\n",
    "            batch_preds = []\n",
    "            batch_labels = []\n",
    "            for i in range(y_batch.shape[0]):\n",
    "                gt_vals = y_batch[i][mask[i] > 0]  # true targets\n",
    "                preds = y_pred[i]\n",
    "\n",
    "                if len(gt_vals) == 0:\n",
    "                    continue\n",
    "\n",
    "                cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "                row_ind, col_ind = linear_sum_assignment(cost.cpu().numpy())\n",
    "\n",
    "                matched_preds = preds[col_ind].cpu().numpy()\n",
    "                matched_labels = gt_vals[row_ind].cpu().numpy()\n",
    "\n",
    "                batch_preds.extend(matched_preds.tolist())\n",
    "                batch_labels.extend(matched_labels.tolist())\n",
    "\n",
    "            all_preds_reg.extend(batch_preds)\n",
    "            all_labels_reg.extend(batch_labels)\n",
    "\n",
    "    # Convert to arrays\n",
    "    all_preds_reg = np.array(all_preds_reg)\n",
    "    all_labels_reg = np.array(all_labels_reg)\n",
    "\n",
    "    # Regression metrics\n",
    "    mse = ((all_preds_reg - all_labels_reg) ** 2).mean()\n",
    "    mae = np.abs(all_preds_reg - all_labels_reg).mean()\n",
    "\n",
    "    print(\"\\n📊 Validation Metrics (Hungarian matched):\")\n",
    "    print(f\"  Regression → MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "\n",
    "    return {\"mse\": mse, \"mae\": mae}\n",
    "\n",
    "\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=True,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    lr=0.001,\n",
    "    batch_size=50,\n",
    "    max_epochs=300,\n",
    "    save_model=True,\n",
    "    return_val_accuracy = True,\n",
    "    test_mode = True,\n",
    "    early_stop = False\n",
    "):\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            # make_step(add_label_normalized_candles),\n",
    "            make_step(add_candle_rocp),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "            \n",
    "        ],\n",
    "        # norm_methods={\n",
    "        #     \"main\": {\n",
    "        #         \"upper_shadow\": \"robust\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "        #         \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "        #         \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "        #     }\n",
    "        # },\n",
    "        per_window_flags=[\n",
    "            False, \n",
    "          False, \n",
    "        #   True\n",
    "                ]\n",
    "    )\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/lstm_model_multireg_multihead_{timestamp}.pt\"\n",
    "    meta_out  = f\"{model_out_dir}/lstm_meta_multireg_multihead_{timestamp}.pkl\"\n",
    "\n",
    "    # Preprocess: pad linePrices and sequences\n",
    "    if do_validation:\n",
    "        train_ds, val_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline\n",
    "        )\n",
    "    else:\n",
    "        train_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=False\n",
    "        )\n",
    "        val_ds = None\n",
    "\n",
    "    sample = train_ds[0][0]  # first sample's features\n",
    "    if isinstance(sample, dict):  # multiple feature groups\n",
    "        input_dim = sample['main'].shape[1]\n",
    "    else:  # single tensor\n",
    "        input_dim = sample.shape[1]\n",
    "\n",
    "    model = LSTMMultiRegressor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        max_len_y=max_len_y,\n",
    "        lr=lr\n",
    "    )\n",
    "    init_args = {\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"max_len_y\": max_len_y,\n",
    "    \"lr\": lr\n",
    "}\n",
    "\n",
    "    model_class_info = {\n",
    "        \"module\": model.__class__.__module__,\n",
    "        \"class\": model.__class__.__name__,\n",
    "        \"init_args\": init_args\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_batch) if val_ds else None\n",
    "    # --- Early stopping --- #\n",
    "    if early_stop == True:\n",
    "        from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",   # metric to monitor (must be logged in your LightningModule)\n",
    "            patience=10,          # number of epochs with no improvement before stopping\n",
    "            min_delta=0.001,      # minimum improvement to qualify as \"better\"\n",
    "            mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=model_out_dir,\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks=[early_stop_callback,checkpoint_callback]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        fast_dev_run=test_mode,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        callbacks= callbacks if early_stop else None\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # --- Debug / Test mode --- #\n",
    "    if test_mode:\n",
    "        save_model = False\n",
    "        from itertools import islice\n",
    "\n",
    "        # Try to grab 3rd batch; if not available, take first\n",
    "        try:\n",
    "            batch = next(islice(iter(train_loader), 2, 3))\n",
    "        except StopIteration:\n",
    "            batch = next(iter(train_loader))\n",
    "\n",
    "        X_batch_dict, y_batch, lengths = batch\n",
    "\n",
    "        print(\"🔍 Debug batch:\")\n",
    "        if isinstance(X_batch_dict, dict):\n",
    "            print(\"  Keys in X_batch:\", list(X_batch_dict.keys()))\n",
    "        print(\"  y_batch shape:\", y_batch.shape)\n",
    "        print(\"  First label in batch:\", y_batch[0])\n",
    "\n",
    "        # --- Track real column names for each feature group ---\n",
    "        feature_names_dict = {}\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            if name == \"main\":\n",
    "                # Use actual feature columns after preprocessing\n",
    "                feature_names_dict[name] = feature_cols\n",
    "            else:\n",
    "                # For extra feature groups, fallback to generic names\n",
    "                feature_names_dict[name] = [f\"{name}_{i}\" for i in range(X_batch.shape[2])]\n",
    "\n",
    "        dfs = []\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            print(f\"\\nFeature group: {name}\")\n",
    "            print(\"  X_batch shape:\", X_batch.shape)\n",
    "            print(\"  First sequence in batch (first  steps):\\n\", X_batch[0][:])\n",
    "\n",
    "            batch_size_, seq_len, feature_dim = X_batch.shape\n",
    "            df_part = pd.DataFrame(\n",
    "                X_batch.reshape(batch_size_ * seq_len, feature_dim).numpy(),\n",
    "                columns=feature_names_dict[name]\n",
    "            )\n",
    "            dfs.append(df_part)\n",
    "\n",
    "        # Combine all feature groups horizontally\n",
    "        global df_seq\n",
    "        df_seq = pd.concat(dfs, axis=1)\n",
    "        print(\"\\n✅ Combined df_seq shape:\", df_seq.shape)\n",
    "        print(\"✅ Column names in df_seq:\", df_seq.columns.tolist())\n",
    "\n",
    "        \n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        trainer.save_checkpoint(model_out)\n",
    "        joblib.dump({\n",
    "            \"input_dim\": input_dim,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"max_len_y\": max_len_y,\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"scalers\": pipeline.scalers,\n",
    "            \"pipeline_config\": pipeline.export_config(),\n",
    "            \"model_class_info\": model_class_info \n",
    "        }, meta_out)\n",
    "        print(f\"✅ Model saved to {model_out}\")\n",
    "        print(f\"✅ Meta saved to {meta_out}\")\n",
    "\n",
    "\n",
    "        \n",
    "    # --- Evaluation --- #\n",
    "    if do_validation:\n",
    "        metrics = evaluate_model(model, val_loader)\n",
    "        if return_val_accuracy:\n",
    "            return metrics\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/line_seq_ordered.csv\",\n",
    "        do_validation=True,\n",
    "        test_mode = False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a7940",
   "metadata": {},
   "source": [
    "## ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb16e4",
   "metadata": {},
   "source": [
    "### cnn lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d529c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG SAMPLE CHECK (Torch mode) ===\n",
      "\n",
      "--- Sequence 0 ---\n",
      "Label: [1.235186 0.       0.       0.       0.       0.       0.       0.\n",
      " 0.      ] Encoded (padded): [1.235186 0.       0.       0.       0.       0.       0.       0.\n",
      " 0.      ]\n",
      "[main] Shape: (3, 12)\n",
      "[main] First few rows:\n",
      " [[ 0.0334583  -0.0164952  -0.08295181 -0.05172484  0.0091133   0.06722008\n",
      "   0.05172484  0.3         1.3036697   1.3155504   1.1531377   1.2362376 ]\n",
      " [-0.05151443 -0.0062422   0.04603237  0.0048193   0.05244192  0.02449848\n",
      "   0.00457536  0.7         1.236512    1.3073386   1.2062193   1.2421954 ]\n",
      " [ 0.00163378 -0.04961828 -0.31281227 -0.19497368  0.00318     0.17110091\n",
      "   0.19259259  0.3         1.2385321   1.2424706   0.8288991   1.        ]]\n",
      "==========================\n",
      "\n",
      "features ['open_dif', 'high_dif', 'low_dif', 'close_dif', 'upper_shadow', 'lower_shadow', 'body', 'color', 'open_prop', 'high_prop', 'low_prop', 'close_prop']\n",
      "🔍 Debug batch:\n",
      "  Keys in X_batch: ['main']\n",
      "  y_batch shape: torch.Size([2, 9])\n",
      "  First label in batch: tensor([1.2352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "\n",
      "Feature group: main\n",
      "  X_batch shape: torch.Size([2, 3, 12])\n",
      "  First sequence in batch (first  steps):\n",
      " tensor([[ 0.0335, -0.0165, -0.0830, -0.0517,  0.0091,  0.0672,  0.0517,  0.3000,\n",
      "          1.3037,  1.3156,  1.1531,  1.2362],\n",
      "        [-0.0515, -0.0062,  0.0460,  0.0048,  0.0524,  0.0245,  0.0046,  0.7000,\n",
      "          1.2365,  1.3073,  1.2062,  1.2422],\n",
      "        [ 0.0016, -0.0496, -0.3128, -0.1950,  0.0032,  0.1711,  0.1926,  0.3000,\n",
      "          1.2385,  1.2425,  0.8289,  1.0000]])\n",
      "\n",
      "✅ Combined df_seq shape: (6, 12)\n",
      "✅ Column names in df_seq: ['open_dif', 'high_dif', 'low_dif', 'close_dif', 'upper_shadow', 'lower_shadow', 'body', 'color', 'open_prop', 'high_prop', 'low_prop', 'close_prop']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type      | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | fc1           | Linear    | 195    | train\n",
      "1 | ln1           | LayerNorm | 30     | train\n",
      "2 | k1            | Conv1d    | 240    | train\n",
      "3 | k3            | Conv1d    | 690    | train\n",
      "4 | fusion_conv2d | Conv2d    | 3      | train\n",
      "5 | lstm          | LSTM      | 6.3 K  | train\n",
      "6 | mdn_head      | Linear    | 891    | train\n",
      "----------------------------------------------------\n",
      "8.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.3 K     Total params\n",
      "0.033     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4a0ac5bd0e4c7a834a906ea7f5a3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from preprocess.multi_regression_seq_dif3 import preprocess_sequences_csv_multilines\n",
    "# from models.LSTM.lstm_multi_line_reg_seq_dif import LSTMMultiRegressor\n",
    "from utils.make_step import make_step\n",
    "from utils.padding_batch_reg import collate_batch\n",
    "from utils.get_init_argumens import get_init_args\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "from add_ons.drop_columns2 import drop_columns\n",
    "from add_ons.normalize_candle_seq import add_label_normalized_candles\n",
    "from add_ons.feature_pipeline5 import FeaturePipeline\n",
    "from add_ons.candle_dif_rate_of_change_percentage2 import add_candle_rocp\n",
    "from add_ons.candle_rate_of_change import add_candle_ratios\n",
    "from add_ons.candle_proportion_simple import add_candle_shape_features\n",
    "from sklearn.metrics import accuracy_score, f1_score,mean_squared_error,mean_absolute_error\n",
    "from utils.to_address import to_address\n",
    "# ---------------- Evaluation ---------------- #\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model_mdn(model, val_loader, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate CNN–LSTM–MDN model (multi-head, top-pi selection per line).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    model : pl.LightningModule with multi-head MDN forward\n",
    "    val_loader : DataLoader yielding (X, y, lengths)\n",
    "    threshold : optional threshold for validity classification\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with mse, mae, acc, f1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_reg, all_labels_reg = [], []\n",
    "    all_preds_len, all_labels_len = [], []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for X_batch, y_batch, lengths in val_loader:\n",
    "        # Move to device\n",
    "        if isinstance(X_batch, dict):\n",
    "            X_batch = {k: v.to(device) for k, v in X_batch.items()}\n",
    "        else:\n",
    "            X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        mdn_params = model(X_batch, lengths)\n",
    "\n",
    "        B, num_lines = y_batch.shape\n",
    "        y_pred_lines = []\n",
    "\n",
    "        for i in range(num_lines):\n",
    "            pi, mu = mdn_params['pi'], mdn_params['mu']  # both (B, n_components)\n",
    "            \n",
    "            # Pick component with highest pi per sample\n",
    "            top_idx = torch.argmax(pi, dim=1, keepdim=True)     # (B,1)\n",
    "            selected_mu = mu.gather(1, top_idx).squeeze(1)     # (B,)\n",
    "\n",
    "            # Mask padded targets\n",
    "            mask = (y_batch[:, i] != 0)\n",
    "            selected_mu[~mask] = 0.0\n",
    "\n",
    "            y_pred_lines.append(selected_mu)\n",
    "\n",
    "        y_pred_all = torch.stack(y_pred_lines, dim=1)  # (B, num_lines)\n",
    "\n",
    "        # Last valid step per sample\n",
    "        y_len = (y_batch > 0).sum(dim=1)\n",
    "        idx = torch.clamp(y_len - 1, min=0)\n",
    "        y_true = y_batch[torch.arange(B), idx]\n",
    "        y_pred = y_pred_all[torch.arange(B), idx]\n",
    "\n",
    "        all_preds_reg.append(y_pred.cpu().numpy())\n",
    "        all_labels_reg.append(y_true.cpu().numpy())\n",
    "\n",
    "        # --- Validity classification ---\n",
    "        pred_valid_lines = []\n",
    "        for i in range(num_lines):\n",
    "            pi = mdn_params['pi']    # (B, n_components)\n",
    "            top_idx = torch.argmax(pi, dim=1, keepdim=True)\n",
    "            pi_max = pi.gather(1, top_idx).squeeze(1)\n",
    "            pred_valid_lines.append((pi_max > threshold).long())\n",
    "\n",
    "        pred_valid_all = torch.stack(pred_valid_lines, dim=1)\n",
    "        pred_valid_last = pred_valid_all[torch.arange(B), idx]\n",
    "        true_valid_last = torch.ones_like(pred_valid_last)\n",
    "\n",
    "        all_preds_len.extend(pred_valid_last.cpu().numpy().tolist())\n",
    "        all_labels_len.extend(true_valid_last.cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    y_pred_reg = np.concatenate(all_preds_reg)\n",
    "    y_true_reg = np.concatenate(all_labels_reg)\n",
    "\n",
    "    mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
    "    mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "    acc = accuracy_score(all_labels_len, all_preds_len)\n",
    "    f1 = f1_score(all_labels_len, all_preds_len)\n",
    "\n",
    "    print(\"mse:\", mse, \"mae:\", mae, \"acc:\", acc, \"f1:\", f1)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=False,\n",
    "    hidden_dim=32,\n",
    "    num_layers=1,\n",
    "    lr=0.001,\n",
    "    feature_eng=15,\n",
    "    n_components=9,\n",
    "    dropout = 0.1,\n",
    "    batch_size=2,\n",
    "    max_epochs=600,\n",
    "    save_model=False,\n",
    "    return_val_accuracy = True,\n",
    "    test_mode = False,\n",
    "    early_stop = False\n",
    "):\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/lstm_model_multireg_{timestamp}.pt\"\n",
    "    meta_out  = f\"{model_out_dir}/lstm_meta_multireg_{timestamp}.pkl\"\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            # make_step(add_label_normalized_candles),\n",
    "            make_step(add_candle_rocp),\n",
    "            make_step(add_candle_shape_features),\n",
    "            make_step(add_label_normalized_candles),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "            \n",
    "        ],\n",
    "        # norm_methods={\n",
    "        #     \"main\": {\n",
    "        #         \"upper_shadow\": \"standard\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "        #         # \"open_dif\":\"standard\",\"close_dif\":\"standard\",\"high_dif\":\"standard\",\"low_dif\":\"standard\"\n",
    "        #         # \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "        #         # \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "        #     }\n",
    "        # },\n",
    "        per_window_flags=[\n",
    "            False, \n",
    "          False, \n",
    "          True,\n",
    "          True\n",
    "                ]\n",
    "    )\n",
    "    # Preprocess: pad linePrices and sequences\n",
    "\n",
    "    if do_validation:\n",
    "        train_ds, val_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline,\n",
    "            preserve_order= True\n",
    "        )\n",
    "    else:\n",
    "        train_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline,\n",
    "            preserve_order= True,\n",
    "        )\n",
    "        val_ds = None\n",
    "    print(\"features\",feature_cols)\n",
    "    sample = train_ds[0][0]  # first sample's features\n",
    "    if isinstance(sample, dict):  # multiple feature groups\n",
    "        input_dim = sample['main'].shape[1]\n",
    "    else:  # single tensor\n",
    "        input_dim = sample.shape[1]\n",
    "\n",
    "    model = cnn_lstm(input_dim=input_dim, feature_eng= feature_eng, hidden_dim=hidden_dim, \n",
    "                     n_components=n_components,  lr=lr, dropout=dropout,num_lines=max_len_y)\n",
    "    init_args = get_init_args(model, input_dim=input_dim,feature_eng= feature_eng\n",
    "                              ,hidden_dim=hidden_dim, n_components=n_components,\n",
    "                              lr=lr, dropout=dropout,num_lines=max_len_y)\n",
    "    model_class_info = {\n",
    "        \"module\": model.__class__.__module__,\n",
    "        \"class\": model.__class__.__name__,\n",
    "        \"init_args\": init_args\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_batch) if val_ds else None\n",
    "    \n",
    "    # --- Debug / Test mode --- #\n",
    "    if test_mode:\n",
    "        save_model = False\n",
    "        from itertools import islice\n",
    "\n",
    "        # Try to grab 3rd batch; if not available, take first\n",
    "        try:\n",
    "            batch = next(islice(iter(train_loader), 2, 3))\n",
    "        except StopIteration:\n",
    "            batch = next(iter(train_loader))\n",
    "\n",
    "        X_batch_dict, y_batch, lengths = batch\n",
    "\n",
    "        print(\"🔍 Debug batch:\")\n",
    "        if isinstance(X_batch_dict, dict):\n",
    "            print(\"  Keys in X_batch:\", list(X_batch_dict.keys()))\n",
    "        print(\"  y_batch shape:\", y_batch.shape)\n",
    "        print(\"  First label in batch:\", y_batch[0])\n",
    "\n",
    "        # --- Track real column names for each feature group ---\n",
    "        feature_names_dict = {}\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            if name == \"main\":\n",
    "                # Use actual feature columns after preprocessing\n",
    "                feature_names_dict[name] = feature_cols\n",
    "            else:\n",
    "                # For extra feature groups, fallback to generic names\n",
    "                feature_names_dict[name] = [f\"{name}_{i}\" for i in range(X_batch.shape[2])]\n",
    "\n",
    "        dfs = []\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            print(f\"\\nFeature group: {name}\")\n",
    "            print(\"  X_batch shape:\", X_batch.shape)\n",
    "            print(\"  First sequence in batch (first  steps):\\n\", X_batch[0][:])\n",
    "\n",
    "            batch_size_, seq_len, feature_dim = X_batch.shape\n",
    "            df_part = pd.DataFrame(\n",
    "                X_batch.reshape(batch_size_ * seq_len, feature_dim).numpy(),\n",
    "                columns=feature_names_dict[name]\n",
    "            )\n",
    "            dfs.append(df_part)\n",
    "\n",
    "        # Combine all feature groups horizontally\n",
    "        global df_seq\n",
    "        df_seq = pd.concat(dfs, axis=1)\n",
    "        print(\"\\n✅ Combined df_seq shape:\", df_seq.shape)\n",
    "        print(\"✅ Column names in df_seq:\", df_seq.columns.tolist())\n",
    "\n",
    "    # --- Early stopping --- #\n",
    "    if early_stop == True:\n",
    "        from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",   # metric to monitor (must be logged in your LightningModule)\n",
    "            patience=10,          # number of epochs with no improvement before stopping\n",
    "            min_delta=0.001,      # minimum improvement to qualify as \"better\"\n",
    "            mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=model_out_dir,\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks=[early_stop_callback,checkpoint_callback]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        fast_dev_run=test_mode,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        callbacks= callbacks if early_stop else None\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        trainer.save_checkpoint(model_out)\n",
    "        joblib.dump({\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"max_len_y\": max_len_y,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"scalers\": pipeline.scalers,\n",
    "    \"pipeline_config\": pipeline.export_config(),\n",
    "    \"model_class_info\": model_class_info   # ✅ save model class info\n",
    "}, meta_out)\n",
    "        \n",
    "    # --- Evaluation --- #\n",
    "    if do_validation:\n",
    "        mse, mae, acc, f1 = evaluate_model_mdn(model, val_loader)\n",
    "        if return_val_accuracy:\n",
    "            return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/debug_test_seq.csv\",\n",
    "        save_model=True,\n",
    "        do_validation=False,\n",
    "        test_mode = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0fd2dc",
   "metadata": {},
   "source": [
    "### cnn transforemer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948aa8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG SAMPLE CHECK (Torch mode) ===\n",
      "\n",
      "--- Sequence 0 ---\n",
      "Label: [1.143628 0.       0.       0.       0.       0.       0.       0.\n",
      " 0.      ] Encoded (padded): [1.143628 0.       0.       0.       0.       0.       0.       0.\n",
      " 0.      ]\n",
      "Shape: (5, 4)\n",
      "First few rows of sequence:\n",
      " [[ 0.01562355 -0.00180042 -0.01639293  0.0093857 ]\n",
      " [ 0.00938704  0.12409948  0.04899828  0.12622231]\n",
      " [ 0.12622082 -0.00192766  0.09665822  0.00645032]\n",
      " [ 0.00645032 -0.00251821 -0.02505807 -0.05388233]\n",
      " [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cnn_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 304\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m: mse, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m: mae, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1}\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/iatell/projects/meta-learning/data/line_seq_ordered.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 190\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data_csv, labels_csv, model_out_dir, do_validation, hidden_dim, num_layers, lr, batch_size, max_epochs, save_model, return_val_accuracy, test_mode, early_stop)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# single tensor\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 190\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_transformer\u001b[49m(input_dim, feature_eng\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, num_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m    191\u001b[0m )\n\u001b[1;32m    192\u001b[0m init_args \u001b[38;5;241m=\u001b[39m get_init_args(model, input_dim\u001b[38;5;241m=\u001b[39minput_dim,num_lines\u001b[38;5;241m=\u001b[39m max_len_y )\n\u001b[1;32m    194\u001b[0m model_class_info \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m,\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: init_args\n\u001b[1;32m    198\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from preprocess.multi_regression_seq_dif2 import preprocess_sequences_csv_multilines\n",
    "# from models.LSTM.lstm_multi_line_reg_seq_dif import LSTMMultiRegressor\n",
    "from utils.make_step import make_step\n",
    "from utils.padding_batch_reg import collate_batch\n",
    "from utils.get_init_argumens import get_init_args\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "from add_ons.drop_column import drop_columns\n",
    "from add_ons.normalize_candle_seq import add_label_normalized_candles\n",
    "from add_ons.feature_pipeline4 import FeaturePipeline\n",
    "from add_ons.candle_dif_rate_of_change_percentage2 import add_candle_rocp\n",
    "from add_ons.candle_rate_of_change import add_candle_ratios\n",
    "from sklearn.metrics import accuracy_score, f1_score,mean_squared_error,mean_absolute_error\n",
    "from utils.to_address import to_address\n",
    "# ---------------- Evaluation ---------------- #\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model_mdn(model, val_loader, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Evaluate CNN–LSTM–MDN model (multi-head, top-pi selection per line).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    model : pl.LightningModule with multi-head MDN forward\n",
    "    val_loader : DataLoader yielding (X, y, lengths)\n",
    "    threshold : optional threshold for validity classification\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with mse, mae, acc, f1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_reg, all_labels_reg = [], []\n",
    "    all_preds_len, all_labels_len = [], []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for X_batch, y_batch, lengths in val_loader:\n",
    "        # Move to device\n",
    "        if isinstance(X_batch, dict):\n",
    "            X_batch = {k: v.to(device) for k, v in X_batch.items()}\n",
    "        else:\n",
    "            X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        mdn_params = model(X_batch, lengths)\n",
    "\n",
    "        B, num_lines = y_batch.shape\n",
    "        y_pred_lines = []\n",
    "\n",
    "        for i in range(num_lines):\n",
    "            pi, mu = mdn_params['pi'], mdn_params['mu']  # both (B, n_components)\n",
    "            \n",
    "            # Pick component with highest pi per sample\n",
    "            top_idx = torch.argmax(pi, dim=1, keepdim=True)     # (B,1)\n",
    "            selected_mu = mu.gather(1, top_idx).squeeze(1)     # (B,)\n",
    "\n",
    "            # Mask padded targets\n",
    "            mask = (y_batch[:, i] != 0)\n",
    "            selected_mu[~mask] = 0.0\n",
    "\n",
    "            y_pred_lines.append(selected_mu)\n",
    "\n",
    "        y_pred_all = torch.stack(y_pred_lines, dim=1)  # (B, num_lines)\n",
    "\n",
    "        # Last valid step per sample\n",
    "        y_len = (y_batch > 0).sum(dim=1)\n",
    "        idx = torch.clamp(y_len - 1, min=0)\n",
    "        y_true = y_batch[torch.arange(B), idx]\n",
    "        y_pred = y_pred_all[torch.arange(B), idx]\n",
    "\n",
    "        all_preds_reg.append(y_pred.cpu().numpy())\n",
    "        all_labels_reg.append(y_true.cpu().numpy())\n",
    "\n",
    "        # --- Validity classification ---\n",
    "        pred_valid_lines = []\n",
    "        for i in range(num_lines):\n",
    "            pi = mdn_params['pi']    # (B, n_components)\n",
    "            top_idx = torch.argmax(pi, dim=1, keepdim=True)\n",
    "            pi_max = pi.gather(1, top_idx).squeeze(1)\n",
    "            pred_valid_lines.append((pi_max > threshold).long())\n",
    "\n",
    "        pred_valid_all = torch.stack(pred_valid_lines, dim=1)\n",
    "        pred_valid_last = pred_valid_all[torch.arange(B), idx]\n",
    "        true_valid_last = torch.ones_like(pred_valid_last)\n",
    "\n",
    "        all_preds_len.extend(pred_valid_last.cpu().numpy().tolist())\n",
    "        all_labels_len.extend(true_valid_last.cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    y_pred_reg = np.concatenate(all_preds_reg)\n",
    "    y_true_reg = np.concatenate(all_labels_reg)\n",
    "\n",
    "    mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
    "    mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "    acc = accuracy_score(all_labels_len, all_preds_len)\n",
    "    f1 = f1_score(all_labels_len, all_preds_len)\n",
    "\n",
    "    print(\"mse:\", mse, \"mae:\", mae, \"acc:\", acc, \"f1:\", f1)\n",
    "    return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=True,\n",
    "    hidden_dim=32,\n",
    "    num_layers=1,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    max_epochs=500,\n",
    "    save_model=False,\n",
    "    return_val_accuracy = True,\n",
    "    test_mode = False,\n",
    "    early_stop = False\n",
    "):\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/lstm_model_multireg_{timestamp}.pt\"\n",
    "    meta_out  = f\"{model_out_dir}/lstm_meta_multireg_{timestamp}.pkl\"\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            # make_step(add_label_normalized_candles),\n",
    "            make_step(add_candle_rocp),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "            \n",
    "        ],\n",
    "        # norm_methods={\n",
    "        #     \"main\": {\n",
    "        #         \"upper_shadow\": \"robust\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "        #         \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "        #         \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "        #     }\n",
    "        # },\n",
    "        per_window_flags=[\n",
    "            False, \n",
    "          False, \n",
    "        #   True\n",
    "                ]\n",
    "    )\n",
    "    # Preprocess: pad linePrices and sequences\n",
    "\n",
    "    if do_validation:\n",
    "        train_ds, val_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline,\n",
    "            preserve_order= True\n",
    "        )\n",
    "    else:\n",
    "        train_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=False,\n",
    "            feature_pipeline=pipeline,\n",
    "            preserve_order= True\n",
    "        )\n",
    "        val_ds = None\n",
    "\n",
    "    sample = train_ds[0][0]  # first sample's features\n",
    "    if isinstance(sample, dict):  # multiple feature groups\n",
    "        input_dim = sample['main'].shape[1]\n",
    "    else:  # single tensor\n",
    "        input_dim = sample.shape[1]\n",
    "\n",
    "    model = cnn_transformer(input_dim, feature_eng=15, hidden_dim=32, n_components=9, num_lines=9, lr=1e-3, dropout=0.1\n",
    "    )\n",
    "    init_args = get_init_args(model, input_dim=input_dim,num_lines= max_len_y )\n",
    "\n",
    "    model_class_info = {\n",
    "        \"module\": model.__class__.__module__,\n",
    "        \"class\": model.__class__.__name__,\n",
    "        \"init_args\": init_args\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_batch) if val_ds else None\n",
    "    \n",
    "    # --- Debug / Test mode --- #\n",
    "    if test_mode:\n",
    "        save_model = False\n",
    "        from itertools import islice\n",
    "\n",
    "        # Try to grab 3rd batch; if not available, take first\n",
    "        try:\n",
    "            batch = next(islice(iter(train_loader), 2, 3))\n",
    "        except StopIteration:\n",
    "            batch = next(iter(train_loader))\n",
    "\n",
    "        X_batch_dict, y_batch, lengths = batch\n",
    "\n",
    "        print(\"🔍 Debug batch:\")\n",
    "        if isinstance(X_batch_dict, dict):\n",
    "            print(\"  Keys in X_batch:\", list(X_batch_dict.keys()))\n",
    "        print(\"  y_batch shape:\", y_batch.shape)\n",
    "        print(\"  First label in batch:\", y_batch[0])\n",
    "\n",
    "        # --- Track real column names for each feature group ---\n",
    "        feature_names_dict = {}\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            if name == \"main\":\n",
    "                # Use actual feature columns after preprocessing\n",
    "                feature_names_dict[name] = feature_cols\n",
    "            else:\n",
    "                # For extra feature groups, fallback to generic names\n",
    "                feature_names_dict[name] = [f\"{name}_{i}\" for i in range(X_batch.shape[2])]\n",
    "\n",
    "        dfs = []\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            print(f\"\\nFeature group: {name}\")\n",
    "            print(\"  X_batch shape:\", X_batch.shape)\n",
    "            print(\"  First sequence in batch (first  steps):\\n\", X_batch[0][:])\n",
    "\n",
    "            batch_size_, seq_len, feature_dim = X_batch.shape\n",
    "            df_part = pd.DataFrame(\n",
    "                X_batch.reshape(batch_size_ * seq_len, feature_dim).numpy(),\n",
    "                columns=feature_names_dict[name]\n",
    "            )\n",
    "            dfs.append(df_part)\n",
    "\n",
    "        # Combine all feature groups horizontally\n",
    "        global df_seq\n",
    "        df_seq = pd.concat(dfs, axis=1)\n",
    "        print(\"\\n✅ Combined df_seq shape:\", df_seq.shape)\n",
    "        print(\"✅ Column names in df_seq:\", df_seq.columns.tolist())\n",
    "\n",
    "    # --- Early stopping --- #\n",
    "    if early_stop == True:\n",
    "        from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",   # metric to monitor (must be logged in your LightningModule)\n",
    "            patience=10,          # number of epochs with no improvement before stopping\n",
    "            min_delta=0.001,      # minimum improvement to qualify as \"better\"\n",
    "            mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=model_out_dir,\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks=[early_stop_callback,checkpoint_callback]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        fast_dev_run=test_mode,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        callbacks= callbacks if early_stop else None\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        trainer.save_checkpoint(model_out)\n",
    "        joblib.dump({\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"max_len_y\": max_len_y,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"scalers\": pipeline.scalers,\n",
    "    \"pipeline_config\": pipeline.export_config(),\n",
    "    \"model_class_info\": model_class_info   # ✅ save model class info\n",
    "}, meta_out)\n",
    "        \n",
    "    # --- Evaluation --- #\n",
    "    if do_validation:\n",
    "        mse, mae, acc, f1 = evaluate_model_mdn(model, val_loader)\n",
    "        if return_val_accuracy:\n",
    "            return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/line_seq_ordered.csv\",\n",
    "        save_model=True,\n",
    "        do_validation=True,\n",
    "        test_mode = False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3023ae",
   "metadata": {},
   "source": [
    "### two head lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91951ca2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | lstm        | LSTM              | 68.6 K | train\n",
      "1 | fc_reg      | Linear            | 774    | train\n",
      "2 | fc_len      | Linear            | 774    | train\n",
      "3 | loss_fn_reg | MSELoss           | 0      | train\n",
      "4 | loss_fn_len | BCEWithLogitsLoss | 0      | train\n",
      "----------------------------------------------------------\n",
      "70.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "70.2 K    Total params\n",
      "0.281     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG SAMPLE CHECK (Torch mode) ===\n",
      "\n",
      "--- Sequence 0 ---\n",
      "Label: [1.086008 1.126277 1.165107 0.970955 0.       0.      ] Encoded (padded): [1.086008 1.126277 1.165107 0.970955 0.       0.      ]\n",
      "[main] Shape: (5, 4)\n",
      "[main] First few rows:\n",
      " [[ 0.00645032 -0.00251821 -0.02505807 -0.05388233]\n",
      " [-0.04985064 -0.0454773  -0.17924407 -0.07724382]\n",
      " [-0.08115927 -0.05037893  0.09358804 -0.03372177]\n",
      " [-0.03365467 -0.03511871 -0.06278902  0.03521458]\n",
      " [ 0.03742796  0.00087057 -0.13184595 -0.11191386]]\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6b9bc314ea4e17b85c57d428d1d2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to models/saved_models/lstm_model_multireg_multihead_20250913_143827.pt\n",
      "✅ Meta saved to models/saved_models/lstm_meta_multireg_multihead_20250913_143827.pkl\n",
      "\n",
      "📊 Validation Metrics:\n",
      "  Regression → MSE: 0.454865, MAE: 0.510467\n",
      "  Length     → Acc: 0.0667, F1: 0.0096\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "from preprocess.multi_regression_seq_dif3 import preprocess_sequences_csv_multilines\n",
    "# from models.LSTM.lstm_multi_line_reg_seq_dif import LSTMMultiRegressor\n",
    "from utils.print_batch import print_batch\n",
    "from utils.to_address import to_address\n",
    "from utils.json_to_csv import json_to_csv_in_memory\n",
    "from utils.padding_batch_reg import collate_batch\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from add_ons.feature_pipeline5 import FeaturePipeline\n",
    "from add_ons.drop_column import drop_columns\n",
    "from add_ons.candle_dif_rate_of_change_percentage2 import add_candle_rocp\n",
    "from add_ons.candle_proportion import add_candle_proportions\n",
    "from add_ons.candle_rate_of_change import add_candle_ratios\n",
    "from utils.make_step import make_step\n",
    "\n",
    "# ---------------- Evaluation ---------------- #\n",
    "def evaluate_model(model, val_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds_reg, all_labels_reg = [], []\n",
    "    all_preds_len, all_labels_len = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in val_loader:\n",
    "            # Send to same device as model\n",
    "            device = next(model.parameters()).device\n",
    "            X_batch = {k: v.to(device) for k, v in X_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Forward pass: regression + length logits\n",
    "            y_pred, len_logits = model(X_batch, lengths)\n",
    "\n",
    "            # Regression targets\n",
    "            all_preds_reg.append(y_pred.cpu().numpy())\n",
    "            all_labels_reg.append(y_batch.cpu().numpy())\n",
    "\n",
    "            # Length targets\n",
    "            true_lengths = lengths.cpu().numpy()\n",
    "            pred_lengths = model.predict_length(len_logits).cpu().numpy()\n",
    "\n",
    "            all_labels_len.extend(true_lengths.tolist())\n",
    "            all_preds_len.extend(pred_lengths.tolist())\n",
    "\n",
    "    # ----- Regression metrics -----\n",
    "    all_preds_reg = np.vstack(all_preds_reg)\n",
    "    all_labels_reg = np.vstack(all_labels_reg)\n",
    "\n",
    "    mse = ((all_preds_reg - all_labels_reg) ** 2).mean()\n",
    "    mae = np.abs(all_preds_reg - all_labels_reg).mean()\n",
    "\n",
    "    # ----- Length metrics -----\n",
    "\n",
    "\n",
    "    acc = accuracy_score(all_labels_len, all_preds_len)\n",
    "    f1 = f1_score(all_labels_len, all_preds_len, average=\"macro\")\n",
    "\n",
    "    print(\"\\n📊 Validation Metrics:\")\n",
    "    print(f\"  Regression → MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "    print(f\"  Length     → Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=True,\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    max_epochs=50,\n",
    "    save_model=True,\n",
    "    return_val_accuracy = True,\n",
    "    test_mode = True,\n",
    "    early_stop = False\n",
    "):\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            # make_step(add_label_normalized_candles),\n",
    "            make_step(add_candle_rocp),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "            \n",
    "        ],\n",
    "        # norm_methods={\n",
    "        #     \"main\": {\n",
    "        #         \"upper_shadow\": \"robust\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "        #         \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "        #         \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "        #     }\n",
    "        # },\n",
    "        per_window_flags=[\n",
    "            False, \n",
    "          False, \n",
    "        #   True\n",
    "                ]\n",
    "    )\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/lstm_model_multireg_multihead_{timestamp}.pt\"\n",
    "    meta_out  = f\"{model_out_dir}/lstm_meta_multireg_multihead_{timestamp}.pkl\"\n",
    "\n",
    "    # Preprocess: pad linePrices and sequences\n",
    "    if do_validation:\n",
    "        train_ds, val_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline\n",
    "        )\n",
    "    else:\n",
    "        train_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=False\n",
    "        )\n",
    "        val_ds = None\n",
    "\n",
    "    sample = train_ds[0][0]  # first sample's features\n",
    "    if isinstance(sample, dict):  # multiple feature groups\n",
    "        input_dim = sample['main'].shape[1]\n",
    "    else:  # single tensor\n",
    "        input_dim = sample.shape[1]\n",
    "\n",
    "    model = LSTMMultiRegressor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        max_len_y=max_len_y,\n",
    "        lr=lr\n",
    "    )\n",
    "    init_args = {\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"max_len_y\": max_len_y,\n",
    "    \"lr\": lr\n",
    "}\n",
    "\n",
    "    model_class_info = {\n",
    "        \"module\": model.__class__.__module__,\n",
    "        \"class\": model.__class__.__name__,\n",
    "        \"init_args\": init_args\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_batch) if val_ds else None\n",
    "    # --- Early stopping --- #\n",
    "    if early_stop == True:\n",
    "        from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",   # metric to monitor (must be logged in your LightningModule)\n",
    "            patience=10,          # number of epochs with no improvement before stopping\n",
    "            min_delta=0.001,      # minimum improvement to qualify as \"better\"\n",
    "            mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=model_out_dir,\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks=[early_stop_callback,checkpoint_callback]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        fast_dev_run=test_mode,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        callbacks= callbacks if early_stop else None\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # --- Debug / Test mode --- #\n",
    "    if test_mode:\n",
    "        save_model = False\n",
    "        from itertools import islice\n",
    "\n",
    "        # Try to grab 3rd batch; if not available, take first\n",
    "        try:\n",
    "            batch = next(islice(iter(train_loader), 2, 3))\n",
    "        except StopIteration:\n",
    "            batch = next(iter(train_loader))\n",
    "\n",
    "        X_batch_dict, y_batch, lengths = batch\n",
    "\n",
    "        print(\"🔍 Debug batch:\")\n",
    "        if isinstance(X_batch_dict, dict):\n",
    "            print(\"  Keys in X_batch:\", list(X_batch_dict.keys()))\n",
    "        print(\"  y_batch shape:\", y_batch.shape)\n",
    "        print(\"  First label in batch:\", y_batch[0])\n",
    "\n",
    "        # --- Track real column names for each feature group ---\n",
    "        feature_names_dict = {}\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            if name == \"main\":\n",
    "                # Use actual feature columns after preprocessing\n",
    "                feature_names_dict[name] = feature_cols\n",
    "            else:\n",
    "                # For extra feature groups, fallback to generic names\n",
    "                feature_names_dict[name] = [f\"{name}_{i}\" for i in range(X_batch.shape[2])]\n",
    "\n",
    "        dfs = []\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            print(f\"\\nFeature group: {name}\")\n",
    "            print(\"  X_batch shape:\", X_batch.shape)\n",
    "            print(\"  First sequence in batch (first  steps):\\n\", X_batch[0][:])\n",
    "\n",
    "            batch_size_, seq_len, feature_dim = X_batch.shape\n",
    "            df_part = pd.DataFrame(\n",
    "                X_batch.reshape(batch_size_ * seq_len, feature_dim).numpy(),\n",
    "                columns=feature_names_dict[name]\n",
    "            )\n",
    "            dfs.append(df_part)\n",
    "\n",
    "        # Combine all feature groups horizontally\n",
    "        global df_seq\n",
    "        df_seq = pd.concat(dfs, axis=1)\n",
    "        print(\"\\n✅ Combined df_seq shape:\", df_seq.shape)\n",
    "        print(\"✅ Column names in df_seq:\", df_seq.columns.tolist())\n",
    "\n",
    "        \n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        trainer.save_checkpoint(model_out)\n",
    "        joblib.dump({\n",
    "            \"input_dim\": input_dim,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"max_len_y\": max_len_y,\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"scalers\": pipeline.scalers,\n",
    "            \"pipeline_config\": pipeline.export_config(),\n",
    "            \"model_class_info\": model_class_info \n",
    "        }, meta_out)\n",
    "        print(f\"✅ Model saved to {model_out}\")\n",
    "        print(f\"✅ Meta saved to {meta_out}\")\n",
    "\n",
    "\n",
    "        \n",
    "    # --- Evaluation --- #\n",
    "    if do_validation:\n",
    "        mse, mae, acc, f1 = evaluate_model(model, val_loader)\n",
    "        if return_val_accuracy:\n",
    "            return {\"mse\": mse, \"mae\": mae, \"acc\": acc, \"f1\": f1}\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/seq_line_labels.csv\",\n",
    "        do_validation=True,\n",
    "        test_mode = False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838866e",
   "metadata": {},
   "source": [
    "### xgboost two head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439210d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.make_step import make_step\n",
    "from preprocess.multi_regression_seq_dif2 import preprocess_sequences_csv_multilines\n",
    "from add_ons.drop_column import drop_columns\n",
    "from add_ons.feature_pipeline4 import FeaturePipeline\n",
    "from add_ons.normalize_candle_seq import add_label_normalized_candles\n",
    "from add_ons.candle_dif_rate_of_change_percentage2 import add_candle_rocp\n",
    "from add_ons.candle_rate_of_change import add_candle_ratios\n",
    "# ---------------- Evaluation ---------------- #\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def evaluate_model(model, length_model, X_val, y_val, true_lengths, return_sequences=False):\n",
    "    \"\"\"\n",
    "    Evaluate multi-output regression with predicted sequence lengths.\n",
    "    Permutation-invariant: sorts both predictions and true values before computing metrics.\n",
    "    Can optionally return the predicted vs true sequences for inspection.\n",
    "    \"\"\"\n",
    "    y_pred_full = model.predict(X_val)\n",
    "    pred_lengths = np.round(length_model.predict(X_val)).astype(int)\n",
    "\n",
    "    print(\"\\n📊 Validation Report (Multi-Regression with variable-length sequences):\")\n",
    "    mse_list, mae_list, r2_list = [], [], []\n",
    "\n",
    "    pred_vs_true_list = []  # store predicted vs true sequences if needed\n",
    "\n",
    "    for i, (pred, pred_len, true_y, true_len) in enumerate(zip(y_pred_full, pred_lengths, y_val, true_lengths)):\n",
    "        L = min(pred_len, true_len)\n",
    "        pred_trunc = np.sort(pred[:L])       # sort predictions for permutation-invariant metrics\n",
    "        true_trunc = np.sort(true_y[:L])     # sort true values\n",
    "\n",
    "        mse = mean_squared_error(true_trunc, pred_trunc)\n",
    "        mae = mean_absolute_error(true_trunc, pred_trunc)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            try:\n",
    "                r2 = r2_score(true_trunc, pred_trunc)\n",
    "            except ValueError:\n",
    "                r2 = np.nan\n",
    "\n",
    "        mse_list.append(mse)\n",
    "        mae_list.append(mae)\n",
    "        r2_list.append(r2)\n",
    "\n",
    "        print(f\"\\nSample {i}:\")\n",
    "        print(f\"  Predicted length: {pred_len}, True length: {true_len}\")\n",
    "        print(f\"  MSE: {mse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "        print(f\"  Predicted lines: {pred_trunc}\")\n",
    "        print(f\"  True lines     : {true_trunc}\")\n",
    "\n",
    "        if return_sequences:\n",
    "            pred_vs_true_list.append((pred_trunc, true_trunc))\n",
    "\n",
    "    print(\"\\n--- Global Scores ---\")\n",
    "    print(f\"Mean MSE: {np.mean(mse_list):.6f}\")\n",
    "    print(f\"Mean MAE: {np.mean(mae_list):.6f}\")\n",
    "    print(f\"Mean R²: {np.nanmean(r2_list):.6f}\")\n",
    "\n",
    "    results = {\"mse\": np.mean(mse_list), \"mae\": np.mean(mae_list), \"r2\": np.nanmean(r2_list)}\n",
    "    \n",
    "    if return_sequences:\n",
    "        results[\"pred_vs_true\"] = pred_vs_true_list\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model_xgb_multireg(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=True,\n",
    "    n_estimators=1000,\n",
    "    max_depth=16,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    save_model=False,\n",
    "    return_val_metrics=True,\n",
    "    **model_params\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a multi-output XGBoost regressor with a linked sequence-length predictor.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/xgb_model_multireg_{timestamp}.pkl\"\n",
    "    length_model_out = f\"{model_out_dir}/xgb_model_seq_len_{timestamp}.pkl\"\n",
    "    meta_out = f\"{model_out_dir}/xgb_meta_multireg_{timestamp}.pkl\"\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            # make_step(add_label_normalized_candles),\n",
    "            make_step(add_candle_rocp),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "            \n",
    "        ],\n",
    "        # norm_methods={\n",
    "        #     \"main\": {\n",
    "        #         \"upper_shadow\": \"robust\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "        #         \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "        #         \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "        #     }\n",
    "        # },\n",
    "        per_window_flags=[\n",
    "            False, \n",
    "          False, \n",
    "        #   True\n",
    "                ]\n",
    "    )\n",
    "    # --- Preprocess data ---\n",
    "    if do_validation:\n",
    "        X_train, y_train, X_val, y_val, df, feature_cols, max_len_y, seq_lengths_true = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=True,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline\n",
    "        )\n",
    "    else:\n",
    "        X_train, y_train, df, feature_cols, max_len_y, seq_lengths_true = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=True,\n",
    "            feature_pipeline=pipeline\n",
    "        )\n",
    "        X_val, y_val = None, None\n",
    "\n",
    "\n",
    "    # --- Sequence length targets ---\n",
    "    if do_validation:\n",
    "        idx_train, idx_val = train_test_split(\n",
    "            np.arange(len(seq_lengths_true)),\n",
    "            test_size=0.2,  # match your preprocess split\n",
    "            random_state=42\n",
    "        )\n",
    "        train_lengths = np.array(seq_lengths_true)[idx_train]\n",
    "        val_lengths   = np.array(seq_lengths_true)[idx_val]\n",
    "    else:\n",
    "        train_lengths = np.array(seq_lengths_true)\n",
    "\n",
    "    # --- Train max-line regression ---\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        objective=\"reg:squarederror\",\n",
    "        **model_params\n",
    "    )\n",
    "    model = MultiOutputRegressor(xgb_model, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # --- Train length predictor ---\n",
    "    xgb_len_model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        objective=\"reg:squarederror\",\n",
    "        **model_params\n",
    "    )\n",
    "    xgb_len_model.fit(X_train, train_lengths)\n",
    "\n",
    "\n",
    "    # --- Save models ---\n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        \n",
    "        # Save trained models\n",
    "        joblib.dump(model, model_out)\n",
    "        joblib.dump(xgb_len_model, length_model_out)\n",
    "        \n",
    "        # Save full metadata\n",
    "        meta_dict = {\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"target_dim\": max_len_y,\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"subsample\": subsample,\n",
    "            \"colsample_bytree\": colsample_bytree,\n",
    "            \"model_params\": model_params,\n",
    "            \"scalers\": pipeline.scalers,\n",
    "            \"pipeline_config\": pipeline.export_config(),\n",
    "            \"multioutput_wrapper\": {\n",
    "                \"class\": model.__class__.__name__,\n",
    "                \"module\": model.__class__.__module__,\n",
    "            }\n",
    "        }\n",
    "        joblib.dump(meta_dict, meta_out)\n",
    "        \n",
    "        print(f\"✅ Model saved to {model_out}\")\n",
    "        print(f\"✅ Length predictor saved to {length_model_out}\")\n",
    "        print(f\"✅ Metadata saved to {meta_out}\")\n",
    "    # --- Evaluate ---\n",
    "    val_metrics = None\n",
    "    if do_validation:\n",
    "        metrics = evaluate_model(model, xgb_len_model, X_val, y_val, val_lengths, return_sequences=True)\n",
    "\n",
    "\n",
    "    if return_val_metrics:\n",
    "        return val_metrics\n",
    "\n",
    "# ---------------- Main ---------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    train_model_xgb_multireg(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/line_seq_ordered.csv\",\n",
    "        do_validation=True,\n",
    "        save_model=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e330df",
   "metadata": {},
   "source": [
    "### Hungarian lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "from preprocess.multi_regression_seq_dif3 import preprocess_sequences_csv_multilines\n",
    "# from models.LSTM.lstm_multi_line_reg_seq_dif import LSTMMultiRegressor\n",
    "from utils.print_batch import print_batch\n",
    "from utils.to_address import to_address\n",
    "from utils.json_to_csv import json_to_csv_in_memory\n",
    "from utils.padding_batch_reg import collate_batch\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from add_ons.feature_pipeline5 import FeaturePipeline\n",
    "from add_ons.drop_column import drop_columns\n",
    "from add_ons.candle_dif_rate_of_change_percentage2 import add_candle_rocp\n",
    "from add_ons.candle_proportion import add_candle_proportions\n",
    "from add_ons.candle_rate_of_change import add_candle_ratios\n",
    "from utils.make_step import make_step\n",
    "\n",
    "# ---------------- Evaluation ---------------- #\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds_reg, all_labels_reg = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in val_loader:\n",
    "            device = next(model.parameters()).device\n",
    "            X_batch = {k: v.to(device) for k, v in X_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Forward pass: regression only\n",
    "            y_pred = model(X_batch, lengths)\n",
    "\n",
    "            mask = (y_batch != 0).float()\n",
    "\n",
    "            # --- Hungarian assignment per batch ---\n",
    "            batch_preds = []\n",
    "            batch_labels = []\n",
    "            #y_batch.shape[0] is batch actually\n",
    "            for i in range(y_batch.shape[0]):\n",
    "                gt_vals = y_batch[i][mask[i] > 0]  # true targets\n",
    "                preds = y_pred[i]\n",
    "\n",
    "                if len(gt_vals) == 0:\n",
    "                    continue\n",
    "\n",
    "                cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "                row_ind, col_ind = linear_sum_assignment(cost.cpu().numpy())\n",
    "\n",
    "                matched_preds = preds[col_ind].cpu().numpy()\n",
    "                matched_labels = gt_vals[row_ind].cpu().numpy()\n",
    "\n",
    "                batch_preds.extend(matched_preds.tolist())\n",
    "                batch_labels.extend(matched_labels.tolist())\n",
    "\n",
    "            all_preds_reg.extend(batch_preds)\n",
    "            all_labels_reg.extend(batch_labels)\n",
    "\n",
    "    # Convert to arrays\n",
    "    all_preds_reg = np.array(all_preds_reg)\n",
    "    all_labels_reg = np.array(all_labels_reg)\n",
    "\n",
    "    # Regression metrics\n",
    "    mse = ((all_preds_reg - all_labels_reg) ** 2).mean()\n",
    "    mae = np.abs(all_preds_reg - all_labels_reg).mean()\n",
    "\n",
    "    print(\"\\n📊 Validation Metrics (Hungarian matched):\")\n",
    "    print(f\"  Regression → MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "\n",
    "    return {\"mse\": mse, \"mae\": mae}\n",
    "\n",
    "\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=True,\n",
    "    hidden_dim=30,\n",
    "    num_layers=1,\n",
    "    lr=0.001,\n",
    "    batch_size=50,\n",
    "    max_epochs=100,\n",
    "    save_model=True,\n",
    "    return_val_accuracy = True,\n",
    "    test_mode = True,\n",
    "    early_stop = False\n",
    "):\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            # make_step(add_label_normalized_candles),\n",
    "            make_step(add_candle_rocp),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "            \n",
    "        ],\n",
    "        # norm_methods={\n",
    "        #     \"main\": {\n",
    "        #         \"upper_shadow\": \"robust\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "        #         \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "        #         \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "        #     }\n",
    "        # },\n",
    "        per_window_flags=[\n",
    "            False, \n",
    "          False, \n",
    "        #   True\n",
    "                ]\n",
    "    )\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/lstm_model_multireg_multihead_{timestamp}.pt\"\n",
    "    meta_out  = f\"{model_out_dir}/lstm_meta_multireg_multihead_{timestamp}.pkl\"\n",
    "\n",
    "    # Preprocess: pad linePrices and sequences\n",
    "    if do_validation:\n",
    "        train_ds, val_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline,\n",
    "            preserve_order= True\n",
    "        )\n",
    "    else:\n",
    "        train_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=False,\n",
    "            preserve_order= True\n",
    "        )\n",
    "        val_ds = None\n",
    "\n",
    "    sample = train_ds[0][0]  # first sample's features\n",
    "    if isinstance(sample, dict):  # multiple feature groups\n",
    "        input_dim = sample['main'].shape[1]\n",
    "    else:  # single tensor\n",
    "        input_dim = sample.shape[1]\n",
    "\n",
    "    model = LSTMMultiRegressor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        max_len_y=max_len_y,\n",
    "        lr=lr\n",
    "    )\n",
    "    init_args = {\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"max_len_y\": max_len_y,\n",
    "    \"lr\": lr\n",
    "}\n",
    "\n",
    "    model_class_info = {\n",
    "        \"module\": model.__class__.__module__,\n",
    "        \"class\": model.__class__.__name__,\n",
    "        \"init_args\": init_args\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_batch) if val_ds else None\n",
    "    # --- Early stopping --- #\n",
    "    if early_stop == True:\n",
    "        from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",   # metric to monitor (must be logged in your LightningModule)\n",
    "            patience=10,          # number of epochs with no improvement before stopping\n",
    "            min_delta=0.001,      # minimum improvement to qualify as \"better\"\n",
    "            mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=model_out_dir,\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks=[early_stop_callback,checkpoint_callback]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        fast_dev_run=test_mode,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        callbacks= callbacks if early_stop else None\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # --- Debug / Test mode --- #\n",
    "    if test_mode:\n",
    "        save_model = False\n",
    "        from itertools import islice\n",
    "\n",
    "        # Try to grab 3rd batch; if not available, take first\n",
    "        try:\n",
    "            batch = next(islice(iter(train_loader), 2, 3))\n",
    "        except StopIteration:\n",
    "            batch = next(iter(train_loader))\n",
    "\n",
    "        X_batch_dict, y_batch, lengths = batch\n",
    "\n",
    "        print(\"🔍 Debug batch:\")\n",
    "        if isinstance(X_batch_dict, dict):\n",
    "            print(\"  Keys in X_batch:\", list(X_batch_dict.keys()))\n",
    "        print(\"  y_batch shape:\", y_batch.shape)\n",
    "        print(\"  First label in batch:\", y_batch[0])\n",
    "\n",
    "        # --- Track real column names for each feature group ---\n",
    "        feature_names_dict = {}\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            if name == \"main\":\n",
    "                # Use actual feature columns after preprocessing\n",
    "                feature_names_dict[name] = feature_cols\n",
    "            else:\n",
    "                # For extra feature groups, fallback to generic names\n",
    "                feature_names_dict[name] = [f\"{name}_{i}\" for i in range(X_batch.shape[2])]\n",
    "\n",
    "        dfs = []\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            print(f\"\\nFeature group: {name}\")\n",
    "            print(\"  X_batch shape:\", X_batch.shape)\n",
    "            print(\"  First sequence in batch (first  steps):\\n\", X_batch[0][:])\n",
    "\n",
    "            batch_size_, seq_len, feature_dim = X_batch.shape\n",
    "            df_part = pd.DataFrame(\n",
    "                X_batch.reshape(batch_size_ * seq_len, feature_dim).numpy(),\n",
    "                columns=feature_names_dict[name]\n",
    "            )\n",
    "            dfs.append(df_part)\n",
    "\n",
    "        # Combine all feature groups horizontally\n",
    "        global df_seq\n",
    "        df_seq = pd.concat(dfs, axis=1)\n",
    "        print(\"\\n✅ Combined df_seq shape:\", df_seq.shape)\n",
    "        print(\"✅ Column names in df_seq:\", df_seq.columns.tolist())\n",
    "\n",
    "        \n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        trainer.save_checkpoint(model_out)\n",
    "        joblib.dump({\n",
    "            \"input_dim\": input_dim,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"max_len_y\": max_len_y,\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"scalers\": pipeline.scalers,\n",
    "            \"pipeline_config\": pipeline.export_config(),\n",
    "            \"model_class_info\": model_class_info \n",
    "        }, meta_out)\n",
    "        print(f\"✅ Model saved to {model_out}\")\n",
    "        print(f\"✅ Meta saved to {meta_out}\")\n",
    "\n",
    "\n",
    "        \n",
    "    # --- Evaluation --- #\n",
    "    if do_validation:\n",
    "        metrics = evaluate_model(model, val_loader)\n",
    "        if return_val_accuracy:\n",
    "            return metrics\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/line_seq_ordered.csv\",\n",
    "        do_validation=True,\n",
    "        test_mode = False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8d4ec",
   "metadata": {},
   "source": [
    "### Hungarian CNN-attention lstm weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1000a9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name          | Type          | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | loss_fn_reg   | MSELoss       | 0      | train\n",
      "1 | attention     | TanhAttention | 960    | train\n",
      "2 | branches      | ModuleList    | 3.7 K  | train\n",
      "3 | fusion_conv2d | Sequential    | 1.3 K  | train\n",
      "4 | lstm          | LSTM          | 5.5 K  | train\n",
      "5 | regressor     | Sequential    | 609    | train\n",
      "--------------------------------------------------------\n",
      "12.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.1 K    Total params\n",
      "0.048     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG SAMPLE CHECK (Torch mode) ===\n",
      "\n",
      "--- Sequence 0 ---\n",
      "Label: [ 0.        -0.8797163 -1.5644624  0.         0.         0.\n",
      "  0.         0.         0.       ] Encoded (padded): [ 0.        -0.8797163 -1.5644624  0.         0.         0.\n",
      "  0.         0.         0.       ]\n",
      "[main] Shape: (5, 4)\n",
      "[main] First few rows:\n",
      " [[0.8082308  0.81475425 0.75175154 0.7888969 ]\n",
      " [0.7890243  0.912331   0.7600072  0.8652578 ]\n",
      " [0.8661357  0.9025476  0.8342968  0.8796678 ]\n",
      " [0.8796678  0.90092266 0.82062024 0.88792413]\n",
      " [0.88792527 1.0127267  0.86082923 1.        ]]\n",
      "[candle_shape] Shape: (5, 4)\n",
      "[candle_shape] First few rows:\n",
      " [[0.00807125 0.0470852  0.02392123 0.3       ]\n",
      " [0.05440368 0.03677583 0.08810496 0.7       ]\n",
      " [0.02600957 0.0367597  0.01538321 0.7       ]\n",
      " [0.01463923 0.06712486 0.00929843 0.7       ]\n",
      " [0.01272671 0.03051616 0.11207467 0.7       ]]\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b349f2a0b1a547c29c574556274daac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b56193827954f13baef783d4505df07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Debug batch:\n",
      "  Keys in X_batch: ['main', 'candle_shape']\n",
      "  y_batch shape: torch.Size([50, 9])\n",
      "  First label in batch: tensor([ 0.4956, -0.0894,  0.0000, -0.5792,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000])\n",
      "\n",
      "Feature group: main\n",
      "  X_batch shape: torch.Size([50, 32, 4])\n",
      "  First sequence in batch (first  steps):\n",
      " tensor([[1.0445, 1.0667, 0.9537, 0.9824],\n",
      "        [0.9816, 1.0644, 0.9717, 1.0553],\n",
      "        [1.0547, 1.1151, 1.0503, 1.0598],\n",
      "        [1.0592, 1.0695, 0.9651, 0.9876],\n",
      "        [0.9877, 1.0345, 0.9158, 0.9283],\n",
      "        [0.9287, 0.9873, 0.9055, 0.9586],\n",
      "        [0.9586, 0.9932, 0.8848, 0.9173],\n",
      "        [0.9173, 0.9317, 0.8775, 0.9074],\n",
      "        [0.9074, 0.9882, 0.8847, 0.9768],\n",
      "        [0.9765, 1.0285, 0.9576, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "\n",
      "Feature group: candle_shape\n",
      "  X_batch shape: torch.Size([50, 32, 4])\n",
      "  First sequence in batch (first  steps):\n",
      " tensor([[0.0212, 0.0292, 0.0594, 0.3000],\n",
      "        [0.0087, 0.0101, 0.0698, 0.7000],\n",
      "        [0.0522, 0.0042, 0.0048, 0.7000],\n",
      "        [0.0097, 0.0228, 0.0677, 0.3000],\n",
      "        [0.0474, 0.0135, 0.0602, 0.3000],\n",
      "        [0.0300, 0.0250, 0.0311, 0.7000],\n",
      "        [0.0361, 0.0353, 0.0431, 0.3000],\n",
      "        [0.0157, 0.0329, 0.0108, 0.3000],\n",
      "        [0.0117, 0.0250, 0.0711, 0.7000],\n",
      "        [0.0285, 0.0194, 0.0235, 0.7000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "\n",
      "✅ Combined df_seq shape: (1600, 8)\n",
      "✅ Column names in df_seq: ['open_prop', 'high_prop', 'low_prop', 'close_prop', 'candle_shape_0', 'candle_shape_1', 'candle_shape_2', 'candle_shape_3']\n",
      "\n",
      "📊 Validation Metrics (Hungarian matched):\n",
      "  Regression → MSE: 0.784592, MAE: 0.534842\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Current notebook location\n",
    "# notebook_path = Path().resolve()\n",
    "\n",
    "# # Add parent folder (meta/) to sys.path\n",
    "# sys.path.append(str(notebook_path.parent))\n",
    "import joblib\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "from preprocess.multi_regression_seq_dif3 import preprocess_sequences_csv_multilines\n",
    "# from models.LSTM.lstm_multi_line_reg_seq_dif import LSTMMultiRegressor\n",
    "from utils.print_batch import print_batch\n",
    "from utils.to_address import to_address\n",
    "from utils.json_to_csv import json_to_csv_in_memory\n",
    "from utils.padding_batch_reg import collate_batch\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from add_ons.feature_pipeline5 import FeaturePipeline\n",
    "from add_ons.drop_columns2 import drop_columns\n",
    "from add_ons.candle_dif_rate_of_change_percentage2 import add_candle_rocp\n",
    "from add_ons.candle_proportion import add_candle_proportions\n",
    "from add_ons.candle_rate_of_change import add_candle_ratios\n",
    "from add_ons.candle_proportion_simple import add_candle_shape_features\n",
    "from add_ons.normalize_candle_seq import add_label_normalized_candles\n",
    "from utils.make_step import make_step\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# ---------------- Evaluation ---------------- #\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds_reg, all_labels_reg = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in val_loader:\n",
    "            device = next(model.parameters()).device\n",
    "            X_batch = {k: v.to(device) for k, v in X_batch.items()}\n",
    "            y_batch = y_batch.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Forward pass: regression only\n",
    "            y_pred = model(X_batch, lengths)\n",
    "\n",
    "            mask = (y_batch != 0).float()\n",
    "\n",
    "            # --- Hungarian assignment per batch ---\n",
    "            batch_preds = []\n",
    "            batch_labels = []\n",
    "            #y_batch.shape[0] is batch actually\n",
    "            for i in range(y_batch.shape[0]):\n",
    "                gt_vals = y_batch[i][mask[i] > 0]  # true targets\n",
    "                preds = y_pred[i]\n",
    "\n",
    "                if len(gt_vals) == 0:\n",
    "                    continue\n",
    "\n",
    "                cost = torch.cdist(gt_vals.unsqueeze(1), preds.unsqueeze(1), p=2).pow(2)\n",
    "                row_ind, col_ind = linear_sum_assignment(cost.cpu().numpy())\n",
    "\n",
    "                matched_preds = preds[col_ind].cpu().numpy()\n",
    "                matched_labels = gt_vals[row_ind].cpu().numpy()\n",
    "\n",
    "                batch_preds.extend(matched_preds.tolist())\n",
    "                batch_labels.extend(matched_labels.tolist())\n",
    "\n",
    "            all_preds_reg.extend(batch_preds)\n",
    "            all_labels_reg.extend(batch_labels)\n",
    "\n",
    "    # Convert to arrays\n",
    "    all_preds_reg = np.array(all_preds_reg)\n",
    "    all_labels_reg = np.array(all_labels_reg)\n",
    "\n",
    "    # Regression metrics\n",
    "    mse = ((all_preds_reg - all_labels_reg) ** 2).mean()\n",
    "    mae = np.abs(all_preds_reg - all_labels_reg).mean()\n",
    "\n",
    "    print(\"\\n📊 Validation Metrics (Hungarian matched):\")\n",
    "    print(f\"  Regression → MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "\n",
    "    return {\"mse\": mse, \"mae\": mae}\n",
    "\n",
    "\n",
    "# ---------------- Train ---------------- #\n",
    "def train_model(\n",
    "    data_csv,\n",
    "    labels_csv,\n",
    "    model_out_dir=\"models/saved_models\",\n",
    "    do_validation=True,\n",
    "    hidden_dim=30,\n",
    "    num_layers=1,\n",
    "    lr=0.001,\n",
    "    batch_size=50,\n",
    "    max_epochs=500,\n",
    "    save_model=True,\n",
    "    return_val_accuracy = True,\n",
    "    test_mode = False,\n",
    "    early_stop = False,\n",
    "    attention_name = \"tanh_attention\",\n",
    "    optimizer_name= \"adamw\",\n",
    "    kernels = [3,5,7,11],\n",
    "    cnn_out_channels =32,\n",
    "    first_drop = 0.3,\n",
    "    second_drop = 0.3,\n",
    "    third_drop= 0.3,\n",
    "    scheduler_name = \"reduce_on_plateau\",\n",
    "    optimizer_params={\"weight_decay\": 0.01},\n",
    "    scheduler_params={\"factor\": 0.2, \"patience\": 3} \n",
    "):\n",
    "\n",
    "    pipeline = FeaturePipeline(\n",
    "        steps=[\n",
    "            make_step(add_candle_shape_features, seperatable = \"complete\", dict_name = \"candle_shape\"),\n",
    "            # make_step(add_candle_rocp),\n",
    "            make_step(add_label_normalized_candles),\n",
    "            make_step(drop_columns, cols_to_drop=[\"open\",\"high\",\"low\",\"close\",\"volume\"]),\n",
    "        ],\n",
    "        # norm_methods={\n",
    "        #     \"main\": {\n",
    "        #         \"upper_shadow\": \"robust\", \"body\": \"standard\", \"lower_shadow\": \"standard\",\n",
    "        #         \"upper_body_ratio\": \"standard\", \"lower_body_ratio\": \"standard\",\n",
    "        #         \"upper_lower_body_ratio\": \"standard\", \"Candle_Color\": \"standard\"\n",
    "        #     }\n",
    "        # },\n",
    "        per_window_flags=[\n",
    "        False, \n",
    "        True, \n",
    "        True\n",
    "                ]\n",
    "    )\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_out = f\"{model_out_dir}/lstm_model_multireg_multihead_{timestamp}.pt\"\n",
    "    meta_out  = f\"{model_out_dir}/lstm_meta_multireg_multihead_{timestamp}.pkl\"\n",
    "\n",
    "    # Preprocess: pad linePrices and sequences\n",
    "    if do_validation:\n",
    "        train_ds, val_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=True,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=True,\n",
    "            feature_pipeline=pipeline,\n",
    "            preserve_order= True\n",
    "        )\n",
    "    else:\n",
    "        train_ds, df, feature_cols, max_len_y = preprocess_sequences_csv_multilines(\n",
    "            data_csv, labels_csv,\n",
    "            val_split=False,\n",
    "            for_xgboost=False,\n",
    "            debug_sample=False,\n",
    "            preserve_order= True,\n",
    "            feature_pipeline=pipeline,\n",
    "        )\n",
    "        val_ds = None\n",
    "\n",
    "    sample = train_ds[0][0]  # first sample's features\n",
    "    if isinstance(sample, dict):\n",
    "        # build a dict of input_dims for all feature groups\n",
    "        input_dim = {k: v.shape[1] for k, v in sample.items()}\n",
    "    else:\n",
    "        # single tensor → wrap into dict with a default key\n",
    "        input_dim = {\"main\": sample.shape[1]}\n",
    "\n",
    "    model = CNNAttentionLSTMMultiRegressor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        max_len_y=max_len_y,\n",
    "        lr=lr,\n",
    "        attention_name = attention_name,\n",
    "        optimizer_name= optimizer_name,\n",
    "        kernels = kernels,\n",
    "        cnn_out_channels =cnn_out_channels,\n",
    "        first_drop = first_drop,\n",
    "        second_drop = second_drop,\n",
    "        third_drop = third_drop,\n",
    "        scheduler_name = scheduler_name,\n",
    "        optimizer_params= optimizer_params,\n",
    "        scheduler_params= scheduler_params \n",
    "    )\n",
    "    init_args = {\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"max_len_y\": max_len_y,\n",
    "    \"lr\": lr,\n",
    "    \"attention_name\" : attention_name,\n",
    "    \"optimizer_name\": optimizer_name,\n",
    "    \"kernels\" : kernels,\n",
    "    \"cnn_out_channels\" :cnn_out_channels,\n",
    "    \"first_drop\" : first_drop,\n",
    "    \"second_drop\" : second_drop,\n",
    "    \"third_drop\": third_drop,\n",
    "    \"scheduler_name\" : scheduler_name,\n",
    "    \"optimizer_params\":optimizer_params,\n",
    "    \"scheduler_params\":scheduler_params\n",
    "}\n",
    "\n",
    "    model_class_info = {\n",
    "        \"module\": model.__class__.__module__ ,\n",
    "        \"class\": model.__class__.__name__ ,\n",
    "        \"init_args\": init_args\n",
    "    }\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_batch) if val_ds else None\n",
    "    # --- Early stopping --- #\n",
    "    if early_stop == True:\n",
    "        from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",   # metric to monitor (must be logged in your LightningModule)\n",
    "            patience=10,          # number of epochs with no improvement before stopping\n",
    "            min_delta=0.001,      # minimum improvement to qualify as \"better\"\n",
    "            mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=model_out_dir,\n",
    "            filename=\"best_model\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\"\n",
    "        )\n",
    "        callbacks=[early_stop_callback,checkpoint_callback]\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        fast_dev_run=test_mode,\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        callbacks= callbacks if early_stop else None\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # --- Debug / Test mode --- #\n",
    "    if test_mode:\n",
    "        save_model = False\n",
    "        from itertools import islice\n",
    "\n",
    "        # Try to grab 3rd batch; if not available, take first\n",
    "        try:\n",
    "            batch = next(islice(iter(train_loader), 2, 3))\n",
    "        except StopIteration:\n",
    "            batch = next(iter(train_loader))\n",
    "\n",
    "        X_batch_dict, y_batch, lengths = batch\n",
    "\n",
    "        print(\"🔍 Debug batch:\")\n",
    "        if isinstance(X_batch_dict, dict):\n",
    "            print(\"  Keys in X_batch:\", list(X_batch_dict.keys()))\n",
    "        print(\"  y_batch shape:\", y_batch.shape)\n",
    "        print(\"  First label in batch:\", y_batch[0])\n",
    "\n",
    "        # --- Track real column names for each feature group ---\n",
    "        feature_names_dict = {}\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            if name == \"main\":\n",
    "                # Use actual feature columns after preprocessing\n",
    "                feature_names_dict[name] = feature_cols\n",
    "            else:\n",
    "                # For extra feature groups, fallback to generic names\n",
    "                feature_names_dict[name] = [f\"{name}_{i}\" for i in range(X_batch.shape[2])]\n",
    "\n",
    "        dfs = []\n",
    "        for name, X_batch in X_batch_dict.items():\n",
    "            print(f\"\\nFeature group: {name}\")\n",
    "            print(\"  X_batch shape:\", X_batch.shape)\n",
    "            print(\"  First sequence in batch (first  steps):\\n\", X_batch[0][:])\n",
    "\n",
    "            batch_size_, seq_len, feature_dim = X_batch.shape\n",
    "            df_part = pd.DataFrame(\n",
    "                X_batch.reshape(batch_size_ * seq_len, feature_dim).numpy(),\n",
    "                columns=feature_names_dict[name]\n",
    "            )\n",
    "            dfs.append(df_part)\n",
    "\n",
    "        # Combine all feature groups horizontally\n",
    "        global df_seq\n",
    "        df_seq = pd.concat(dfs, axis=1)\n",
    "        print(\"\\n✅ Combined df_seq shape:\", df_seq.shape)\n",
    "        print(\"✅ Column names in df_seq:\", df_seq.columns.tolist())\n",
    "\n",
    "    if save_model:\n",
    "        os.makedirs(model_out_dir, exist_ok=True)\n",
    "        trainer.save_checkpoint(model_out)\n",
    "        joblib.dump({\n",
    "            \"input_dim\": input_dim,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"max_len_y\": max_len_y,\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"scalers\": pipeline.scalers,\n",
    "            \"pipeline_config\": pipeline.export_config(),\n",
    "            \"model_class_info\": model_class_info,\n",
    "            \"target_scalers\": pipeline.target_scalers, \n",
    "        }, meta_out)\n",
    "        print(f\"✅ Model saved to {model_out}\")\n",
    "        print(f\"✅ Meta saved to {meta_out}\")\n",
    "\n",
    "        \n",
    "    # --- Evaluation --- #\n",
    "    if do_validation:\n",
    "        metrics = evaluate_model(model, val_loader)\n",
    "        if return_val_accuracy:\n",
    "            return {\"accuracy\": metrics[\"mse\"] * (-1)}\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/line_seq_ordered.csv\",\n",
    "        do_validation=True,\n",
    "        test_mode = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "399de860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "open_prop",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "high_prop",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "low_prop",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "close_prop",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "candle_shape_0",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "candle_shape_1",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "candle_shape_2",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "candle_shape_3",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "340a621d-f7ad-4ba4-9b7c-b213fc546741",
       "rows": [
        [
         "0",
         "1.0445178",
         "1.0667005",
         "0.953729",
         "0.9824383",
         "0.021237278",
         "0.02922252",
         "0.059433583",
         "0.3"
        ],
        [
         "1",
         "0.98164165",
         "1.0644296",
         "0.97173727",
         "1.055252",
         "0.008697212",
         "0.010089628",
         "0.06975612",
         "0.7"
        ],
        [
         "2",
         "1.0546947",
         "1.1151447",
         "1.0502931",
         "1.0597926",
         "0.05222931",
         "0.004173279",
         "0.004810289",
         "0.7"
        ],
        [
         "3",
         "1.0592325",
         "1.0695418",
         "0.96508294",
         "0.9875636",
         "0.0097328555",
         "0.022763854",
         "0.06766103",
         "0.3"
        ],
        [
         "4",
         "0.98769796",
         "1.0344781",
         "0.915788",
         "0.92828107",
         "0.047362685",
         "0.013458308",
         "0.060156986",
         "0.3"
        ],
        [
         "5",
         "0.9287078",
         "0.9873177",
         "0.9054929",
         "0.9585582",
         "0.030002844",
         "0.02499707",
         "0.03114093",
         "0.7"
        ],
        [
         "6",
         "0.9585582",
         "0.993181",
         "0.8848486",
         "0.91725546",
         "0.03611969",
         "0.035330307",
         "0.04308837",
         "0.3"
        ],
        [
         "7",
         "0.91725546",
         "0.9316835",
         "0.8775442",
         "0.90736717",
         "0.01572952",
         "0.03286757",
         "0.010780328",
         "0.3"
        ],
        [
         "8",
         "0.90736717",
         "0.98819953",
         "0.88465935",
         "0.9768153",
         "0.0116543975",
         "0.025026068",
         "0.07109647",
         "0.7"
        ],
        [
         "9",
         "0.97653145",
         "1.0284756",
         "0.9576083",
         "1.0",
         "0.028475624",
         "0.019377967",
         "0.023468547",
         "0.7"
        ],
        [
         "32",
         "1.0191729",
         "1.0332433",
         "0.97430694",
         "0.9810289",
         "0.013805678",
         "0.00685191",
         "0.037426412",
         "0.3"
        ],
        [
         "33",
         "0.98139787",
         "1.0362962",
         "0.97109467",
         "1.0093262",
         "0.026720779",
         "0.010498525",
         "0.027670303",
         "0.7"
        ],
        [
         "34",
         "1.0096475",
         "1.011474",
         "0.96377546",
         "0.98304117",
         "0.001809041",
         "0.019598152",
         "0.026352048",
         "0.3"
        ],
        [
         "35",
         "0.9825978",
         "1.0115165",
         "0.9668735",
         "1.0080599",
         "0.0034288964",
         "0.01600279",
         "0.025258483",
         "0.7"
        ],
        [
         "36",
         "1.0078768",
         "1.0306256",
         "0.9956647",
         "1.0243657",
         "0.006111088",
         "0.012116582",
         "0.016096678",
         "0.7"
        ],
        [
         "37",
         "1.0241028",
         "1.0254754",
         "0.9902357",
         "1.0",
         "0.001340222",
         "0.009764308",
         "0.023535542",
         "0.3"
        ],
        [
         "64",
         "1.1580337",
         "1.231967",
         "1.0719141",
         "1.191816",
         "0.033688895",
         "0.07436712",
         "0.028345253",
         "0.7"
        ],
        [
         "65",
         "1.1919379",
         "1.2152275",
         "1.1371099",
         "1.177291",
         "0.01953928",
         "0.034130227",
         "0.012288183",
         "0.3"
        ],
        [
         "66",
         "1.177291",
         "1.2143464",
         "1.0311258",
         "1.120371",
         "0.031475082",
         "0.07965684",
         "0.048348367",
         "0.3"
        ],
        [
         "67",
         "1.120371",
         "1.12374",
         "0.8933596",
         "1.039741",
         "0.0030070757",
         "0.14078645",
         "0.0719672",
         "0.3"
        ],
        [
         "68",
         "1.0399144",
         "1.0756731",
         "0.9553544",
         "1.0",
         "0.034386255",
         "0.044645656",
         "0.03838226",
         "0.3"
        ],
        [
         "96",
         "1.2318733",
         "1.2406131",
         "1.1851661",
         "1.2344172",
         "0.005019263",
         "0.03791553",
         "0.0020608727",
         "0.7"
        ],
        [
         "97",
         "1.2338412",
         "1.2391399",
         "1.2035799",
         "1.2220955",
         "0.0042945864",
         "0.015150686",
         "0.0095196245",
         "0.3"
        ],
        [
         "98",
         "1.2220945",
         "1.2328979",
         "1.1593457",
         "1.1633896",
         "0.008840082",
         "0.0034758972",
         "0.048036277",
         "0.3"
        ],
        [
         "99",
         "1.1633824",
         "1.1932929",
         "1.1431036",
         "1.1881462",
         "0.004331613",
         "0.01743088",
         "0.020842478",
         "0.7"
        ],
        [
         "100",
         "1.187026",
         "1.1910183",
         "1.1557783",
         "1.1723156",
         "0.003363353",
         "0.014106605",
         "0.012392669",
         "0.3"
        ],
        [
         "101",
         "1.1725491",
         "1.178496",
         "1.1098087",
         "1.120534",
         "0.0050717867",
         "0.009571539",
         "0.044360805",
         "0.3"
        ],
        [
         "102",
         "1.1206008",
         "1.1210246",
         "1.0213044",
         "1.0338824",
         "0.00037821205",
         "0.0121657215",
         "0.07738567",
         "0.3"
        ],
        [
         "103",
         "1.0343751",
         "1.076021",
         "1.0195453",
         "1.0589373",
         "0.01613281",
         "0.014336918",
         "0.023195222",
         "0.7"
        ],
        [
         "104",
         "1.0592295",
         "1.0838423",
         "1.0029832",
         "1.0628062",
         "0.01979279",
         "0.05310112",
         "0.0033654228",
         "0.7"
        ],
        [
         "105",
         "1.0627662",
         "1.0765496",
         "1.0287007",
         "1.0509228",
         "0.012969514",
         "0.02114522",
         "0.0111439675",
         "0.3"
        ],
        [
         "106",
         "1.0509259",
         "1.0801358",
         "1.0369303",
         "1.0602592",
         "0.018746883",
         "0.01331728",
         "0.008802943",
         "0.7"
        ],
        [
         "107",
         "1.0601965",
         "1.12437",
         "1.0553029",
         "1.1228824",
         "0.0013247168",
         "0.0046156817",
         "0.055825915",
         "0.7"
        ],
        [
         "108",
         "1.1227992",
         "1.1264232",
         "1.086308",
         "1.1069344",
         "0.0032277482",
         "0.018633876",
         "0.01412953",
         "0.3"
        ],
        [
         "109",
         "1.1069344",
         "1.1114217",
         "1.0140932",
         "1.043367",
         "0.0040537114",
         "0.028056992",
         "0.057426646",
         "0.3"
        ],
        [
         "110",
         "1.0431869",
         "1.0535953",
         "1.0042176",
         "1.0389754",
         "0.009977497",
         "0.033453863",
         "0.004037149",
         "0.3"
        ],
        [
         "111",
         "1.0389775",
         "1.0744779",
         "1.0307366",
         "1.0687737",
         "0.0053370935",
         "0.007931761",
         "0.027878974",
         "0.7"
        ],
        [
         "112",
         "1.0686308",
         "1.0718465",
         "1.017385",
         "1.0425214",
         "0.003009195",
         "0.024111068",
         "0.024432624",
         "0.3"
        ],
        [
         "113",
         "1.0425482",
         "1.0629565",
         "1.0191307",
         "1.0433793",
         "0.018763267",
         "0.022461643",
         "0.00079663284",
         "0.7"
        ],
        [
         "114",
         "1.0433793",
         "1.0908343",
         "1.0428896",
         "1.0669942",
         "0.022343272",
         "0.00046930352",
         "0.022132132",
         "0.7"
        ],
        [
         "115",
         "1.067133",
         "1.0689312",
         "1.0339553",
         "1.0477369",
         "0.0016850467",
         "0.013153593",
         "0.018175947",
         "0.3"
        ],
        [
         "116",
         "1.0478027",
         "1.0594589",
         "0.9876566",
         "1.0",
         "0.01112443",
         "0.012343381",
         "0.04562185",
         "0.3"
        ],
        [
         "128",
         "1.0416647",
         "1.0518255",
         "1.0042897",
         "1.0267121",
         "0.009754401",
         "0.021838987",
         "0.014354513",
         "0.3"
        ],
        [
         "129",
         "1.0267123",
         "1.0528046",
         "1.0212198",
         "1.0250456",
         "0.025413403",
         "0.0037322845",
         "0.001623341",
         "0.3"
        ],
        [
         "130",
         "1.0250382",
         "1.0409721",
         "0.985833",
         "1.0",
         "0.015544706",
         "0.014166986",
         "0.02442658",
         "0.3"
        ],
        [
         "160",
         "1.0321423",
         "1.0550525",
         "1.0321423",
         "1.0425798",
         "0.011963441",
         "0.0",
         "0.010011175",
         "0.7"
        ],
        [
         "161",
         "1.0424657",
         "1.0601007",
         "1.0176966",
         "1.0328035",
         "0.016916651",
         "0.014627118",
         "0.009268469",
         "0.3"
        ],
        [
         "162",
         "1.0326169",
         "1.0481427",
         "1.0208638",
         "1.044387",
         "0.0035961627",
         "0.011381746",
         "0.011269909",
         "0.7"
        ],
        [
         "163",
         "1.044387",
         "1.0474805",
         "0.98952824",
         "1.0",
         "0.0029620004",
         "0.010471775",
         "0.042500455",
         "0.3"
        ],
        [
         "192",
         "1.0838621",
         "1.0864887",
         "0.9549881",
         "0.9753272",
         "0.0024234522",
         "0.020853598",
         "0.1001371",
         "0.3"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 513
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_prop</th>\n",
       "      <th>high_prop</th>\n",
       "      <th>low_prop</th>\n",
       "      <th>close_prop</th>\n",
       "      <th>candle_shape_0</th>\n",
       "      <th>candle_shape_1</th>\n",
       "      <th>candle_shape_2</th>\n",
       "      <th>candle_shape_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.044518</td>\n",
       "      <td>1.066700</td>\n",
       "      <td>0.953729</td>\n",
       "      <td>0.982438</td>\n",
       "      <td>0.021237</td>\n",
       "      <td>0.029223</td>\n",
       "      <td>0.059434</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.981642</td>\n",
       "      <td>1.064430</td>\n",
       "      <td>0.971737</td>\n",
       "      <td>1.055252</td>\n",
       "      <td>0.008697</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.069756</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.054695</td>\n",
       "      <td>1.115145</td>\n",
       "      <td>1.050293</td>\n",
       "      <td>1.059793</td>\n",
       "      <td>0.052229</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.059232</td>\n",
       "      <td>1.069542</td>\n",
       "      <td>0.965083</td>\n",
       "      <td>0.987564</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.022764</td>\n",
       "      <td>0.067661</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.987698</td>\n",
       "      <td>1.034478</td>\n",
       "      <td>0.915788</td>\n",
       "      <td>0.928281</td>\n",
       "      <td>0.047363</td>\n",
       "      <td>0.013458</td>\n",
       "      <td>0.060157</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>0.929585</td>\n",
       "      <td>0.995679</td>\n",
       "      <td>0.921836</td>\n",
       "      <td>0.931931</td>\n",
       "      <td>0.068404</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>0.931369</td>\n",
       "      <td>1.048423</td>\n",
       "      <td>0.925005</td>\n",
       "      <td>1.030514</td>\n",
       "      <td>0.017379</td>\n",
       "      <td>0.006833</td>\n",
       "      <td>0.096209</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>1.031084</td>\n",
       "      <td>1.053510</td>\n",
       "      <td>0.957832</td>\n",
       "      <td>0.976830</td>\n",
       "      <td>0.021751</td>\n",
       "      <td>0.019449</td>\n",
       "      <td>0.052618</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>0.977235</td>\n",
       "      <td>1.012543</td>\n",
       "      <td>0.963175</td>\n",
       "      <td>1.004886</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>0.014387</td>\n",
       "      <td>0.027516</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>1.004886</td>\n",
       "      <td>1.023366</td>\n",
       "      <td>0.984806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>513 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      open_prop  high_prop  low_prop  close_prop  candle_shape_0  \\\n",
       "0      1.044518   1.066700  0.953729    0.982438        0.021237   \n",
       "1      0.981642   1.064430  0.971737    1.055252        0.008697   \n",
       "2      1.054695   1.115145  1.050293    1.059793        0.052229   \n",
       "3      1.059232   1.069542  0.965083    0.987564        0.009733   \n",
       "4      0.987698   1.034478  0.915788    0.928281        0.047363   \n",
       "...         ...        ...       ...         ...             ...   \n",
       "1573   0.929585   0.995679  0.921836    0.931931        0.068404   \n",
       "1574   0.931369   1.048423  0.925005    1.030514        0.017379   \n",
       "1575   1.031084   1.053510  0.957832    0.976830        0.021751   \n",
       "1576   0.977235   1.012543  0.963175    1.004886        0.007620   \n",
       "1577   1.004886   1.023366  0.984806    1.000000        0.018390   \n",
       "\n",
       "      candle_shape_1  candle_shape_2  candle_shape_3  \n",
       "0           0.029223        0.059434             0.3  \n",
       "1           0.010090        0.069756             0.7  \n",
       "2           0.004173        0.004810             0.7  \n",
       "3           0.022764        0.067661             0.3  \n",
       "4           0.013458        0.060157             0.3  \n",
       "...              ...             ...             ...  \n",
       "1573        0.008336        0.002518             0.7  \n",
       "1574        0.006833        0.096209             0.7  \n",
       "1575        0.019449        0.052618             0.3  \n",
       "1576        0.014387        0.027516             0.7  \n",
       "1577        0.015195        0.004862             0.3  \n",
       "\n",
       "[513 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seq = df_seq.loc[~(df_seq==0).all(axis=1)]\n",
    "df_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc099c49",
   "metadata": {},
   "source": [
    "# server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754255c5",
   "metadata": {},
   "source": [
    "## MDN server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67861978",
   "metadata": {},
   "source": [
    "### cnn lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "import glob\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from servers.pre_process.multi_reg_dif_seq2 import ServerPreprocess, import_class, build_pipeline_from_config\n",
    "# from models.LSTM.cnn_lstm_mdn import CNNLSTM_MDN  # <-- your updated \"last-output\" model\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# ---------------- Load model and meta ----------------\n",
    "meta_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_meta_multireg_*.pkl\")[0]\n",
    "state_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_model_multireg*.pt\")[0]\n",
    "\n",
    "meta = joblib.load(meta_path)\n",
    "FEATURES = meta['feature_cols']\n",
    "print(\"features\",FEATURES)\n",
    "# ---------------- Model ----------------\n",
    "# Reconstruct model class\n",
    "#for python file:\n",
    "# model_cls_info = meta[\"model_class_info\"]\n",
    "# ModelClass = import_class(model_cls_info[\"module\"], model_cls_info[\"class\"])\n",
    "model_cls_info = meta[\"model_class_info\"]\n",
    "ModelClass = cnn_lstm\n",
    "# Initialize model with original args\n",
    "model = ModelClass(**model_cls_info[\"init_args\"])\n",
    "model = cnn_lstm.load_from_checkpoint(state_path)\n",
    "model.eval()\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "df = pd.read_csv( \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "# ---------------- Setup pipeline ----------------\n",
    "pipeline = build_pipeline_from_config(meta[\"pipeline_config\"])\n",
    "pipeline.scalers = meta[\"scalers\"]\n",
    "\n",
    "# Stateful preprocessing instance\n",
    "preproc = ServerPreprocess(feature_pipeline=pipeline)\n",
    "\n",
    "\n",
    "# ---------------- Routes ----------------\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"sequential.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/get_and_add_data\")\n",
    "def get_and_add_data():\n",
    "    dense = df.set_index('timestamp').asfreq('D').ffill()\n",
    "    initial_seq_len = 21\n",
    "    next_idx = request.args.get(\"idx\", type=int)\n",
    "    if next_idx is None:\n",
    "        # First call → load initial candles\n",
    "        if len(preproc.dataset) == 0:\n",
    "            for _, row in dense.iloc[:initial_seq_len].iterrows():\n",
    "                preproc.add_candle(row)\n",
    "        candles = [\n",
    "            {'time': int(ts.timestamp()),\n",
    "             'open': float(row.open),\n",
    "             'high': float(row.high),\n",
    "             'low': float(row.low),\n",
    "             'close': float(row.close)}\n",
    "            for ts, row in dense.iloc[:initial_seq_len].iterrows()\n",
    "        ]\n",
    "        print(\"Returning initial candles:\", candles)\n",
    "\n",
    "        return jsonify({\n",
    "            \"initial_seq_len\": initial_seq_len,\n",
    "            \"next_idx\": initial_seq_len,\n",
    "            \"candles\": candles\n",
    "        })\n",
    "    else:\n",
    "        # Subsequent calls → 1 candle\n",
    "        if next_idx >= len(dense):\n",
    "            print(\"Reached end of data at index:\", next_idx)\n",
    "            return jsonify({\"error\": \"End of data\"}), 404\n",
    "\n",
    "        row = dense.iloc[next_idx]\n",
    "        candle = {\n",
    "            'time': int(row.name.timestamp()),\n",
    "            'open': float(row.open),\n",
    "            'high': float(row.high),\n",
    "            'low': float(row.low),\n",
    "            'close': float(row.close)\n",
    "        }\n",
    "\n",
    "        # ✅ Add to preproc automatically\n",
    "        preproc.add_candle(row)\n",
    "\n",
    "        return jsonify({\n",
    "            \"next_idx\": next_idx + 1,\n",
    "            \"candle\": candle\n",
    "        })\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    seq_len = data.get(\"seq_len\")\n",
    "\n",
    "    if not seq_len or not isinstance(seq_len, int):\n",
    "        return jsonify({\"error\": \"Provide 'seq_len' as an int\"}), 400\n",
    "\n",
    "    try:\n",
    "        # prepare subsequence from current state\n",
    "        seq_dict = preproc.prepare_seq(seq_len)  # returns dict of DataFrames\n",
    "    except ValueError as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "    # Convert dict of DataFrames to dict of tensors\n",
    "    dict_x = {k: torch.from_numpy(v.values.astype(np.float32)).unsqueeze(0)\n",
    "            for k, v in seq_dict.items()}\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mdn_out = model(dict_x)\n",
    "\n",
    "    pi    = mdn_out['pi'][0].cpu().numpy()\n",
    "    mu    = mdn_out['mu'][0].cpu().numpy()\n",
    "    sigma = mdn_out['sigma'][0].cpu().numpy()\n",
    "    last_close = preproc.reference_dataset.iloc[-1]['close']\n",
    "\n",
    "    return jsonify({\n",
    "        'pred_prices': (last_close * mu).tolist(),\n",
    "        'pred_sigmas': (last_close * sigma).tolist(),\n",
    "        'pi': pi.tolist()\n",
    "    })\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65996c",
   "metadata": {},
   "source": [
    "## lstm two head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from servers.pre_process.multi_reg_dif_seq2 import ServerPreprocess, import_class, build_pipeline_from_config\n",
    "# from models.LSTM.two_head_lstm import LSTMMultiRegressor  # your new model\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# ---------------- Load model and meta ----------------\n",
    "meta_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_meta_multireg_multihead_*.pkl\")[0]\n",
    "state_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_model_multireg_multihead_*.pt\")[0]\n",
    "\n",
    "\n",
    "meta = joblib.load(meta_path)\n",
    "FEATURES = meta['feature_cols']\n",
    "print(\"features\", FEATURES)\n",
    "\n",
    "# Initialize model class\n",
    "model_cls_info = meta[\"model_class_info\"]\n",
    "init_args = model_cls_info[\"init_args\"]\n",
    "model = LSTMMultiRegressor.load_from_checkpoint(state_path, **init_args)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "df = pd.read_csv(\"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "# ---------------- Setup pipeline ----------------\n",
    "pipeline = build_pipeline_from_config(meta[\"pipeline_config\"])\n",
    "pipeline.scalers = meta[\"scalers\"]\n",
    "preproc = ServerPreprocess(feature_pipeline=pipeline)\n",
    "\n",
    "# ---------------- Routes ----------------\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"two_head.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/get_and_add_data\")\n",
    "def get_and_add_data():\n",
    "    dense = df.set_index('timestamp').asfreq('D').ffill()\n",
    "    initial_seq_len = 21\n",
    "    next_idx = request.args.get(\"idx\", type=int)\n",
    "\n",
    "    if next_idx is None:\n",
    "        if len(preproc.dataset) == 0:\n",
    "            for _, row in dense.iloc[:initial_seq_len].iterrows():\n",
    "                preproc.add_candle(row)\n",
    "        candles = [{'time': int(ts.timestamp()),\n",
    "                    'open': float(row.open),\n",
    "                    'high': float(row.high),\n",
    "                    'low': float(row.low),\n",
    "                    'close': float(row.close)}\n",
    "                   for ts, row in dense.iloc[:initial_seq_len].iterrows()]\n",
    "        return jsonify({\n",
    "            \"initial_seq_len\": initial_seq_len,\n",
    "            \"next_idx\": initial_seq_len,\n",
    "            \"candles\": candles\n",
    "        })\n",
    "    else:\n",
    "        if next_idx >= len(dense):\n",
    "            return jsonify({\"error\": \"End of data\"}), 404\n",
    "        row = dense.iloc[next_idx]\n",
    "        candle = {'time': int(row.name.timestamp()),\n",
    "                  'open': float(row.open),\n",
    "                  'high': float(row.high),\n",
    "                  'low': float(row.low),\n",
    "                  'close': float(row.close)}\n",
    "        preproc.add_candle(row)\n",
    "        return jsonify({\"next_idx\": next_idx + 1, \"candle\": candle})\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    seq_len = data.get(\"seq_len\")\n",
    "\n",
    "    if not seq_len or not isinstance(seq_len, int):\n",
    "        return jsonify({\"error\": \"Provide 'seq_len' as an int\"}), 400\n",
    "\n",
    "    try:\n",
    "        seq_dict = preproc.prepare_seq(seq_len)\n",
    "    except ValueError as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "    # Convert dict of DataFrames to dict of tensors\n",
    "    dict_x = {k: torch.from_numpy(v.values.astype(np.float32)).unsqueeze(0)\n",
    "            for k, v in seq_dict.items()}\n",
    "    print(\"dict\",dict_x)\n",
    "    lengths = torch.tensor([seq_len], dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred, len_logits = model( dict_x, lengths)\n",
    "\n",
    "    last_close = preproc.reference_dataset.iloc[-1]['close']\n",
    "    pred_prices = (last_close * y_pred[0]).tolist()\n",
    "    pred_len = model.predict_length(len_logits).item()\n",
    "\n",
    "    return jsonify({\n",
    "        \"pred_prices\": pred_prices,\n",
    "        \"pred_len\": pred_len\n",
    "    })\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f38c1",
   "metadata": {},
   "source": [
    "## Hungarian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f3016f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features ['open_prop', 'high_prop', 'low_prop', 'close_prop']\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [17/Sep/2025 23:48:54] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:48:55] \"GET /get_and_add_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:48:55] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:48:59] \"GET /get_and_add_data?idx=21 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:48:59] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8057, 0.5789, 0.9783, 0.7046, 1.1104, 0.9502, 0.1393, 0.8178, 0.6108]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:18] \"GET /get_and_add_data?idx=22 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:18] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8077, 0.5808, 0.9814, 0.7017, 1.1184, 0.9495, 0.1368, 0.8219, 0.6133]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:03] \"GET /get_and_add_data?idx=23 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:03] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7915, 0.5632, 0.9664, 0.6838, 1.1046, 0.9332, 0.1326, 0.8123, 0.5991]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:04] \"GET /get_and_add_data?idx=24 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:04] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7961, 0.5658, 0.9690, 0.6901, 1.1071, 0.9385, 0.1342, 0.8149, 0.6028]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:04] \"GET /get_and_add_data?idx=25 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:04] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7955, 0.5663, 0.9682, 0.6915, 1.1037, 0.9382, 0.1351, 0.8127, 0.6021]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:04] \"GET /get_and_add_data?idx=26 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:04] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7915, 0.5631, 0.9653, 0.6863, 1.1030, 0.9340, 0.1333, 0.8112, 0.5989]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:05] \"GET /get_and_add_data?idx=27 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:05] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7893, 0.5598, 0.9633, 0.6827, 1.1022, 0.9312, 0.1320, 0.8105, 0.5973]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:05] \"GET /get_and_add_data?idx=28 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:05] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8025, 0.5716, 0.9735, 0.6988, 1.1103, 0.9452, 0.1364, 0.8178, 0.6080]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:05] \"GET /get_and_add_data?idx=29 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:05] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8135, 0.5851, 0.9841, 0.7118, 1.1187, 0.9567, 0.1397, 0.8232, 0.6179]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:06] \"GET /get_and_add_data?idx=30 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:06] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8059, 0.5787, 0.9787, 0.7016, 1.1145, 0.9475, 0.1366, 0.8189, 0.6114]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:06] \"GET /get_and_add_data?idx=31 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:06] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8202, 0.5917, 0.9908, 0.7154, 1.1288, 0.9616, 0.1393, 0.8296, 0.6246]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:07] \"GET /get_and_add_data?idx=32 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:07] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8078, 0.5809, 0.9803, 0.7056, 1.1135, 0.9507, 0.1394, 0.8198, 0.6125]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:07] \"GET /get_and_add_data?idx=33 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:07] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7986, 0.5718, 0.9733, 0.6912, 1.1114, 0.9397, 0.1341, 0.8164, 0.6057]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:07] \"GET /get_and_add_data?idx=34 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:07] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8164, 0.5863, 0.9871, 0.7124, 1.1242, 0.9594, 0.1405, 0.8285, 0.6203]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:07] \"GET /get_and_add_data?idx=35 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:07] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"GET /get_and_add_data?idx=36 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8264, 0.6002, 0.9975, 0.7266, 1.1294, 0.9718, 0.1445, 0.8317, 0.6293]])\n",
      "tensor([[0.7965, 0.5731, 0.9709, 0.6911, 1.1070, 0.9403, 0.1343, 0.8130, 0.6035]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:23] \"GET /get_and_add_data?idx=37 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:23] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"GET /get_and_add_data?idx=38 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8002, 0.5722, 0.9747, 0.6929, 1.1166, 0.9441, 0.1353, 0.8215, 0.6073]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"GET /get_and_add_data?idx=39 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7761, 0.5471, 0.9525, 0.6688, 1.0874, 0.9185, 0.1306, 0.8017, 0.5846]])\n",
      "tensor([[0.7782, 0.5496, 0.9536, 0.6707, 1.0937, 0.9209, 0.1289, 0.8040, 0.5880]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"GET /get_and_add_data?idx=40 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"GET /get_and_add_data?idx=41 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:08] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7907, 0.5605, 0.9632, 0.6863, 1.1006, 0.9340, 0.1338, 0.8107, 0.5981]])\n",
      "tensor([[0.8034, 0.5739, 0.9747, 0.7011, 1.1094, 0.9467, 0.1379, 0.8175, 0.6084]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:09] \"GET /get_and_add_data?idx=42 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:09] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:09] \"GET /get_and_add_data?idx=43 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:09] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7840, 0.5570, 0.9597, 0.6774, 1.0972, 0.9260, 0.1311, 0.8063, 0.5928]])\n",
      "tensor([[0.7993, 0.5684, 0.9715, 0.6934, 1.1105, 0.9417, 0.1352, 0.8179, 0.6055]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:09] \"GET /get_and_add_data?idx=44 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:09] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7755, 0.5480, 0.9510, 0.6699, 1.0877, 0.9182, 0.1295, 0.7998, 0.5852]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:09] \"GET /get_and_add_data?idx=45 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:09] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:10] \"GET /get_and_add_data?idx=46 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:10] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7782, 0.5496, 0.9537, 0.6719, 1.0943, 0.9218, 0.1304, 0.8054, 0.5881]])\n",
      "tensor([[0.7820, 0.5523, 0.9557, 0.6771, 1.0933, 0.9254, 0.1315, 0.8053, 0.5905]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:10] \"GET /get_and_add_data?idx=47 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:10] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7757, 0.5478, 0.9510, 0.6698, 1.0895, 0.9185, 0.1293, 0.8011, 0.5857]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [17/Sep/2025 23:49:10] \"GET /get_and_add_data?idx=48 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 23:49:10] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7993, 0.5686, 0.9710, 0.6952, 1.1091, 0.9428, 0.1363, 0.8174, 0.6055]])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Add parent folder (meta/) to sys.path\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from servers.pre_process.multi_reg_dif_seq2 import ServerPreprocess, import_class, build_pipeline_from_config\n",
    "# from models.LSTM.two_head_lstm import LSTMMultiRegressor  # your new model\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# ---------------- Load model and meta ----------------\n",
    "meta_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_meta_multireg_multihead_*.pkl\")[0]\n",
    "state_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_model_multireg_multihead_*.pt\")[0]\n",
    "\n",
    "\n",
    "meta = joblib.load(meta_path)\n",
    "FEATURES = meta['feature_cols']\n",
    "print(\"features\", FEATURES)\n",
    "\n",
    "# Initialize model class\n",
    "model_cls_info = meta[\"model_class_info\"]\n",
    "init_args = model_cls_info[\"init_args\"]\n",
    "model = CNNAttentionLSTMMultiRegressor.load_from_checkpoint(state_path, **init_args)\n",
    "model.eval()\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "df = pd.read_csv(\"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "# ---------------- Setup pipeline ----------------\n",
    "pipeline = build_pipeline_from_config(meta[\"pipeline_config\"])\n",
    "pipeline.scalers = meta[\"scalers\"]\n",
    "preproc = ServerPreprocess(feature_pipeline=pipeline)\n",
    "\n",
    "# ---------------- Routes ----------------\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"hungarian.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/get_and_add_data\")\n",
    "def get_and_add_data():\n",
    "    dense = df.set_index('timestamp').asfreq('D').ffill()\n",
    "    initial_seq_len = 21\n",
    "    next_idx = request.args.get(\"idx\", type=int)\n",
    "\n",
    "    if next_idx is None:\n",
    "        if len(preproc.dataset) == 0:\n",
    "            for _, row in dense.iloc[:initial_seq_len].iterrows():\n",
    "                preproc.add_candle(row)\n",
    "        candles = [{'time': int(ts.timestamp()),\n",
    "                    'open': float(row.open),\n",
    "                    'high': float(row.high),\n",
    "                    'low': float(row.low),\n",
    "                    'close': float(row.close)}\n",
    "                   for ts, row in dense.iloc[:initial_seq_len].iterrows()]\n",
    "        return jsonify({\n",
    "            \"initial_seq_len\": initial_seq_len,\n",
    "            \"next_idx\": initial_seq_len,\n",
    "            \"candles\": candles\n",
    "        })\n",
    "    else:\n",
    "        if next_idx >= len(dense):\n",
    "            return jsonify({\"error\": \"End of data\"}), 404\n",
    "        row = dense.iloc[next_idx]\n",
    "        candle = {'time': int(row.name.timestamp()),\n",
    "                  'open': float(row.open),\n",
    "                  'high': float(row.high),\n",
    "                  'low': float(row.low),\n",
    "                  'close': float(row.close)}\n",
    "        preproc.add_candle(row)\n",
    "        return jsonify({\"next_idx\": next_idx + 1, \"candle\": candle})\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    seq_len = data.get(\"seq_len\")\n",
    "\n",
    "    if not seq_len or not isinstance(seq_len, int):\n",
    "        return jsonify({\"error\": \"Provide 'seq_len' as an int\"}), 400\n",
    "\n",
    "    try:\n",
    "        seq_dict = preproc.prepare_seq(seq_len)\n",
    "    except ValueError as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "    # Convert dict of DataFrames to dict of tensors\n",
    "    dict_x = {k: torch.from_numpy(v.values.astype(np.float32)).unsqueeze(0)\n",
    "              for k, v in seq_dict.items()}\n",
    "    lengths = torch.tensor([seq_len], dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(dict_x, lengths)  # only regression head now\n",
    "    print(y_pred)\n",
    "    last_close = preproc.reference_dataset.iloc[-1]['close']\n",
    "    pred_prices = (last_close * y_pred[0]).tolist()\n",
    "\n",
    "    return jsonify({\n",
    "        \"pred_prices\": pred_prices\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e43ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features ['open_dif', 'high_dif', 'low_dif', 'close_dif']\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [17/Sep/2025 01:38:16] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 01:38:16] \"GET /get_and_add_data HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 01:38:17] \"GET /validation_samples HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [17/Sep/2025 01:38:19] \"POST /validation_test HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Test Sample 12 ---\n",
      "seq_len: 112, raw_ts: 1554595200000000000\n",
      "target_y: [0.7261959910392761, 0.7998110055923462, 0.6823400259017944, 0.9392110109329224, 0.6541470289230347, 0.0, 0.0, 0.0, 0.0]\n",
      "Converting reference_dataset index from int64 to datetime...\n",
      "Timestamp 2019-04-07 00:00:00 missing, reindexing...\n",
      "\n",
      "--- prepare_seq_valid debug ---\n",
      "Requested end_idx (raw): 2019-04-07 00:00:00, seq_len: 112\n",
      "Timestamp 2018-12-17 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-18 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-19 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-20 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-21 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-22 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-23 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-24 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-25 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-26 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-27 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-28 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-29 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-30 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2018-12-31 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-01 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-02 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-03 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-04 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-05 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-06 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-07 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-08 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-09 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-10 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-11 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-12 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-13 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-14 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-15 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-16 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-17 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-18 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-19 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-20 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-21 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-22 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-23 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-24 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-25 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-26 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-27 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-28 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-29 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-30 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-01-31 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-01 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-02 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-03 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-04 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-05 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-06 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-07 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-08 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-09 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-10 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-11 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-12 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-13 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-14 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-15 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-16 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-17 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-18 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-19 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-20 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-21 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-22 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-23 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-24 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-25 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-26 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-27 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-02-28 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-01 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-02 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-03 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-04 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-05 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-06 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-07 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-08 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-09 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-10 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-11 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-12 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-13 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-14 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-15 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-16 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-17 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-18 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-19 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-20 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-21 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-22 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-23 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-24 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-25 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-26 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-27 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-28 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-29 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-30 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-03-31 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-04-01 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-04-02 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-04-03 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-04-04 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-04-05 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-04-06 00:00:00 missing in dataset → adding candle\n",
      "Timestamp 2019-04-07 00:00:00 missing in dataset → adding candle\n",
      "Sliced 'main' shape: (21, 4)\n",
      "After apply_window 'main' shape: (21, 4)\n",
      "--- prepare_seq_valid end ---\n",
      "\n",
      "main: shape (21, 4), dtype float32\n",
      "lengths tensor: tensor([112])\n",
      "Passing dict_x['main'] to model: shape torch.Size([1, 21, 4])\n",
      "Model forward error: start (21) + length (1) exceeds dimension size (21).\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Current notebook location\n",
    "# notebook_path = Path().resolve()\n",
    "\n",
    "# # Add parent folder (meta/) to sys.path\n",
    "# sys.path.append(str(notebook_path.parent))\n",
    "# from pathlib import Path\n",
    "import pickle\n",
    "import glob\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from servers.pre_process.multi_reg_dif_seq2 import ServerPreprocess, import_class, build_pipeline_from_config\n",
    "# from models.LSTM.two_head_lstm import LSTMMultiRegressor  # your new model\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# ---------------- Load model and meta ----------------\n",
    "meta_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_meta_multireg_multihead_*.pkl\")[0]\n",
    "state_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/lstm_model_multireg_multihead_*.pt\")[0]\n",
    "\n",
    "\n",
    "meta = joblib.load(meta_path)\n",
    "FEATURES = meta['feature_cols']\n",
    "print(\"features\", FEATURES)\n",
    "\n",
    "# Initialize model class\n",
    "model_cls_info = meta[\"model_class_info\"]\n",
    "init_args = model_cls_info[\"init_args\"]\n",
    "model = CNNAttentionLSTMMultiRegressor.load_from_checkpoint(state_path, **init_args)\n",
    "model.eval()\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "df = pd.read_csv(\n",
    "    \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "    parse_dates=['timestamp']\n",
    ")\n",
    "# print(f\"DF loaded, shape: {df.shape}\")\n",
    "\n",
    "# ---------------- Load validation ----------------\n",
    "val_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/val_dataset_*.pkl\")[0]\n",
    "# print(f\"Loading validation dataset from: {val_path}\")\n",
    "\n",
    "with open(val_path, \"rb\") as f:\n",
    "    val_data = pickle.load(f)\n",
    "\n",
    "# print(f\"Validation data keys: {list(val_data.keys())}\")\n",
    "# print(f\"Number of validation samples: {len(val_data['y'])}\")\n",
    "\n",
    "VAL_SAMPLES = []\n",
    "for i in range(len(val_data[\"y\"])):\n",
    "    seq_len = int(val_data[\"x_lengths\"][i])\n",
    "    time_indices = val_data[\"time_indices\"][i]  # timestamps, not integer indices\n",
    "    end_ts = pd.to_datetime(time_indices[-1])   # convert last timestamp to pd.Timestamp\n",
    "    \n",
    "    # print(f\"\\nSample {i}:\")\n",
    "    # print(f\"  seq_len: {seq_len}\")\n",
    "    # print(f\"  time_indices: {time_indices}\")\n",
    "    # print(f\"  end_ts: {end_ts}\")\n",
    "    \n",
    "    # find row index in df corresponding to this timestamp\n",
    "    matching_rows = df.index[df['timestamp'] == end_ts].tolist()\n",
    "    # if not matching_rows:\n",
    "    #     print(f\"  WARNING: no matching row in df for timestamp {end_ts}, skipping sample\")\n",
    "    #     continue\n",
    "    \n",
    "    row_idx = matching_rows[0]\n",
    "    end_time = str(df.iloc[row_idx].name)  # timestamp as string\n",
    "    \n",
    "    VAL_SAMPLES.append({\n",
    "        \"idx\": i,\n",
    "        \"seq_len\": seq_len,\n",
    "        \"end_idx\": row_idx,\n",
    "        \"end_time\": end_time\n",
    "    })\n",
    "\n",
    "# print(f\"\\nTotal valid samples loaded: {len(VAL_SAMPLES)}\")\n",
    "\n",
    "\n",
    "# ---------------- Setup pipeline ----------------\n",
    "pipeline = build_pipeline_from_config(meta[\"pipeline_config\"])\n",
    "pipeline.scalers = meta[\"scalers\"]\n",
    "preproc = ServerPreprocess(feature_pipeline=pipeline)\n",
    "\n",
    "# ---------------- Routes ----------------\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"hungarian2.html\")\n",
    "\n",
    "@app.route(\"/validation_samples\")\n",
    "def validation_samples():\n",
    "    return jsonify(VAL_SAMPLES)\n",
    "\n",
    "@app.route(\"/get_and_add_data\")\n",
    "def get_and_add_data():\n",
    "    dense = df.set_index('timestamp').asfreq('D').ffill()\n",
    "    initial_seq_len = 21\n",
    "    next_idx = request.args.get(\"idx\", type=int)\n",
    "\n",
    "    if next_idx is None:\n",
    "        if len(preproc.dataset) == 0:\n",
    "            for _, row in dense.iloc[:initial_seq_len].iterrows():\n",
    "                preproc.add_candle(row)\n",
    "        candles = [{'time': int(ts.timestamp()),\n",
    "                    'open': float(row.open),\n",
    "                    'high': float(row.high),\n",
    "                    'low': float(row.low),\n",
    "                    'close': float(row.close)}\n",
    "                   for ts, row in dense.iloc[:initial_seq_len].iterrows()]\n",
    "        return jsonify({\n",
    "            \"initial_seq_len\": initial_seq_len,\n",
    "            \"next_idx\": initial_seq_len,\n",
    "            \"candles\": candles\n",
    "        })\n",
    "    else:\n",
    "        if next_idx >= len(dense):\n",
    "            return jsonify({\"error\": \"End of data\"}), 404\n",
    "        row = dense.iloc[next_idx]\n",
    "        candle = {'time': int(row.name.timestamp()),\n",
    "                  'open': float(row.open),\n",
    "                  'high': float(row.high),\n",
    "                  'low': float(row.low),\n",
    "                  'close': float(row.close)}\n",
    "        preproc.add_candle(row)\n",
    "        return jsonify({\"next_idx\": next_idx + 1, \"candle\": candle})\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    seq_len = data.get(\"seq_len\")\n",
    "\n",
    "    if not seq_len or not isinstance(seq_len, int):\n",
    "        return jsonify({\"error\": \"Provide 'seq_len' as an int\"}), 400\n",
    "\n",
    "    try:\n",
    "        seq_dict = preproc.prepare_seq(seq_len)\n",
    "    except ValueError as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "    # Convert dict of DataFrames to dict of tensors\n",
    "    dict_x = {k: torch.from_numpy(v.values.astype(np.float32)).unsqueeze(0)\n",
    "              for k, v in seq_dict.items()}\n",
    "    lengths = torch.tensor([seq_len], dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(dict_x, lengths)  # only regression head now\n",
    "    print(y_pred)\n",
    "    last_close = preproc.reference_dataset.iloc[-1]['close']\n",
    "    pred_prices = (last_close * y_pred[0]).tolist()\n",
    "\n",
    "    return jsonify({\n",
    "        \"pred_prices\": pred_prices\n",
    "    })\n",
    "\n",
    "@app.route(\"/validation_test\", methods=[\"POST\"])\n",
    "def validation_test():\n",
    "    data = request.get_json(force=True)\n",
    "    sample_idx = data.get(\"sample_idx\")\n",
    "    \n",
    "    if sample_idx is None or not (0 <= sample_idx < len(val_data[\"y\"])):\n",
    "        return jsonify({\"error\": \"Invalid sample_idx\"}), 400\n",
    "\n",
    "    seq_len = int(val_data[\"x_lengths\"][sample_idx])\n",
    "    raw_ts = int(val_data[\"time_indices\"][sample_idx][-1])  # nanoseconds\n",
    "    ts = pd.to_datetime(raw_ts)  # Timestamp\n",
    "\n",
    "    target_y = val_data[\"y\"][sample_idx][:seq_len].tolist()\n",
    "\n",
    "    print(f\"\\n--- Validation Test Sample {sample_idx} ---\")\n",
    "    print(f\"seq_len: {seq_len}, raw_ts: {raw_ts}\")\n",
    "    print(f\"target_y: {target_y}\")\n",
    "\n",
    "    # --- Ensure reference dataset has datetime index ---\n",
    "    ref = preproc.reference_dataset\n",
    "    if np.issubdtype(ref.index.dtype, np.integer):\n",
    "        print(\"Converting reference_dataset index from int64 to datetime...\")\n",
    "        ref.index = pd.to_datetime(ref.index)\n",
    "\n",
    "    # --- Make sure requested timestamp exists ---\n",
    "    if ts not in ref.index:\n",
    "        print(f\"Timestamp {ts} missing, reindexing...\")\n",
    "        freq = pd.infer_freq(ref.index[:10]) or \"D\"  # auto-detect, fallback daily\n",
    "        full_index = pd.date_range(start=ref.index.min(), end=ref.index.max(), freq=freq)\n",
    "        ref = ref.reindex(full_index, method=\"ffill\")\n",
    "        preproc.reference_dataset = ref\n",
    "\n",
    "    # --- Prepare sequence ---\n",
    "    try:\n",
    "        seq_dict = preproc.prepare_seq_valid(end_idx=ts, seq_len=seq_len)\n",
    "    except ValueError as e:\n",
    "        print(f\"prepare_seq_valid error: {e}\")\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "    # Convert to tensors\n",
    "    dict_x = {}\n",
    "    for k, v in seq_dict.items():\n",
    "        arr = v.values.astype(np.float32)\n",
    "        dict_x[k] = torch.from_numpy(arr).unsqueeze(0)  # add batch dim\n",
    "        print(f\"{k}: shape {arr.shape}, dtype {arr.dtype}\")\n",
    "\n",
    "    lengths = torch.tensor([seq_len], dtype=torch.long)\n",
    "    print(f\"lengths tensor: {lengths}\")\n",
    "\n",
    "    # Model forward\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            print(f\"Passing dict_x['main'] to model: shape {dict_x['main'].shape}\")\n",
    "            y_pred = model(dict_x, lengths)\n",
    "            print(f\"y_pred shape: {y_pred.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model forward error: {e}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "    last_close = preproc.reference_dataset.loc[ts, 'close']\n",
    "    pred_prices = (last_close * y_pred[0]).tolist()\n",
    "\n",
    "    return jsonify({\n",
    "        \"sample_idx\": sample_idx,\n",
    "        \"end_time\": int(ts.timestamp() * 1000),  # unix ms for frontend\n",
    "        \"seq_len\": seq_len,\n",
    "        \"pred_prices\": pred_prices,\n",
    "        \"target_y\": target_y\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edae95b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "startTime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "endTime",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "startIndex",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "endIndex",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "linePrice_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "linePrice_9",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "caf18009-c014-40df-bd99-518fd835a07c",
       "rows": [
        [
         "0",
         "1514764800",
         "1515110400",
         "0",
         "4",
         null,
         "0.878016",
         "0.788209",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "1514764800",
         "1515283200",
         "0",
         "6",
         null,
         "1.05529",
         "0.923251",
         "0.828937",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "1515024000",
         "1515369600",
         "3",
         "7",
         "1.143628",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "1514937600",
         "1515456000",
         "2",
         "8",
         "1.139775",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "1515110400",
         "1515542400",
         "4",
         "9",
         "1.143279",
         "0.964469",
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "1515196800",
         "1515628800",
         "5",
         "10",
         "1.290228",
         "1.126277",
         "1.086008",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "1515283200",
         "1515888000",
         "6",
         "13",
         "1.105121",
         "1.041538",
         "0.982194",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "1515369600",
         "1516060800",
         "7",
         "15",
         "1.236932",
         "1.364445",
         "1.299815",
         null,
         "1.177543",
         "1.053524",
         null,
         null,
         null
        ],
        [
         "8",
         "1515801600",
         "1516320000",
         "12",
         "18",
         "0.954276",
         "1.173294",
         "0.785035",
         "1.238004",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "1516147200",
         "1516492800",
         "16",
         "20",
         "0.996497",
         null,
         "1.16283",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "1516060800",
         "1516924800",
         "15",
         "25",
         null,
         "0.989209",
         "1.026983",
         "0.922247",
         "1.154039",
         null,
         null,
         null,
         null
        ],
        [
         "11",
         "1515974400",
         "1517443200",
         "14",
         "31",
         "1.259327",
         null,
         "1.143742",
         "1.218046",
         "1.042605",
         "1.383168",
         null,
         null,
         null
        ],
        [
         "12",
         "1516838400",
         "1517788800",
         "24",
         "35",
         null,
         "1.67662",
         "1.476347",
         "1.322714",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "1517443200",
         "1518134400",
         "31",
         "39",
         "0.866167",
         "1.044538",
         "0.790359",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "1517702400",
         "1518048000",
         "34",
         "38",
         "0.913325",
         "0.840066",
         "0.77626",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "1517875200",
         "1518566400",
         "36",
         "44",
         "0.908825",
         "0.803359",
         null,
         "0.962592",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "1518134400",
         "1518825600",
         "39",
         "47",
         "0.772655",
         null,
         "0.723089",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "1518307200",
         "1518912000",
         "41",
         "48",
         "0.82336",
         null,
         "0.776309",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "1518480000",
         "1518912000",
         "43",
         "48",
         null,
         null,
         "0.819596",
         "1.0605",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "1518480000",
         "1519171200",
         "43",
         "51",
         "1.068102",
         "0.991338",
         null,
         "0.817215",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "20",
         "1518652800",
         "1519344000",
         "45",
         "53",
         "1.106209",
         "1.015549",
         null,
         "0.965396",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "21",
         "1518912000",
         "1519689600",
         "48",
         "57",
         "1.058517",
         "0.977161",
         null,
         "0.919841",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "22",
         "1518825600",
         "1518998400",
         "47",
         "49",
         null,
         "0.929502",
         "0.989077",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "1517875200",
         "1518048000",
         "36",
         "38",
         "0.918052",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "1518134400",
         "1518566400",
         "39",
         "44",
         "0.902621",
         null,
         "0.855058",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "25",
         "1519084800",
         "1519516800",
         "50",
         "55",
         "1.168618",
         null,
         "1.060616",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "26",
         "1519516800",
         "1519948800",
         "55",
         "60",
         "0.93202",
         "0.866519",
         null,
         "0.988669",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "27",
         "1519776000",
         "1520121600",
         "58",
         "62",
         null,
         null,
         "0.898584",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         "1519862400",
         "1520121600",
         "59",
         "62",
         null,
         null,
         null,
         "0.999443",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         "1518566400",
         "1518825600",
         "44",
         "47",
         null,
         null,
         null,
         null,
         "0.90719",
         null,
         null,
         null,
         null
        ],
        [
         "30",
         "1520035200",
         "1520640000",
         "61",
         "68",
         "1.311277",
         null,
         "1.055028",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         "1520380800",
         "1520726400",
         "65",
         "69",
         null,
         "0.923406",
         null,
         "0.970552",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "32",
         "1520467200",
         "1520985600",
         "66",
         "72",
         "1.137321",
         null,
         "1.070346",
         null,
         "1.18516",
         "1.022507",
         null,
         null,
         null
        ],
        [
         "33",
         "1520640000",
         "1521244800",
         "68",
         "75",
         "1.17662",
         "1.057056",
         "1.111053",
         "1.236402",
         "0.97799",
         "0.933635",
         null,
         null,
         null
        ],
        [
         "34",
         "1521158400",
         "1521504000",
         "74",
         "78",
         "0.924926",
         "0.869038",
         null,
         "0.830086",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         "1521244800",
         "1521504000",
         "75",
         "78",
         null,
         null,
         "0.875812",
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "36",
         "1521331200",
         "1521676800",
         "76",
         "80",
         "1.022609",
         null,
         null,
         "1.048557",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "1521590400",
         "1521849600",
         "79",
         "82",
         "1.04014",
         null,
         null,
         "0.987174",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         "1521676800",
         "1522022400",
         "80",
         "84",
         null,
         "1.092904",
         null,
         "1.039106",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         "1521763200",
         "1522108800",
         "81",
         "85",
         null,
         null,
         "1.086192",
         "1.140392",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         "1521936000",
         "1522281600",
         "83",
         "87",
         null,
         null,
         "1.121892",
         "1.198509",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "41",
         "1522108800",
         "1522368000",
         "85",
         "88",
         "1.158468",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "42",
         "1522195200",
         "1522540800",
         "86",
         "90",
         null,
         null,
         "0.999198",
         "1.167526",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "43",
         "1522281600",
         "1522627200",
         "87",
         "91",
         null,
         "0.981897",
         null,
         "0.93271",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "44",
         "1522368000",
         "1522800000",
         "88",
         "93",
         null,
         null,
         null,
         null,
         "1.010566",
         "1.088278",
         null,
         null,
         null
        ],
        [
         "45",
         "1522454400",
         "1523059200",
         "89",
         "96",
         null,
         null,
         "0.98939",
         "1.074732",
         "0.93906",
         null,
         null,
         null,
         null
        ],
        [
         "46",
         "1522713600",
         "1523059200",
         "92",
         "96",
         null,
         null,
         "0.982825",
         "1.074732",
         "0.95219",
         null,
         null,
         null,
         null
        ],
        [
         "47",
         "1522281600",
         "1523404800",
         "87",
         "100",
         null,
         "0.987649",
         "1.044069",
         null,
         "0.937739",
         "1.078789",
         null,
         null,
         null
        ],
        [
         "48",
         "1522886400",
         "1523059200",
         "94",
         "96",
         null,
         null,
         "0.971884",
         null,
         "0.947813",
         null,
         null,
         null,
         null
        ],
        [
         "49",
         "1522972800",
         "1523404800",
         "95",
         "100",
         null,
         null,
         "0.991989",
         null,
         null,
         "1.024539",
         "0.948589",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 364
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>startIndex</th>\n",
       "      <th>endIndex</th>\n",
       "      <th>linePrice_1</th>\n",
       "      <th>linePrice_2</th>\n",
       "      <th>linePrice_3</th>\n",
       "      <th>linePrice_4</th>\n",
       "      <th>linePrice_5</th>\n",
       "      <th>linePrice_6</th>\n",
       "      <th>linePrice_7</th>\n",
       "      <th>linePrice_8</th>\n",
       "      <th>linePrice_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514764800</td>\n",
       "      <td>1515110400</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878016</td>\n",
       "      <td>0.788209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514764800</td>\n",
       "      <td>1515283200</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.055290</td>\n",
       "      <td>0.923251</td>\n",
       "      <td>0.828937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1515024000</td>\n",
       "      <td>1515369600</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.143628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1514937600</td>\n",
       "      <td>1515456000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.139775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1515110400</td>\n",
       "      <td>1515542400</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.143279</td>\n",
       "      <td>0.964469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>1647216000</td>\n",
       "      <td>1648339200</td>\n",
       "      <td>1533</td>\n",
       "      <td>1546</td>\n",
       "      <td>0.873783</td>\n",
       "      <td>0.889793</td>\n",
       "      <td>0.902754</td>\n",
       "      <td>0.847861</td>\n",
       "      <td>0.840999</td>\n",
       "      <td>0.814315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1629417600</td>\n",
       "      <td>1630108800</td>\n",
       "      <td>1327</td>\n",
       "      <td>1335</td>\n",
       "      <td>1.001120</td>\n",
       "      <td>1.013533</td>\n",
       "      <td>0.976295</td>\n",
       "      <td>1.031057</td>\n",
       "      <td>0.963152</td>\n",
       "      <td>0.958041</td>\n",
       "      <td>0.944168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>1612742400</td>\n",
       "      <td>1613174400</td>\n",
       "      <td>1134</td>\n",
       "      <td>1139</td>\n",
       "      <td>0.984341</td>\n",
       "      <td>1.000241</td>\n",
       "      <td>1.021441</td>\n",
       "      <td>0.949513</td>\n",
       "      <td>0.930585</td>\n",
       "      <td>1.035826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>1608940800</td>\n",
       "      <td>1609632000</td>\n",
       "      <td>1090</td>\n",
       "      <td>1098</td>\n",
       "      <td>0.795270</td>\n",
       "      <td>0.875328</td>\n",
       "      <td>0.805007</td>\n",
       "      <td>0.861264</td>\n",
       "      <td>0.783370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>1588636800</td>\n",
       "      <td>1594166400</td>\n",
       "      <td>855</td>\n",
       "      <td>919</td>\n",
       "      <td>1.048390</td>\n",
       "      <td>1.078658</td>\n",
       "      <td>0.957585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.919750</td>\n",
       "      <td>0.870564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.108926</td>\n",
       "      <td>0.99542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      startTime     endTime  startIndex  endIndex  linePrice_1  linePrice_2  \\\n",
       "0    1514764800  1515110400           0         4          NaN     0.878016   \n",
       "1    1514764800  1515283200           0         6          NaN     1.055290   \n",
       "2    1515024000  1515369600           3         7     1.143628          NaN   \n",
       "3    1514937600  1515456000           2         8     1.139775          NaN   \n",
       "4    1515110400  1515542400           4         9     1.143279     0.964469   \n",
       "..          ...         ...         ...       ...          ...          ...   \n",
       "359  1647216000  1648339200        1533      1546     0.873783     0.889793   \n",
       "360  1629417600  1630108800        1327      1335     1.001120     1.013533   \n",
       "361  1612742400  1613174400        1134      1139     0.984341     1.000241   \n",
       "362  1608940800  1609632000        1090      1098     0.795270     0.875328   \n",
       "363  1588636800  1594166400         855       919     1.048390     1.078658   \n",
       "\n",
       "     linePrice_3  linePrice_4  linePrice_5  linePrice_6  linePrice_7  \\\n",
       "0       0.788209          NaN          NaN          NaN          NaN   \n",
       "1       0.923251     0.828937          NaN          NaN          NaN   \n",
       "2            NaN          NaN          NaN          NaN          NaN   \n",
       "3            NaN          NaN          NaN          NaN          NaN   \n",
       "4            NaN          NaN          NaN          NaN          NaN   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "359     0.902754     0.847861     0.840999     0.814315          NaN   \n",
       "360     0.976295     1.031057     0.963152     0.958041     0.944168   \n",
       "361     1.021441     0.949513     0.930585     1.035826          NaN   \n",
       "362     0.805007     0.861264     0.783370          NaN          NaN   \n",
       "363     0.957585          NaN     0.919750     0.870564          NaN   \n",
       "\n",
       "     linePrice_8  linePrice_9  \n",
       "0            NaN          NaN  \n",
       "1            NaN          NaN  \n",
       "2            NaN          NaN  \n",
       "3            NaN          NaN  \n",
       "4            NaN          NaN  \n",
       "..           ...          ...  \n",
       "359          NaN          NaN  \n",
       "360          NaN          NaN  \n",
       "361          NaN          NaN  \n",
       "362          NaN          NaN  \n",
       "363     1.108926      0.99542  \n",
       "\n",
       "[364 rows x 13 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv( \"/home/iatell/projects/meta-learning/data/line_seq_ordered_added.csv\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4c8b3",
   "metadata": {},
   "source": [
    "## xgboost two head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49722e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current notebook location\n",
    "notebook_path = Path().resolve()\n",
    "sys.path.append(str(notebook_path.parent))\n",
    "\n",
    "import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "from servers.pre_process.multi_reg_dif_seq import ServerPreprocess, build_pipeline_from_config\n",
    "\n",
    "# ---------------- Flask ----------------\n",
    "app = Flask(__name__)\n",
    "\n",
    "# ---------------- Load models + meta ----------------\n",
    "meta_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/xgb_meta_multireg_*.pkl\")[0]\n",
    "model_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/xgb_model_multireg_*.pkl\")[0]\n",
    "len_model_path = glob.glob(\"/home/iatell/projects/meta-learning/play_grounds/models/saved_models/xgb_model_seq_len_*.pkl\")[0]\n",
    "\n",
    "meta = joblib.load(meta_path)\n",
    "FEATURES = meta['feature_cols']\n",
    "print(\"features\", FEATURES)\n",
    "\n",
    "# Models\n",
    "model = joblib.load(model_path)       # MultiOutputRegressor with XGBRegressor inside\n",
    "len_model = joblib.load(len_model_path)\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "df = pd.read_csv(\"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "# ---------------- Setup pipeline ----------------\n",
    "pipeline = build_pipeline_from_config(meta[\"pipeline_config\"])\n",
    "pipeline.scalers = meta[\"scalers\"]\n",
    "\n",
    "# Stateful preprocessing instance\n",
    "preproc = ServerPreprocess(feature_pipeline=pipeline)\n",
    "\n",
    "\n",
    "# ---------------- Routes ----------------\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"xgboost_seq.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/get_and_add_data\")\n",
    "def get_and_add_data():\n",
    "    dense = df.set_index('timestamp').asfreq('D').ffill()\n",
    "    initial_seq_len = 21\n",
    "    next_idx = request.args.get(\"idx\", type=int)\n",
    "\n",
    "    if next_idx is None:\n",
    "        # First call → load initial candles\n",
    "        if len(preproc.dataset) == 0:\n",
    "            for _, row in dense.iloc[:initial_seq_len].iterrows():\n",
    "                preproc.add_candle(row)\n",
    "\n",
    "        candles = [\n",
    "            {'time': int(ts.timestamp()),\n",
    "             'open': float(row.open),\n",
    "             'high': float(row.high),\n",
    "             'low': float(row.low),\n",
    "             'close': float(row.close)}\n",
    "            for ts, row in dense.iloc[:initial_seq_len].iterrows()\n",
    "        ]\n",
    "        return jsonify({\n",
    "            \"initial_seq_len\": initial_seq_len,\n",
    "            \"next_idx\": initial_seq_len,\n",
    "            \"candles\": candles\n",
    "        })\n",
    "    else:\n",
    "        # Subsequent calls → 1 candle\n",
    "        if next_idx >= len(dense):\n",
    "            return jsonify({\"error\": \"End of data\"}), 404\n",
    "\n",
    "        row = dense.iloc[next_idx]\n",
    "        candle = {\n",
    "            'time': int(row.name.timestamp()),\n",
    "            'open': float(row.open),\n",
    "            'high': float(row.high),\n",
    "            'low': float(row.low),\n",
    "            'close': float(row.close)\n",
    "        }\n",
    "\n",
    "        preproc.add_candle(row)\n",
    "\n",
    "        return jsonify({\n",
    "            \"next_idx\": next_idx + 1,\n",
    "            \"candle\": candle\n",
    "        })\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json(force=True)\n",
    "    seq_len = data.get(\"seq_len\")\n",
    "\n",
    "    if not seq_len or not isinstance(seq_len, int):\n",
    "        return jsonify({\"error\": \"Provide 'seq_len' as an int\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Use your XGBoost + preproc logic\n",
    "        X_np = preproc.prepare_xgboost_seq(seq_len, model=len_model)\n",
    "        pred_len = int(np.round(len_model.predict(X_np))[0])\n",
    "        y_pred_full = model.predict(X_np)[0]\n",
    "        pred_trunc = np.sort(y_pred_full[:pred_len])\n",
    "        last_close = preproc.reference_dataset.iloc[-1]['close']\n",
    "        pred_scaled = (last_close * pred_trunc).tolist()\n",
    "\n",
    "        return jsonify({\n",
    "            'pred_length': pred_len,\n",
    "            'pred_lines': pred_scaled\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # <-- This will print the actual exception in the console\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc07d3a",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09e37c",
   "metadata": {},
   "source": [
    "## tensorboard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06403bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching TensorBoard for: lightning_logs/version_167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gio: http://localhost:6006: Operation not supported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 14:16:59.892045: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-17 14:16:59.903731: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758106019.917511   11060 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758106019.921464   11060 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758106019.932187   11060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758106019.932267   11060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758106019.932269   11060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758106019.932271   11060 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-17 14:16:59.935875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import webbrowser\n",
    "\n",
    "logdir = \"lightning_logs\"\n",
    "\n",
    "# 1. Find all version folders\n",
    "versions = [d for d in os.listdir(logdir) if d.startswith(\"version_\")]\n",
    "if not versions:\n",
    "    raise ValueError(\"No version folders found in lightning_logs\")\n",
    "\n",
    "# 2. Sort numerically and get the latest\n",
    "versions.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "latest_version = versions[-1]\n",
    "latest_logdir = os.path.join(logdir, latest_version)\n",
    "print(f\"Launching TensorBoard for: {latest_logdir}\")\n",
    "\n",
    "# 3. Choose a port\n",
    "port = 6006\n",
    "\n",
    "# 4. Launch TensorBoard as a background process\n",
    "subprocess.Popen([\"tensorboard\", f\"--logdir={latest_logdir}\", f\"--port={port}\"])\n",
    "\n",
    "# 5. Open TensorBoard in default browser\n",
    "webbrowser.open(f\"http://localhost:{port}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264cbc4",
   "metadata": {},
   "source": [
    "## tensoarboard tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec717ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Launching TensorBoard for: /home/iatell/projects/meta-learning/tune_logs/cnn_lstm_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 01:18:23.217633: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-16 01:18:23.229407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757972903.243704  158487 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757972903.248104  158487 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757972903.259545  158487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757972903.259578  158487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757972903.259581  158487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757972903.259582  158487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-16 01:18:23.263163: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gio: http://localhost:6006: Operation not supported\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import webbrowser\n",
    "\n",
    "# Base Ray Tune log directory\n",
    "base_logdir = \"/home/iatell/projects/meta-learning/tune_logs\"\n",
    "\n",
    "# 1. Find all experiment folders\n",
    "experiments = [d for d in os.listdir(base_logdir) if os.path.isdir(os.path.join(base_logdir, d))]\n",
    "if not experiments:\n",
    "    raise ValueError(\"No experiment folders found in tune_logs\")\n",
    "\n",
    "# 2. Sort by modification time and get the latest experiment\n",
    "experiments.sort(key=lambda x: os.path.getmtime(os.path.join(base_logdir, x)))\n",
    "latest_experiment = experiments[-1]\n",
    "latest_logdir = os.path.join(base_logdir, latest_experiment)\n",
    "print(f\"🚀 Launching TensorBoard for: {latest_logdir}\")\n",
    "\n",
    "# 3. Choose a port\n",
    "port = 6006\n",
    "\n",
    "# 4. Launch TensorBoard as a background process\n",
    "subprocess.Popen([\n",
    "    \"tensorboard\",\n",
    "    f\"--logdir={latest_logdir}\",\n",
    "    f\"--port={port}\"\n",
    "])\n",
    "\n",
    "# 5. Open TensorBoard in default browser\n",
    "webbrowser.open(f\"http://localhost:{port}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609a59a",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd2907",
   "metadata": {},
   "source": [
    "## tuning cnn- attention lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c39a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-09-16 01:13:15</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:32.57        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.5/15.5 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -0.002748463086296991<br>Logical resource usage: 1.0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th>attention_name  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  cnn_out_channels</th><th style=\"text-align: right;\">  first_drop</th><th style=\"text-align: right;\">  hidden_dim</th><th>kernels      </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  max_epochs</th><th style=\"text-align: right;\">  num_layers</th><th>optimizer_name  </th><th style=\"text-align: right;\">            optimizer_params/wei\n",
       "ght_decay</th><th>scheduler_name   </th><th style=\"text-align: right;\">   scheduler_params/T_m\n",
       "ax</th><th style=\"text-align: right;\">            scheduler_params/eta\n",
       "_min</th><th style=\"text-align: right;\">          scheduler_params/fac\n",
       "tor</th><th style=\"text-align: right;\">  scheduler_params/pat\n",
       "ience</th><th style=\"text-align: right;\">  second_drop</th><th style=\"text-align: right;\">  third_drop</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00000</td><td>TERMINATED</td><td>172.18.55.78:156016</td><td>simple_attention</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">    0.465832</td><td style=\"text-align: right;\">          64</td><td>[3, 5]       </td><td style=\"text-align: right;\">0.000856737</td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">           1</td><td>adamw           </td><td style=\"text-align: right;\">0.000473473</td><td>cosine           </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">1.06994e-06</td><td style=\"text-align: right;\">0.489182 </td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">     0.457087</td><td style=\"text-align: right;\">    0.334107</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         63.7998</td><td style=\"text-align: right;\">-0.00376912</td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00001</td><td>TERMINATED</td><td>172.18.55.78:156020</td><td>simple_attention</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">    0.134259</td><td style=\"text-align: right;\">         128</td><td>[3, 5, 7, 11]</td><td style=\"text-align: right;\">0.00267099 </td><td style=\"text-align: right;\">         150</td><td style=\"text-align: right;\">           3</td><td>adamw           </td><td style=\"text-align: right;\">0.000143287</td><td>onecycle         </td><td style=\"text-align: right;\">20</td><td style=\"text-align: right;\">5.06906e-06</td><td style=\"text-align: right;\">0.159452 </td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">     0.478276</td><td style=\"text-align: right;\">    0.124169</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        107.862 </td><td style=\"text-align: right;\">-0.00171126</td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00002</td><td>TERMINATED</td><td>172.18.55.78:156017</td><td>tanh_attention  </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                16</td><td style=\"text-align: right;\">    0.279615</td><td style=\"text-align: right;\">          64</td><td>[3, 5, 7, 11]</td><td style=\"text-align: right;\">0.00188269 </td><td style=\"text-align: right;\">         150</td><td style=\"text-align: right;\">           3</td><td>adam            </td><td style=\"text-align: right;\">0.000105002</td><td>cosine           </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">1.55418e-05</td><td style=\"text-align: right;\">0.164757 </td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">     0.176765</td><td style=\"text-align: right;\">    0.132369</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         81.0614</td><td style=\"text-align: right;\">-0.00212857</td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00003</td><td>TERMINATED</td><td>172.18.55.78:156014</td><td>tanh_attention  </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">    0.201896</td><td style=\"text-align: right;\">         256</td><td>[3, 5, 7, 11]</td><td style=\"text-align: right;\">0.00475725 </td><td style=\"text-align: right;\">         150</td><td style=\"text-align: right;\">           1</td><td>adamw           </td><td style=\"text-align: right;\">6.51102e-05</td><td>reduce_on_plateau</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">3.861e-05  </td><td style=\"text-align: right;\">0.252568 </td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">     0.314782</td><td style=\"text-align: right;\">    0.451033</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        147.619 </td><td style=\"text-align: right;\">-0.00156249</td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00004</td><td>TERMINATED</td><td>172.18.55.78:156018</td><td>tanh_attention  </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">    0.272345</td><td style=\"text-align: right;\">          32</td><td>[3, 5, 7, 11]</td><td style=\"text-align: right;\">0.00292937 </td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">           2</td><td>adamw           </td><td style=\"text-align: right;\">0.00418518 </td><td>cosine           </td><td style=\"text-align: right;\">20</td><td style=\"text-align: right;\">8.51017e-06</td><td style=\"text-align: right;\">0.143769 </td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">     0.246152</td><td style=\"text-align: right;\">    0.416094</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         59.3197</td><td style=\"text-align: right;\">-0.003694  </td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00005</td><td>TERMINATED</td><td>172.18.55.78:156015</td><td>tanh_attention  </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                16</td><td style=\"text-align: right;\">    0.365547</td><td style=\"text-align: right;\">         128</td><td>[3, 5]       </td><td style=\"text-align: right;\">0.00682566 </td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">           2</td><td>adamw           </td><td style=\"text-align: right;\">4.15075e-05</td><td>reduce_on_plateau</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">2.38103e-06</td><td style=\"text-align: right;\">0.225908 </td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">     0.260768</td><td style=\"text-align: right;\">    0.109157</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         85.0899</td><td style=\"text-align: right;\">-0.0018151 </td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00006</td><td>TERMINATED</td><td>172.18.55.78:156021</td><td>tanh_attention  </td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                16</td><td style=\"text-align: right;\">    0.433743</td><td style=\"text-align: right;\">          64</td><td>[3, 5]       </td><td style=\"text-align: right;\">0.000378752</td><td style=\"text-align: right;\">         100</td><td style=\"text-align: right;\">           2</td><td>adam            </td><td style=\"text-align: right;\">1.5505e-05 </td><td>onecycle         </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">1.21986e-05</td><td style=\"text-align: right;\">0.130873 </td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">     0.255645</td><td style=\"text-align: right;\">    0.147076</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         65.8118</td><td style=\"text-align: right;\">-0.00548884</td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00007</td><td>TERMINATED</td><td>172.18.55.78:156022</td><td>tanh_attention  </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">    0.391982</td><td style=\"text-align: right;\">          64</td><td>[3, 5, 7, 11]</td><td style=\"text-align: right;\">0.00855961 </td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">           2</td><td>adamw           </td><td style=\"text-align: right;\">0.000110736</td><td>onecycle         </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">1.53114e-06</td><td style=\"text-align: right;\">0.0515268</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">     0.317599</td><td style=\"text-align: right;\">    0.479085</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         37.1472</td><td style=\"text-align: right;\">-0.00233827</td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00008</td><td>TERMINATED</td><td>172.18.55.78:156019</td><td>simple_attention</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">                64</td><td style=\"text-align: right;\">    0.353015</td><td style=\"text-align: right;\">         128</td><td>[3, 5, 7, 11]</td><td style=\"text-align: right;\">0.000588154</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">           2</td><td>adam            </td><td style=\"text-align: right;\">0.000870115</td><td>cosine           </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">1.49388e-05</td><td style=\"text-align: right;\">0.148289 </td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">     0.289063</td><td style=\"text-align: right;\">    0.239295</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         56.0168</td><td style=\"text-align: right;\">-0.00315866</td></tr>\n",
       "<tr><td>train_cnn_lstm_tune_a1399_00009</td><td>TERMINATED</td><td>172.18.55.78:156013</td><td>simple_attention</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">                32</td><td style=\"text-align: right;\">    0.1977  </td><td style=\"text-align: right;\">          32</td><td>[3, 5, 7, 11]</td><td style=\"text-align: right;\">0.000236319</td><td style=\"text-align: right;\">          50</td><td style=\"text-align: right;\">           3</td><td>adam            </td><td style=\"text-align: right;\">0.000198311</td><td>cosine           </td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">5.4739e-06 </td><td style=\"text-align: right;\">0.192097 </td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">     0.168235</td><td style=\"text-align: right;\">    0.494452</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         34.1624</td><td style=\"text-align: right;\">-0.00717554</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 💻 CPU: 0.9% | 🧠 RAM: 67.8% | 🎮 GPU: 0.0% VRAM: 1.2%\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m === DEBUG SAMPLE CHECK (Torch mode) ===\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m --- Sequence 0 ---\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m Label: [1.143628 0.       0.       0.       0.       0.       0.       0.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m  0.      ] Encoded (padded): [1.143628 0.       0.       0.       0.       0.       0.       0.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m  0.      ]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m [main] Shape: (5, 4)\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m [main] First few rows:\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m  [[ 0.01562355 -0.00180042 -0.01639293  0.0093857 ]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m  [ 0.00938704  0.12409948  0.04899828  0.12622231]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m  [ 0.12622082 -0.00192766  0.09665822  0.00645032]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m  [ 0.00645032 -0.00251821 -0.02505807 -0.05388233]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m ==========================\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m  [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 2025-09-16 01:10:49.264842: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 2025-09-16 01:10:49.331359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m E0000 00:00:1757972449.360611  156665 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m E0000 00:00:1757972449.369077  156665 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m W0000 00:00:1757972449.413408  156665 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m W0000 00:00:1757972449.413461  156665 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m W0000 00:00:1757972449.413464  156665 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m W0000 00:00:1757972449.413466  156665 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 2025-09-16 01:10:49.424676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s]                            \n",
      "Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s]                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m /home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m   | Name          | Type          | Params | Mode \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m --------------------------------------------------------\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 0 | branches      | ModuleList    | 3.7 K  | train\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 1 | fusion_conv2d | Sequential    | 15     | train\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 2 | lstm          | LSTM          | 25.1 K | train\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 3 | attention     | TanhAttention | 4.2 K  | train\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 4 | regressor     | Sequential    | 2.4 K  | train\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 5 | loss_fn_reg   | MSELoss       | 0      | train\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m --------------------------------------------------------\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 35.4 K    Trainable params\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 35.4 K    Total params\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 0.142     Total estimated model params size (MB)\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 36        Modules in train mode\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m /home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m 4 | regressor     | Sequential    | 681    | train\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m /home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m /home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  10%|█         | 1/10 [00:10<01:36,  0.09it/s, v_num=0, train_loss=0.820]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 💻 CPU: 1.5% | 🧠 RAM: 68.1% | 🎮 GPU: 0.0% VRAM: 1.2%\u001b[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m === DEBUG SAMPLE CHECK (Torch mode) ===\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m --- Sequence 0 ---\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m Label: [1.143628 0.       0.       0.       0.       0.       0.       0.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m  0.      ] Encoded (padded): [1.143628 0.       0.       0.       0.       0.       0.       0.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m  0.      ]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m [main] Shape: (5, 4)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m [main] First few rows:\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m  [[ 0.01562355 -0.00180042 -0.01639293  0.0093857 ]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m  [ 0.00645032 -0.00251821 -0.02505807 -0.05388233]\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m ==========================\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s] \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 0:  20%|██        | 2/10 [00:10<00:42,  0.19it/s, v_num=0, train_loss=0.857]\n",
      "Epoch 0:  20%|██        | 1/5 [00:10<00:42,  0.09it/s]\n",
      "Epoch 0:  20%|██        | 1/5 [00:10<00:42,  0.09it/s, v_num=0, train_loss=0.734]\n",
      "Epoch 0:  20%|██        | 2/10 [00:10<00:43,  0.19it/s, v_num=0, train_loss=0.776]\n",
      "Epoch 0: 100%|██████████| 10/10 [00:00<00:00, 14.04it/s, v_num=0, train_loss=0.507]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 10/10 [00:00<00:00, 14.75it/s, v_num=0, train_loss=0.128]\n",
      "Validation DataLoader 0:  33%|███▎      | 1/3 [00:00<00:00, 56.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 66.39it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 71.49it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 10/10 [00:00<00:00, 13.94it/s, v_num=0, train_loss=0.680, val_loss=0.658]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s, v_num=0, train_loss=0.680, val_loss=0.658]         \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 1:  10%|█         | 1/10 [00:00<00:00, 20.22it/s, v_num=0, train_loss=0.610, val_loss=0.782] \n",
      "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.534, val_loss=0.618]        \n",
      "Epoch 1:  10%|█         | 1/10 [00:00<00:00, 25.12it/s, v_num=0, train_loss=0.303, val_loss=0.743] \n",
      "Epoch 0: 100%|██████████| 3/3 [00:00<00:00,  4.05it/s, v_num=0, train_loss=0.995]\n",
      "Epoch 1:  90%|█████████ | 9/10 [00:00<00:00, 15.03it/s, v_num=0, train_loss=0.402, val_loss=0.782]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s, v_num=0, train_loss=0.00525, val_loss=0.417]         \n",
      "Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s, v_num=0, train_loss=0.00525, val_loss=0.417]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  8.56it/s, v_num=0, train_loss=0.0594, val_loss=0.383]\n",
      "Epoch 1: 100%|██████████| 5/5 [00:00<00:00,  8.55it/s, v_num=0, train_loss=0.0594, val_loss=0.383]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 2:  10%|█         | 1/10 [00:00<00:00, 18.60it/s, v_num=0, train_loss=0.366, val_loss=0.722] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "Epoch 4:  20%|██        | 2/10 [00:00<00:00, 21.18it/s, v_num=0, train_loss=0.607, val_loss=0.614] \n",
      "Epoch 3: 100%|██████████| 10/10 [00:00<00:00, 18.37it/s, v_num=0, train_loss=0.0107, val_loss=0.615]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 4:  20%|██        | 2/10 [00:00<00:00, 42.76it/s, v_num=0, train_loss=0.0439, val_loss=0.500] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "Epoch 4: 100%|██████████| 10/10 [00:00<00:00, 18.68it/s, v_num=0, train_loss=0.507, val_loss=0.614]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 6: 100%|██████████| 10/10 [00:00<00:00, 26.21it/s, v_num=0, train_loss=0.475, val_loss=0.562]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "Epoch 7:  67%|██████▋   | 2/3 [00:00<00:00,  4.75it/s, v_num=0, train_loss=0.677, val_loss=0.946]\u001b[32m [repeated 123x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "Epoch 8:  40%|████      | 4/10 [00:10<00:16,  0.37it/s, v_num=0, train_loss=0.0101, val_loss=0.0124]\u001b[32m [repeated 67x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "Epoch 3:  67%|██████▋   | 2/3 [00:00<00:00,  2.50it/s, v_num=0, train_loss=0.0506, val_loss=0.0748]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 8: 100%|██████████| 10/10 [00:00<00:00, 21.50it/s, v_num=0, train_loss=0.257, val_loss=0.469]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
      "Epoch 7: 100%|██████████| 5/5 [00:00<00:00, 11.57it/s, v_num=0, train_loss=0.0212, val_loss=0.189] \u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 64.53it/s]\u001b[A\u001b[32m [repeated 74x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 74.50it/s]\u001b[A\u001b[32m [repeated 61x across cluster]\u001b[0m\n",
      "Epoch 7: 100%|██████████| 5/5 [00:00<00:00, 10.94it/s, v_num=0, train_loss=0.0212, val_loss=0.145]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.0212, val_loss=0.145]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 9:  10%|█         | 1/10 [00:10<01:35,  0.09it/s, v_num=0, train_loss=0.374, val_loss=0.402]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 8:   0%|          | 0/10 [00:00<?, ?it/s, v_num=0, train_loss=0.000281, val_loss=0.00297]\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
      "Epoch 7:  10%|█         | 1/10 [00:00<00:00, 19.41it/s, v_num=0, train_loss=0.0198, val_loss=0.0773]\n",
      "Epoch 7: 100%|██████████| 3/3 [00:11<00:00,  0.27it/s, v_num=0, train_loss=0.608, val_loss=0.946]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "Epoch 8:  70%|███████   | 7/10 [00:00<00:00, 19.28it/s, v_num=0, train_loss=0.375, val_loss=0.469]\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "Epoch 0: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s, v_num=0, train_loss=0.727, val_loss=0.843]\n",
      "Epoch 0: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s, v_num=0, train_loss=0.727, val_loss=0.843]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "Epoch 8:  80%|████████  | 8/10 [00:00<00:00, 15.69it/s, v_num=0, train_loss=0.00764, val_loss=0.0124]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 8: 100%|██████████| 10/10 [00:00<00:00, 15.65it/s, v_num=0, train_loss=0.00171, val_loss=0.00954]\n",
      "Epoch 8: 100%|██████████| 10/10 [00:00<00:00, 15.63it/s, v_num=0, train_loss=0.00171, val_loss=0.00954]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 9:  20%|██        | 2/10 [00:00<00:00, 21.99it/s, v_num=0, train_loss=0.0057, val_loss=0.00954]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "Epoch 9:  10%|█         | 1/10 [00:00<00:00, 19.21it/s, v_num=0, train_loss=0.00835, val_loss=0.0028]    \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 13: 100%|██████████| 10/10 [00:00<00:00, 21.92it/s, v_num=0, train_loss=0.00875, val_loss=0.00388]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 86.02it/s] \u001b[A\n",
      "Epoch 14:  20%|██        | 2/10 [00:00<00:00, 33.27it/s, v_num=0, train_loss=0.0114, val_loss=0.0105]   \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "Epoch 15:  10%|█         | 1/10 [00:00<00:00, 15.52it/s, v_num=0, train_loss=0.00428, val_loss=0.00242]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [-1:59:50<00:00, -0.28it/s]   \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 23:  30%|███       | 3/10 [00:00<00:00,  8.99it/s, v_num=0, train_loss=0.0131, val_loss=0.00234] \u001b[32m [repeated 267x across cluster]\u001b[0m\n",
      "Epoch 23:  40%|████      | 4/10 [00:00<00:00, 13.98it/s, v_num=0, train_loss=0.0145, val_loss=0.00427]\u001b[32m [repeated 123x across cluster]\u001b[0m\n",
      "Epoch 19:  60%|██████    | 3/5 [00:00<00:00,  8.89it/s, v_num=0, train_loss=0.00889, val_loss=0.00394]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 24: 100%|██████████| 10/10 [00:00<00:00, 18.01it/s, v_num=0, train_loss=0.0212, val_loss=0.0151]\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 122x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 122x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 122x across cluster]\u001b[0m\n",
      "Epoch 24: 100%|██████████| 10/10 [00:00<00:00, 19.11it/s, v_num=0, train_loss=0.000478, val_loss=0.00454]\u001b[32m [repeated 46x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 87.55it/s]\u001b[A\u001b[32m [repeated 145x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 100.94it/s]\u001b[A\u001b[32m [repeated 121x across cluster]\u001b[0m\n",
      "Epoch 21: 100%|██████████| 10/10 [00:00<00:00, 15.49it/s, v_num=0, train_loss=0.0102, val_loss=0.00486]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 13:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00492, val_loss=0.0331]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 23:  10%|█         | 1/10 [00:00<00:00, 21.13it/s, v_num=0, train_loss=0.00534, val_loss=0.00427]\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Epoch 25:   0%|          | 0/10 [00:00<?, ?it/s, v_num=0, train_loss=0.000478, val_loss=0.00276]\u001b[32m [repeated 82x across cluster]\u001b[0m\n",
      "Epoch 12: 100%|██████████| 3/3 [00:01<00:00,  2.97it/s, v_num=0, train_loss=0.00506, val_loss=0.018]\u001b[32m [repeated 58x across cluster]\u001b[0m\n",
      "Epoch 23:  50%|█████     | 5/10 [00:00<00:00, 22.34it/s, v_num=0, train_loss=0.00416, val_loss=0.00442]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Epoch 15:  50%|█████     | 5/10 [00:00<00:00, 13.34it/s, v_num=0, train_loss=0.00809, val_loss=0.00729]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 6: 100%|██████████| 5/5 [00:01<00:00,  3.41it/s, v_num=0, train_loss=0.00396, val_loss=0.00632]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 6: 100%|██████████| 5/5 [00:01<00:00,  3.41it/s, v_num=0, train_loss=0.00396, val_loss=0.00632]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 22:  20%|██        | 2/10 [00:00<00:00, 26.95it/s, v_num=0, train_loss=0.00513, val_loss=0.00486]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 20:  10%|█         | 1/10 [00:00<00:00, 20.15it/s, v_num=0, train_loss=0.00596, val_loss=0.00304]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 20: 100%|██████████| 10/10 [00:00<00:00, 15.89it/s, v_num=0, train_loss=0.000144, val_loss=0.0179]\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [-1:59:50<00:00, -0.28it/s]   \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "                                                                      \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "Epoch 30:  70%|███████   | 7/10 [00:00<00:00, 24.73it/s, v_num=0, train_loss=0.00972, val_loss=0.0118]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "Epoch 17:  33%|███▎      | 1/3 [00:00<00:00,  2.04it/s, v_num=0, train_loss=0.0142, val_loss=0.440]\u001b[32m [repeated 139x across cluster]\u001b[0m\n",
      "Epoch 30:  60%|██████    | 6/10 [00:00<00:00, 12.45it/s, v_num=0, train_loss=0.00395, val_loss=0.00423]\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Epoch 28:  80%|████████  | 8/10 [00:00<00:00, 13.48it/s, v_num=0, train_loss=0.00832, val_loss=0.00575]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 30: 100%|██████████| 10/10 [00:00<00:00, 14.22it/s, v_num=0, train_loss=0.000318, val_loss=0.00257]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 65x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 65x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 65x across cluster]\u001b[0m\n",
      "Epoch 33: 100%|██████████| 10/10 [00:00<00:00, 17.31it/s, v_num=0, train_loss=0.00864, val_loss=0.0115]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  33%|███▎      | 1/3 [00:00<00:00, 58.45it/s]\u001b[A\u001b[32m [repeated 74x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 19.46it/s]\u001b[A\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Epoch 28: 100%|██████████| 10/10 [00:00<00:00, 14.07it/s, v_num=0, train_loss=0.00115, val_loss=0.00509]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 30:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00636, val_loss=0.00205]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 30:  10%|█         | 1/10 [00:00<00:00, 12.13it/s, v_num=0, train_loss=0.0135, val_loss=0.00423]  \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 18:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00334, val_loss=0.0111]\u001b[32m [repeated 38x across cluster]\u001b[0m\n",
      "Epoch 17: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s, v_num=0, train_loss=0.00334, val_loss=0.0132]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Epoch 34:  40%|████      | 4/10 [00:00<00:00, 17.25it/s, v_num=0, train_loss=0.0307, val_loss=0.0114]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Epoch 29: 100%|██████████| 5/5 [00:00<00:00,  9.49it/s, v_num=0, train_loss=0.00636, val_loss=0.00205]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 29: 100%|██████████| 5/5 [00:00<00:00,  9.48it/s, v_num=0, train_loss=0.00636, val_loss=0.00205]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 29:  20%|██        | 2/10 [00:00<00:00, 26.42it/s, v_num=0, train_loss=0.0045, val_loss=0.00509]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 26:  10%|█         | 1/10 [00:00<00:00, 18.36it/s, v_num=0, train_loss=0.00674, val_loss=0.0031] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "Epoch 34:  10%|█         | 1/10 [00:00<00:00, 16.60it/s, v_num=0, train_loss=0.0114, val_loss=0.00339]   \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 38:  70%|███████   | 7/10 [00:00<00:00, 14.81it/s, v_num=0, train_loss=0.0187, val_loss=0.0109]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "Epoch 39:  20%|██        | 2/10 [00:00<00:00, 25.63it/s, v_num=0, train_loss=0.0224, val_loss=0.00973]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 35: 100%|██████████| 10/10 [00:00<00:00, 17.07it/s, v_num=0, train_loss=0.00199, val_loss=0.0024]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "Epoch 41:  20%|██        | 2/10 [00:00<00:00, 28.51it/s, v_num=0, train_loss=0.0162, val_loss=0.0106]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 96.31it/s] \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 38:  10%|█         | 1/10 [00:10<01:36,  0.09it/s, v_num=0, train_loss=0.00319, val_loss=0.0033]\u001b[32m [repeated 130x across cluster]\u001b[0m\n",
      "Epoch 41:  70%|███████   | 7/10 [00:00<00:00, 16.57it/s, v_num=0, train_loss=0.00972, val_loss=0.00336]\u001b[32m [repeated 67x across cluster]\u001b[0m\n",
      "Epoch 37:  40%|████      | 4/10 [00:00<00:00, 14.22it/s, v_num=0, train_loss=0.00694, val_loss=0.00315]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 41: 100%|██████████| 10/10 [00:00<00:00, 17.50it/s, v_num=0, train_loss=0.00964, val_loss=0.00336]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Epoch 37: 100%|██████████| 10/10 [00:00<00:00, 17.60it/s, v_num=0, train_loss=4.38e-5, val_loss=0.00315]\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 79.29it/s]\u001b[A\u001b[32m [repeated 76x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 93.39it/s]\u001b[A\u001b[32m [repeated 63x across cluster]\u001b[0m\n",
      "Epoch 37: 100%|██████████| 10/10 [00:00<00:00, 16.39it/s, v_num=0, train_loss=4.38e-5, val_loss=0.0033]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 38:   0%|          | 0/10 [00:00<?, ?it/s, v_num=0, train_loss=4.38e-5, val_loss=0.0033]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 38:  10%|█         | 1/10 [00:00<00:00, 32.74it/s, v_num=0, train_loss=0.00185, val_loss=0.00237] \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 13:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00428, val_loss=0.0045]\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "Epoch 22: 100%|██████████| 3/3 [00:00<00:00,  3.40it/s, v_num=0, train_loss=0.00896, val_loss=0.005]\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
      "Epoch 42:  60%|██████    | 6/10 [00:00<00:00, 17.46it/s, v_num=0, train_loss=0.0185, val_loss=0.0146]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "Epoch 41: 100%|██████████| 10/10 [00:00<00:00, 16.45it/s, v_num=0, train_loss=0.00964, val_loss=0.00343]\n",
      "Epoch 41: 100%|██████████| 10/10 [00:00<00:00, 16.43it/s, v_num=0, train_loss=0.00964, val_loss=0.00343]\n",
      "Epoch 39:  20%|██        | 2/10 [00:00<00:00, 21.92it/s, v_num=0, train_loss=0.00503, val_loss=0.00347]\n",
      "Epoch 37:  40%|████      | 4/10 [00:00<00:00, 14.24it/s, v_num=0, train_loss=0.00683, val_loss=0.00315]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 42:  10%|█         | 1/10 [00:10<01:35,  0.09it/s, v_num=0, train_loss=0.00684, val_loss=0.00343] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "Epoch 41: 100%|██████████| 10/10 [00:00<00:00, 19.09it/s, v_num=0, train_loss=0.0012, val_loss=0.00242]\n",
      "Epoch 41: 100%|██████████| 10/10 [00:00<00:00, 19.05it/s, v_num=0, train_loss=0.0012, val_loss=0.00242]\n",
      "Epoch 42:  20%|██        | 2/10 [00:00<00:00, 24.36it/s, v_num=0, train_loss=0.00265, val_loss=0.00242]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "Epoch 47:  10%|█         | 1/10 [00:00<00:00, 22.34it/s, v_num=0, train_loss=0.00841, val_loss=0.00346] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 49:  10%|█         | 1/10 [00:00<00:00, 15.05it/s, v_num=0, train_loss=0.00384, val_loss=0.00348]  \n",
      "Epoch 49: 100%|██████████| 10/10 [00:00<00:00, 14.60it/s, v_num=0, train_loss=0.00333, val_loss=0.00664]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m 💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m GPU available: False, used: False\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m TPU available: False, using: 0 TPU cores\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m HPU available: False, using: 0 HPUs\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 2025-09-16 01:10:49.264840: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 2025-09-16 01:10:49.331093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m E0000 00:00:1757972449.360611  156678 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m E0000 00:00:1757972449.369077  156678 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m W0000 00:00:1757972449.413465  156678 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 2025-09-16 01:10:49.424676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m /home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m   warnings.warn(\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m   | Name          | Type          | Params | Mode \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m --------------------------------------------------------\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 0 | branches      | ModuleList    | 3.7 K  | train\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 1 | fusion_conv2d | Sequential    | 15     | train\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 2 | lstm          | LSTM          | 8.4 K  | train\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 3 | attention     | TanhAttention | 1.1 K  | train\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m 4 | regressor     | Sequential    | 2.4 K  | train\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 5 | loss_fn_reg   | MSELoss       | 0      | train\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 13.9 K    Trainable params\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 0         Non-trainable params\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 13.9 K    Total params\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 0.056     Total estimated model params size (MB)\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 36        Modules in train mode\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 0         Modules in eval mode\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m /home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 4 | regressor     | Sequential    | 681    | train\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m /home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m /home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156013)\u001b[0m   Regression → MSE: 0.007176, MAE: 0.060398\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "Epoch 46:  60%|██████    | 6/10 [-1:59:50<-1:59:54, -0.58it/s, v_num=0, train_loss=0.00463, val_loss=0.0023]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 47:  10%|█         | 1/10 [00:00<00:00, 37.42it/s, v_num=0, train_loss=7.27e-5, val_loss=0.00317] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 29:  33%|███▎      | 1/3 [00:00<00:00,  9.76it/s, v_num=0, train_loss=0.0067, val_loss=0.00298]\n",
      "Epoch 53: 100%|██████████| 10/10 [00:00<00:00, 23.74it/s, v_num=0, train_loss=0.000831, val_loss=0.00346]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 49:  50%|█████     | 5/10 [00:00<00:00, 17.78it/s, v_num=0, train_loss=0.00778, val_loss=0.00382]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 57:  20%|██        | 2/10 [00:00<00:00, 42.45it/s, v_num=0, train_loss=0.00757, val_loss=0.00349] \n",
      "Epoch 57:  80%|████████  | 8/10 [00:00<00:00, 28.58it/s, v_num=0, train_loss=0.0119, val_loss=0.00349] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 54:  50%|█████     | 5/10 [00:00<00:00, 24.54it/s, v_num=0, train_loss=0.00539, val_loss=0.00298]\u001b[32m [repeated 239x across cluster]\u001b[0m\n",
      "Epoch 54:  70%|███████   | 7/10 [00:10<00:04,  0.64it/s, v_num=0, train_loss=0.00861, val_loss=0.00298]\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "Epoch 52:  90%|█████████ | 9/10 [00:00<00:00, 19.91it/s, v_num=0, train_loss=0.00235, val_loss=0.00308]\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "Epoch 58: 100%|██████████| 10/10 [00:00<00:00, 22.58it/s, v_num=0, train_loss=0.000631, val_loss=0.00347]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 122x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 122x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 122x across cluster]\u001b[0m\n",
      "Epoch 59: 100%|██████████| 10/10 [00:00<00:00, 27.87it/s, v_num=0, train_loss=0.00254, val_loss=0.00348]\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 99.27it/s]\u001b[A\u001b[32m [repeated 130x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 111.10it/s]\u001b[A\u001b[32m [repeated 122x across cluster]\u001b[0m\n",
      "Epoch 58: 100%|██████████| 10/10 [00:00<00:00, 20.62it/s, v_num=0, train_loss=0.000631, val_loss=0.00348]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Epoch 59:   0%|          | 0/10 [00:00<?, ?it/s, v_num=0, train_loss=0.000631, val_loss=0.00348]\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "Epoch 54:  10%|█         | 1/10 [00:00<00:00, 34.17it/s, v_num=0, train_loss=0.00685, val_loss=0.00298] \u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "Epoch 60:   0%|          | 0/10 [00:00<?, ?it/s, v_num=0, train_loss=0.00254, val_loss=0.00316]\u001b[32m [repeated 83x across cluster]\u001b[0m\n",
      "Epoch 32: 100%|██████████| 3/3 [00:00<00:00,  4.19it/s, v_num=0, train_loss=0.00284, val_loss=0.00345]\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "Epoch 54:  40%|████      | 4/10 [00:00<00:00, 22.82it/s, v_num=0, train_loss=0.0106, val_loss=0.00298] \u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Epoch 28: 100%|██████████| 3/3 [00:00<00:00,  3.00it/s, v_num=0, train_loss=0.0136, val_loss=0.0154]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 18: 100%|██████████| 5/5 [00:01<00:00,  3.83it/s, v_num=0, train_loss=0.00836, val_loss=0.00308]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 50:  20%|██        | 2/10 [00:00<00:00, 22.29it/s, v_num=0, train_loss=0.0102, val_loss=0.00324]  \n",
      "Epoch 49: 100%|██████████| 10/10 [00:00<00:00, 17.12it/s, v_num=0, train_loss=0.00138, val_loss=0.00236]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156022)\u001b[0m   Regression → MSE: 0.002338, MAE: 0.036990\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 56:  10%|█         | 1/10 [00:00<00:00, 10.70it/s, v_num=0, train_loss=0.00286, val_loss=0.00295] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 57: 100%|██████████| 10/10 [00:00<00:00, 24.04it/s, v_num=0, train_loss=0.00136, val_loss=0.00292]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 73:  20%|██        | 2/10 [00:00<00:00, 48.50it/s, v_num=0, train_loss=0.0045, val_loss=0.00337]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "Epoch 74:  60%|██████    | 6/10 [00:00<00:00, 16.13it/s, v_num=0, train_loss=0.00517, val_loss=0.00343]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 68:  20%|██        | 2/10 [00:00<00:00, 21.61it/s, v_num=0, train_loss=0.0129, val_loss=0.00308] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 42:  67%|██████▋   | 2/3 [00:00<00:00,  2.41it/s, v_num=0, train_loss=0.00613, val_loss=0.00359]\u001b[32m [repeated 209x across cluster]\u001b[0m\n",
      "Epoch 73:  20%|██        | 2/10 [00:00<00:00, 28.07it/s, v_num=0, train_loss=0.00358, val_loss=0.00298]\u001b[32m [repeated 76x across cluster]\u001b[0m\n",
      "Epoch 74:  60%|██████    | 6/10 [00:00<00:00, 16.11it/s, v_num=0, train_loss=0.00332, val_loss=0.00343]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 72: 100%|██████████| 10/10 [00:00<00:00, 19.35it/s, v_num=0, train_loss=0.00379, val_loss=0.00298]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 118x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 118x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 118x across cluster]\u001b[0m\n",
      "Epoch 68: 100%|██████████| 3/3 [00:00<00:00,  5.50it/s, v_num=0, train_loss=0.0195, val_loss=0.00551] \u001b[32m [repeated 46x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 129.22it/s]\u001b[A\u001b[32m [repeated 105x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 30.15it/s]\u001b[A\u001b[32m [repeated 119x across cluster]\u001b[0m\n",
      "Epoch 44: 100%|██████████| 3/3 [00:00<00:00,  3.38it/s, v_num=0, train_loss=0.00504, val_loss=0.00173]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 25:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00483, val_loss=0.00186]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Epoch 80:  10%|█         | 1/10 [00:00<00:00, 22.03it/s, v_num=0, train_loss=0.0125, val_loss=0.00343] \u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "Epoch 69:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.0195, val_loss=0.00519]\u001b[32m [repeated 83x across cluster]\u001b[0m\n",
      "Epoch 75: 100%|██████████| 5/5 [00:00<00:00, 10.47it/s, v_num=0, train_loss=0.00318, val_loss=0.00181]\u001b[32m [repeated 55x across cluster]\u001b[0m\n",
      "Epoch 80:  70%|███████   | 7/10 [00:00<00:00, 19.25it/s, v_num=0, train_loss=0.00299, val_loss=0.00343]\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "Epoch 22: 100%|██████████| 5/5 [00:01<00:00,  3.30it/s, v_num=0, train_loss=0.00431, val_loss=0.00207]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 22: 100%|██████████| 5/5 [00:01<00:00,  3.29it/s, v_num=0, train_loss=0.00431, val_loss=0.00207]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 78:  10%|█         | 1/10 [00:00<00:00, 14.33it/s, v_num=0, train_loss=0.00337, val_loss=0.00343]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "Epoch 81:  30%|███       | 3/10 [00:00<00:00, 33.59it/s, v_num=0, train_loss=0.00468, val_loss=0.0035]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 82:  40%|████      | 4/10 [00:00<00:00, 35.31it/s, v_num=0, train_loss=0.00534, val_loss=0.00327] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 81.57it/s] \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 95.58it/s] \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 94.29it/s] \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 88:  40%|████      | 4/10 [00:00<00:00, 23.28it/s, v_num=0, train_loss=0.00436, val_loss=0.00345] \n",
      "Epoch 88: 100%|██████████| 10/10 [00:00<00:00, 21.05it/s, v_num=0, train_loss=0.00302, val_loss=0.00345]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 76:  33%|███▎      | 1/3 [00:00<00:00,  3.97it/s, v_num=0, train_loss=0.00959, val_loss=0.00513]\u001b[32m [repeated 104x across cluster]\u001b[0m\n",
      "Epoch 81:  60%|██████    | 6/10 [00:00<00:00, 16.96it/s, v_num=0, train_loss=0.00395, val_loss=0.00316]\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "Epoch 89:  90%|█████████ | 9/10 [00:00<00:00, 21.56it/s, v_num=0, train_loss=0.00415, val_loss=0.00334]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 87: 100%|██████████| 10/10 [00:00<00:00, 17.18it/s, v_num=0, train_loss=0.000565, val_loss=0.00345]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 54x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 54x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 54x across cluster]\u001b[0m\n",
      "Epoch 75: 100%|██████████| 3/3 [00:00<00:00,  4.16it/s, v_num=0, train_loss=0.00928, val_loss=0.00521]\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 109.39it/s]\u001b[A\u001b[32m [repeated 45x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 43.87it/s]\u001b[A\u001b[32m [repeated 54x across cluster]\u001b[0m\n",
      "Epoch 49: 100%|██████████| 3/3 [00:01<00:00,  2.89it/s, v_num=0, train_loss=0.00249, val_loss=0.00183]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 85:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00322, val_loss=0.00178]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 90:  10%|█         | 1/10 [00:00<00:00, 21.95it/s, v_num=0, train_loss=0.00321, val_loss=0.0034] \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 50:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.0023, val_loss=0.00182]\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
      "Epoch 49: 100%|██████████| 3/3 [00:00<00:00,  3.41it/s, v_num=0, train_loss=0.0023, val_loss=0.00182] \u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Epoch 81:  90%|█████████ | 9/10 [00:11<00:01,  0.81it/s, v_num=0, train_loss=0.00345, val_loss=0.00316]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 84: 100%|██████████| 5/5 [00:00<00:00,  9.28it/s, v_num=0, train_loss=0.00322, val_loss=0.00178]\n",
      "Epoch 84: 100%|██████████| 5/5 [00:00<00:00,  9.27it/s, v_num=0, train_loss=0.00322, val_loss=0.00178]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 86:  10%|█         | 1/10 [00:00<00:00, 38.16it/s, v_num=0, train_loss=0.00964, val_loss=0.0033]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "Epoch 92:  20%|██        | 2/10 [00:00<00:00, 33.06it/s, v_num=0, train_loss=0.0068, val_loss=0.00337]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "Epoch 49: 100%|██████████| 3/3 [00:00<00:00,  3.50it/s, v_num=0, train_loss=0.00366, val_loss=0.00306]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m   Regression → MSE: 0.003159, MAE: 0.042244\n",
      "Epoch 88:  60%|██████    | 3/5 [00:00<00:00, 12.59it/s, v_num=0, train_loss=0.00491, val_loss=0.00188]\n",
      "Epoch 93:  40%|████      | 4/10 [00:00<00:00, 21.08it/s, v_num=0, train_loss=0.00761, val_loss=0.00337]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156019)\u001b[0m `Trainer.fit` stopped: `max_epochs=50` reached.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 96:  50%|█████     | 5/10 [00:00<00:00, 28.73it/s, v_num=0, train_loss=0.00991, val_loss=0.00339]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 98:  10%|█         | 1/10 [00:00<00:00, 38.46it/s, v_num=0, train_loss=0.00582, val_loss=0.00372]  \n",
      "Epoch 89:  20%|██        | 2/10 [00:00<00:00, 42.07it/s, v_num=0, train_loss=0.0051, val_loss=0.00322]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 88:  90%|█████████ | 9/10 [00:00<00:00, 27.86it/s, v_num=0, train_loss=0.00724, val_loss=0.00387]\n",
      "Epoch 97: 100%|██████████| 10/10 [00:00<00:00, 29.13it/s, v_num=0, train_loss=0.000311, val_loss=0.00345]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 57:  33%|███▎      | 1/3 [00:00<00:00,  8.42it/s, v_num=0, train_loss=0.00321, val_loss=0.00185]\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
      "Epoch 91:  70%|███████   | 7/10 [00:00<00:00, 24.39it/s, v_num=0, train_loss=0.00583, val_loss=0.00298]\u001b[32m [repeated 35x across cluster]\u001b[0m\n",
      "Epoch 88:  60%|██████    | 3/5 [00:00<00:00,  9.72it/s, v_num=0, train_loss=0.00355, val_loss=0.00188]\n",
      "Epoch 91: 100%|██████████| 10/10 [00:10<00:00,  0.91it/s, v_num=0, train_loss=0.00463, val_loss=0.00298]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 63x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 63x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 63x across cluster]\u001b[0m\n",
      "Epoch 86: 100%|██████████| 3/3 [00:00<00:00,  7.80it/s, v_num=0, train_loss=0.0068, val_loss=0.0055]\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 71.96it/s]\u001b[A\u001b[32m [repeated 55x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 48.41it/s]\u001b[A\u001b[32m [repeated 60x across cluster]\u001b[0m\n",
      "Epoch 94: 100%|██████████| 5/5 [00:00<00:00, 12.31it/s, v_num=0, train_loss=0.00857, val_loss=0.00185]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 95:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00857, val_loss=0.00185]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 90:  10%|█         | 1/10 [00:00<00:00, 18.97it/s, v_num=0, train_loss=0.00605, val_loss=0.00328]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 57:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00162, val_loss=0.00185]\u001b[32m [repeated 42x across cluster]\u001b[0m\n",
      "Epoch 56: 100%|██████████| 3/3 [00:00<00:00,  4.56it/s, v_num=0, train_loss=0.00162, val_loss=0.00185]\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "Epoch 99:  60%|██████    | 6/10 [00:00<00:00, 26.88it/s, v_num=0, train_loss=0.00497, val_loss=0.00334]\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "Epoch 91:  20%|██        | 2/10 [00:00<00:00, 38.93it/s, v_num=0, train_loss=0.00837, val_loss=0.00298] \n",
      "Epoch 99: 100%|██████████| 10/10 [00:00<00:00, 24.46it/s, v_num=0, train_loss=0.00204, val_loss=0.00346]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156018)\u001b[0m   Regression → MSE: 0.003694, MAE: 0.047637\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [-1:59:50<00:00, -0.19it/s]   \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "Epoch 57: 100%|██████████| 3/3 [00:00<00:00,  5.08it/s, v_num=0, train_loss=0.00835, val_loss=0.00159]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 2/3 [00:00<00:00, 44.02it/s] \u001b[A\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 58:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00835, val_loss=0.00159]        \n",
      "Epoch 93:  70%|███████   | 7/10 [00:00<00:00, 25.48it/s, v_num=0, train_loss=0.0101, val_loss=0.00331] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 94:  30%|███       | 3/10 [00:00<00:00, 25.13it/s, v_num=0, train_loss=0.00556, val_loss=0.00355]\n",
      "Epoch 94:  30%|███       | 3/10 [00:00<00:00, 25.08it/s, v_num=0, train_loss=0.00389, val_loss=0.00355]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 100:  20%|██        | 1/5 [00:00<00:00, 22.94it/s, v_num=0, train_loss=0.00353, val_loss=0.00178]\n",
      "Epoch 95:  20%|██        | 2/10 [00:00<00:00, 27.29it/s, v_num=0, train_loss=0.00785, val_loss=0.00354]  \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 96:  60%|██████    | 6/10 [00:00<00:00, 18.45it/s, v_num=0, train_loss=0.00702, val_loss=0.0034]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "Epoch 99:  50%|█████     | 5/10 [00:00<00:00, 30.22it/s, v_num=0, train_loss=0.00307, val_loss=0.003]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "                                                                       \u001b[A\n",
      "Epoch 105:  20%|██        | 1/5 [00:00<00:00, 27.63it/s, v_num=0, train_loss=0.00353, val_loss=0.00178]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "Epoch 99: 100%|██████████| 10/10 [00:00<00:00, 25.48it/s, v_num=0, train_loss=0.00376, val_loss=0.00375]\n",
      "Epoch 99: 100%|██████████| 10/10 [00:00<00:00, 25.05it/s, v_num=0, train_loss=0.00376, val_loss=0.00375]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m   Regression → MSE: 0.003769, MAE: 0.047488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156016)\u001b[0m `Trainer.fit` stopped: `max_epochs=100` reached.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 10/10 [00:00<00:00, 24.87it/s, v_num=0, train_loss=0.00749, val_loss=0.00319]\n",
      "Epoch 36:  80%|████████  | 4/5 [00:00<00:00,  5.05it/s, v_num=0, train_loss=0.00487, val_loss=0.00161]\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
      "Epoch 108:  80%|████████  | 4/5 [00:10<00:02,  0.37it/s, v_num=0, train_loss=0.00389, val_loss=0.00173]\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "Epoch 99: 100%|██████████| 10/10 [00:00<00:00, 27.55it/s, v_num=0, train_loss=0.00376, val_loss=0.003]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 50x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 50x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 50x across cluster]\u001b[0m\n",
      "Epoch 97: 100%|██████████| 3/3 [00:00<00:00,  8.70it/s, v_num=0, train_loss=0.0143, val_loss=0.00536] \u001b[32m [repeated 26x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 68.42it/s]\u001b[A\u001b[32m [repeated 33x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 56.52it/s]\u001b[A\u001b[32m [repeated 52x across cluster]\u001b[0m\n",
      "Epoch 96: 100%|██████████| 3/3 [00:00<00:00,  7.12it/s, v_num=0, train_loss=0.00681, val_loss=0.00536]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 97:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00681, val_loss=0.00536]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 108:  20%|██        | 1/5 [00:00<00:00, 18.83it/s, v_num=0, train_loss=0.00346, val_loss=0.00173]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 65:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00316, val_loss=0.00184]\u001b[32m [repeated 37x across cluster]\u001b[0m\n",
      "Epoch 64: 100%|██████████| 3/3 [00:00<00:00,  5.05it/s, v_num=0, train_loss=0.00316, val_loss=0.00188]\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "Epoch 98:  30%|███       | 3/10 [00:00<00:00, 34.69it/s, v_num=0, train_loss=0.00586, val_loss=0.00349]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 99: 100%|██████████| 10/10 [00:00<00:00, 25.52it/s, v_num=0, train_loss=0.00376, val_loss=0.00375]\n",
      "Epoch 109:  20%|██        | 1/5 [-1:59:50<-1:59:18, -0.10it/s, v_num=0, train_loss=0.00846, val_loss=0.00178]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 109:  20%|██        | 1/5 [-1:59:50<-1:59:19, -0.10it/s, v_num=0, train_loss=0.00617, val_loss=0.00178]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 99: 100%|██████████| 3/3 [00:00<00:00,  7.23it/s, v_num=0, train_loss=0.00786, val_loss=0.00535]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156021)\u001b[0m   Regression → MSE: 0.005489, MAE: 0.057359\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 111:  40%|████      | 2/5 [00:00<00:00, 27.26it/s, v_num=0, train_loss=0.00742, val_loss=0.00179]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 125:  20%|██        | 1/5 [00:00<00:00, 26.61it/s, v_num=0, train_loss=0.0045, val_loss=0.00183] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 129:  20%|██        | 1/5 [00:00<00:00, 28.29it/s, v_num=0, train_loss=0.00565, val_loss=0.00178]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 130:  60%|██████    | 3/5 [00:00<00:00, 16.03it/s, v_num=0, train_loss=0.00545, val_loss=0.00177]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 130:  60%|██████    | 3/5 [00:00<00:00, 16.01it/s, v_num=0, train_loss=0.0028, val_loss=0.00177] \n",
      "Epoch 79:  33%|███▎      | 1/3 [00:00<00:00,  3.02it/s, v_num=0, train_loss=0.00427, val_loss=0.00162]\u001b[32m [repeated 115x across cluster]\u001b[0m\n",
      "Epoch 131:  60%|██████    | 3/5 [00:10<00:07,  0.28it/s, v_num=0, train_loss=0.0025, val_loss=0.00179] \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 125: 100%|██████████| 5/5 [00:00<00:00, 16.40it/s, v_num=0, train_loss=0.00558, val_loss=0.00183]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Epoch 79: 100%|██████████| 3/3 [00:00<00:00,  5.91it/s, v_num=0, train_loss=0.00146, val_loss=0.00192]\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 79.34it/s]\u001b[A\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 54.89it/s]\u001b[A\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Epoch 79: 100%|██████████| 3/3 [00:00<00:00,  5.66it/s, v_num=0, train_loss=0.00146, val_loss=0.00192]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 77:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00797, val_loss=0.00165]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 123:  20%|██        | 1/5 [00:00<00:00, 17.97it/s, v_num=0, train_loss=0.00306, val_loss=0.00179]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 131:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00364, val_loss=0.00179]\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
      "Epoch 44: 100%|██████████| 5/5 [00:00<00:00,  5.55it/s, v_num=0, train_loss=0.00757, val_loss=0.00145]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Epoch 45:  60%|██████    | 3/5 [00:11<00:07,  0.26it/s, v_num=0, train_loss=0.00249, val_loss=0.00136]\n",
      "Epoch 45:  60%|██████    | 3/5 [00:11<00:07,  0.26it/s, v_num=0, train_loss=0.00531, val_loss=0.00136]\n",
      "Epoch 81: 100%|██████████| 3/3 [-1:59:50<00:00, -0.29it/s, v_num=0, train_loss=0.00722, val_loss=0.00187]   \n",
      "Epoch 81: 100%|██████████| 3/3 [-1:59:50<00:00, -0.29it/s, v_num=0, train_loss=0.00343, val_loss=0.00187]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "Epoch 136:  60%|██████    | 3/5 [00:00<00:00, 16.04it/s, v_num=0, train_loss=0.00328, val_loss=0.00178]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "Epoch 146:  60%|██████    | 3/5 [00:11<00:07,  0.27it/s, v_num=0, train_loss=0.00285, val_loss=0.00179]\u001b[32m [repeated 65x across cluster]\u001b[0m\n",
      "Epoch 146:  40%|████      | 2/5 [00:11<00:16,  0.18it/s, v_num=0, train_loss=0.00255, val_loss=0.00179]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 140: 100%|██████████| 5/5 [00:00<00:00, 19.09it/s, v_num=0, train_loss=0.00374, val_loss=0.00183]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
      "Epoch 145: 100%|██████████| 5/5 [00:00<00:00, 19.26it/s, v_num=0, train_loss=0.00537, val_loss=0.00179]\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 37.98it/s]\u001b[A\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 40.54it/s]\u001b[A\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
      "Epoch 145: 100%|██████████| 5/5 [00:00<00:00, 17.73it/s, v_num=0, train_loss=0.00537, val_loss=0.00179]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 146:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00537, val_loss=0.00179]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 145:  20%|██        | 1/5 [00:00<00:00, 24.85it/s, v_num=0, train_loss=0.00596, val_loss=0.00179]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 89:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00775, val_loss=0.00181]\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "Epoch 88: 100%|██████████| 3/3 [00:00<00:00,  6.72it/s, v_num=0, train_loss=0.00775, val_loss=0.00182]\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "Epoch 147: 100%|██████████| 5/5 [00:00<00:00, 15.95it/s, v_num=0, train_loss=0.0042, val_loss=0.00177] \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 149: 100%|██████████| 5/5 [00:00<00:00, 15.34it/s, v_num=0, train_loss=0.00274, val_loss=0.00166]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m   Regression → MSE: 0.002129, MAE: 0.035417\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156017)\u001b[0m `Trainer.fit` stopped: `max_epochs=150` reached.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 66:  20%|██        | 1/5 [00:00<00:00, 12.47it/s, v_num=0, train_loss=0.00145, val_loss=0.00133]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 119:  33%|███▎      | 1/3 [00:00<00:00,  4.19it/s, v_num=0, train_loss=0.00529, val_loss=0.0016]\u001b[32m [repeated 124x across cluster]\u001b[0m\n",
      "Epoch 63:  40%|████      | 2/5 [00:00<00:00, 11.78it/s, v_num=0, train_loss=0.00246, val_loss=0.00134]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
      "Epoch 119: 100%|██████████| 3/3 [00:10<00:00,  0.27it/s, v_num=0, train_loss=0.00158, val_loss=0.0016]\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 42.67it/s]\u001b[A\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 61.92it/s]\u001b[A\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
      "Epoch 68: 100%|██████████| 5/5 [00:00<00:00,  6.80it/s, v_num=0, train_loss=0.00101, val_loss=0.00134]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 69:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00101, val_loss=0.00134]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 96:  33%|███▎      | 1/3 [00:00<00:00, 14.16it/s, v_num=0, train_loss=0.00438, val_loss=0.00164]\n",
      "Epoch 120:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00158, val_loss=0.00163]\u001b[32m [repeated 59x across cluster]\u001b[0m\n",
      "Epoch 118: 100%|██████████| 3/3 [00:00<00:00,  7.70it/s, v_num=0, train_loss=0.00317, val_loss=0.0016]\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "Epoch 99: 100%|██████████| 3/3 [00:00<00:00,  6.53it/s, v_num=0, train_loss=0.00334, val_loss=0.00189]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156015)\u001b[0m   Regression → MSE: 0.001815, MAE: 0.030199\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 74:  20%|██        | 1/5 [00:00<00:00, 13.74it/s, v_num=0, train_loss=0.0035, val_loss=0.00135] \n",
      "Epoch 76:  60%|██████    | 3/5 [00:00<00:00,  6.75it/s, v_num=0, train_loss=0.00219, val_loss=0.00133]\u001b[32m [repeated 41x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "Epoch 129: 100%|██████████| 3/3 [00:00<00:00,  6.69it/s, v_num=0, train_loss=0.00363, val_loss=0.00163]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 38.63it/s]\u001b[A\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 60.16it/s]\u001b[A\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "Epoch 73: 100%|██████████| 5/5 [00:00<00:00,  7.56it/s, v_num=0, train_loss=0.00726, val_loss=0.00135]\n",
      "Epoch 131:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00303, val_loss=0.0016]\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "Epoch 130: 100%|██████████| 3/3 [00:10<00:00,  0.27it/s, v_num=0, train_loss=0.00303, val_loss=0.00163]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 131: 100%|██████████| 3/3 [-1:59:50<00:00, -0.29it/s, v_num=0, train_loss=0.0061, val_loss=0.00163]\n",
      "Epoch 132:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.0061, val_loss=0.00163]           \n",
      "Epoch 78:  20%|██        | 1/5 [00:00<00:00, 13.85it/s, v_num=0, train_loss=0.00202, val_loss=0.00133]   \n",
      "Epoch 78:  60%|██████    | 3/5 [00:00<00:00, 10.19it/s, v_num=0, train_loss=0.00114, val_loss=0.00133]\n",
      "Epoch 79:  80%|████████  | 4/5 [00:00<00:00, 10.19it/s, v_num=0, train_loss=0.00442, val_loss=0.00133]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 83:  40%|████      | 2/5 [00:00<00:00,  7.60it/s, v_num=0, train_loss=0.00315, val_loss=0.00133]\u001b[32m [repeated 35x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Epoch 142: 100%|██████████| 3/3 [00:11<00:00,  0.27it/s, v_num=0, train_loss=0.00082, val_loss=0.00164]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 44.83it/s]\u001b[A\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 56.77it/s]\u001b[A\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Epoch 142:   0%|          | 0/3 [00:00<?, ?it/s, v_num=0, train_loss=0.00691, val_loss=0.00164]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 141: 100%|██████████| 3/3 [00:00<00:00,  8.13it/s, v_num=0, train_loss=0.00691, val_loss=0.00164]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 142: 100%|██████████| 3/3 [00:11<00:00,  0.27it/s, v_num=0, train_loss=0.00082, val_loss=0.00165]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 79:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00278, val_loss=0.00133]        \n",
      "Epoch 140:  67%|██████▋   | 2/3 [00:00<00:00,  6.44it/s, v_num=0, train_loss=0.00495, val_loss=0.00161]\n",
      "Epoch 143:  33%|███▎      | 1/3 [-1:59:50<-1:59:40, -0.10it/s, v_num=0, train_loss=0.00493, val_loss=0.00165]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 144:  33%|███▎      | 1/3 [00:00<00:00, 10.27it/s, v_num=0, train_loss=0.00349, val_loss=0.00162]   \n",
      "Epoch 84:  60%|██████    | 3/5 [00:00<00:00, 11.30it/s, v_num=0, train_loss=0.00294, val_loss=0.00133]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "Epoch 145:  33%|███▎      | 1/3 [00:00<00:00, 13.73it/s, v_num=0, train_loss=0.0031, val_loss=0.00159]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 86:  20%|██        | 1/5 [00:00<00:00, 18.43it/s, v_num=0, train_loss=0.00107, val_loss=0.00133]\n",
      "Epoch 86: 100%|██████████| 5/5 [00:00<00:00,  7.32it/s, v_num=0, train_loss=0.00449, val_loss=0.00136]\n",
      "Epoch 86: 100%|██████████| 5/5 [00:00<00:00,  7.31it/s, v_num=0, train_loss=0.00449, val_loss=0.00136]\n",
      "Epoch 87:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00449, val_loss=0.00136]        \n",
      "Epoch 87:  40%|████      | 2/5 [00:00<00:00,  5.12it/s, v_num=0, train_loss=0.00146, val_loss=0.00136]\n",
      "Epoch 149: 100%|██████████| 3/3 [00:00<00:00,  6.32it/s, v_num=0, train_loss=0.00199, val_loss=0.00159]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m `Trainer.fit` stopped: `max_epochs=150` reached.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156020)\u001b[0m   Regression → MSE: 0.001711, MAE: 0.030193\n",
      "Epoch 88:  40%|████      | 2/5 [00:00<00:00, 14.84it/s, v_num=0, train_loss=0.00574, val_loss=0.00138]\n",
      "Epoch 90:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00236, val_loss=0.00136]        \n",
      "Epoch 90:  60%|██████    | 3/5 [00:00<00:00,  6.20it/s, v_num=0, train_loss=0.00418, val_loss=0.00136]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 91:  20%|██        | 1/5 [00:00<00:00, 17.67it/s, v_num=0, train_loss=0.00187, val_loss=0.00136]\n",
      "Epoch 94:  40%|████      | 2/5 [00:00<00:00, 10.16it/s, v_num=0, train_loss=0.00361, val_loss=0.00132]\n",
      "Epoch 95:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00324, val_loss=0.00134]        \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 97:  60%|██████    | 3/5 [00:00<00:00,  6.73it/s, v_num=0, train_loss=0.00487, val_loss=0.00135]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 98:  20%|██        | 1/5 [00:00<00:00, 15.37it/s, v_num=0, train_loss=0.00153, val_loss=0.00136]\n",
      "Epoch 100:  40%|████      | 2/5 [00:00<00:00,  9.13it/s, v_num=0, train_loss=0.00151, val_loss=0.00133]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 102:  40%|████      | 2/5 [00:00<00:00, 13.46it/s, v_num=0, train_loss=0.00178, val_loss=0.00134]\n",
      "Epoch 103:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00323, val_loss=0.00133]        \n",
      "Epoch 105:  80%|████████  | 4/5 [00:00<00:00,  9.96it/s, v_num=0, train_loss=0.00727, val_loss=0.00134]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 105:  40%|████      | 2/5 [00:00<00:00,  7.25it/s, v_num=0, train_loss=0.00187, val_loss=0.00134]\u001b[32m [repeated 68x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Epoch 97: 100%|██████████| 5/5 [00:00<00:00,  7.77it/s, v_num=0, train_loss=0.00279, val_loss=0.00135]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 49.88it/s]\u001b[A\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 78.04it/s]\u001b[A\u001b[32m [repeated 30x across cluster]\u001b[0m\n",
      "Epoch 106:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00112, val_loss=0.00134]\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Epoch 105: 100%|██████████| 5/5 [00:00<00:00,  9.88it/s, v_num=0, train_loss=0.00112, val_loss=0.00134]\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Epoch 102: 100%|██████████| 5/5 [00:00<00:00,  7.52it/s, v_num=0, train_loss=0.00323, val_loss=0.00133]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Epoch 85:  20%|██        | 1/5 [00:00<00:00, 13.13it/s, v_num=0, train_loss=0.0038, val_loss=0.00133] \n",
      "Epoch 107:  40%|████      | 2/5 [00:00<00:00,  8.85it/s, v_num=0, train_loss=0.00416, val_loss=0.00133]\n",
      "Epoch 108:  60%|██████    | 3/5 [00:00<00:00, 13.36it/s, v_num=0, train_loss=0.00389, val_loss=0.00134]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 109:  20%|██        | 1/5 [00:00<00:00, 11.34it/s, v_num=0, train_loss=0.0031, val_loss=0.00136]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 112:  20%|██        | 1/5 [00:00<00:00, 12.17it/s, v_num=0, train_loss=0.00239, val_loss=0.00135]\n",
      "Epoch 112:  80%|████████  | 4/5 [00:00<00:00,  8.46it/s, v_num=0, train_loss=0.00633, val_loss=0.00135]\n",
      "Epoch 114:  40%|████      | 2/5 [00:00<00:00,  8.31it/s, v_num=0, train_loss=0.00232, val_loss=0.00133]\n",
      "Epoch 117:  20%|██        | 1/5 [00:00<00:00, 19.60it/s, v_num=0, train_loss=0.00245, val_loss=0.00135]\n",
      "Epoch 117:  60%|██████    | 3/5 [00:00<00:00,  8.60it/s, v_num=0, train_loss=0.0041, val_loss=0.00135] \n",
      "Epoch 118:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00998, val_loss=0.00137]        \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 120:  20%|██        | 1/5 [00:00<00:00, 14.51it/s, v_num=0, train_loss=0.00231, val_loss=0.00137] \n",
      "Epoch 121:  60%|██████    | 3/5 [00:00<00:00,  7.82it/s, v_num=0, train_loss=0.00165, val_loss=0.00135]\u001b[32m [repeated 41x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 120: 100%|██████████| 5/5 [00:00<00:00,  7.85it/s, v_num=0, train_loss=0.00113, val_loss=0.00137]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 47.73it/s]\u001b[A\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 74.97it/s]\u001b[A\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 121:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00113, val_loss=0.00135]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 118: 100%|██████████| 5/5 [00:00<00:00,  8.08it/s, v_num=0, train_loss=0.000704, val_loss=0.00137]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 117: 100%|██████████| 5/5 [00:00<00:00,  8.89it/s, v_num=0, train_loss=0.00998, val_loss=0.00137]\n",
      "Epoch 122:  60%|██████    | 3/5 [00:00<00:00,  7.92it/s, v_num=0, train_loss=0.00397, val_loss=0.00139]\n",
      "Epoch 124:  40%|████      | 2/5 [00:00<00:00,  9.79it/s, v_num=0, train_loss=0.00179, val_loss=0.00134]\n",
      "Epoch 124:  80%|████████  | 4/5 [00:00<00:00,  7.72it/s, v_num=0, train_loss=0.00124, val_loss=0.00134]\n",
      "Epoch 125:  40%|████      | 2/5 [00:00<00:00, 17.86it/s, v_num=0, train_loss=0.00251, val_loss=0.00136] \n",
      "Epoch 125:  80%|████████  | 4/5 [00:00<00:00, 11.70it/s, v_num=0, train_loss=0.00207, val_loss=0.00136]\n",
      "Epoch 126:  80%|████████  | 4/5 [00:00<00:00,  7.32it/s, v_num=0, train_loss=0.00206, val_loss=0.00137]\n",
      "Epoch 127:  40%|████      | 2/5 [00:00<00:00, 16.50it/s, v_num=0, train_loss=0.00273, val_loss=0.00136]\n",
      "Epoch 128: 100%|██████████| 5/5 [00:00<00:00,  8.04it/s, v_num=0, train_loss=0.00152, val_loss=0.0014]\n",
      "Epoch 129:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00152, val_loss=0.0014]        \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 129:  60%|██████    | 3/5 [00:00<00:00,  9.22it/s, v_num=0, train_loss=0.00277, val_loss=0.0014]\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 129: 100%|██████████| 5/5 [00:00<00:00,  8.25it/s, v_num=0, train_loss=0.00253, val_loss=0.0014]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 49.43it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 77.53it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 130:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00253, val_loss=0.00134]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Epoch 126: 100%|██████████| 5/5 [00:00<00:00,  8.55it/s, v_num=0, train_loss=0.00168, val_loss=0.00137]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 130: 100%|██████████| 5/5 [-1:59:51<00:00, -0.50it/s, v_num=0, train_loss=0.00143, val_loss=0.00135]\n",
      "Epoch 131:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00143, val_loss=0.00135]           \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 132:  80%|████████  | 4/5 [00:00<00:00,  7.09it/s, v_num=0, train_loss=0.00391, val_loss=0.00136]\n",
      "Epoch 133:  60%|██████    | 3/5 [00:00<00:00,  9.30it/s, v_num=0, train_loss=0.00211, val_loss=0.00134]\n",
      "Epoch 133: 100%|██████████| 5/5 [00:00<00:00,  8.04it/s, v_num=0, train_loss=0.00738, val_loss=0.00136]\n",
      "Epoch 134:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00738, val_loss=0.00136]        \n",
      "Epoch 135: 100%|██████████| 5/5 [00:00<00:00,  6.89it/s, v_num=0, train_loss=0.00204, val_loss=0.00134]\n",
      "Epoch 136:  20%|██        | 1/5 [00:00<00:00, 16.58it/s, v_num=0, train_loss=0.001, val_loss=0.00134]  \n",
      "Epoch 137:  40%|████      | 2/5 [00:11<00:16,  0.18it/s, v_num=0, train_loss=0.00209, val_loss=0.00133]\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 135: 100%|██████████| 5/5 [00:00<00:00,  7.21it/s, v_num=0, train_loss=0.00204, val_loss=0.00133]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 46.50it/s]\u001b[A\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 72.96it/s]\u001b[A\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 137:   0%|          | 0/5 [00:00<?, ?it/s, v_num=0, train_loss=0.00297, val_loss=0.00133]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 136: 100%|██████████| 5/5 [00:00<00:00,  7.89it/s, v_num=0, train_loss=0.00297, val_loss=0.00134]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 138:  60%|██████    | 3/5 [00:00<00:00,  7.42it/s, v_num=0, train_loss=0.00663, val_loss=0.00137]\n",
      "Epoch 139:  40%|████      | 2/5 [00:00<00:00,  8.86it/s, v_num=0, train_loss=0.00166, val_loss=0.00136]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 141:  80%|████████  | 4/5 [00:00<00:00,  6.82it/s, v_num=0, train_loss=0.00223, val_loss=0.00136]\n",
      "Epoch 142:  40%|████      | 2/5 [00:00<00:00, 17.08it/s, v_num=0, train_loss=0.00161, val_loss=0.00135]\n",
      "Epoch 142:  80%|████████  | 4/5 [00:00<00:00,  9.06it/s, v_num=0, train_loss=0.00318, val_loss=0.00135]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 143:  80%|████████  | 4/5 [00:00<00:00,  8.71it/s, v_num=0, train_loss=0.00369, val_loss=0.00135]\n",
      "Epoch 144:  40%|████      | 2/5 [00:00<00:00, 18.04it/s, v_num=0, train_loss=0.00259, val_loss=0.00135]\n",
      "Epoch 145:  40%|████      | 2/5 [00:00<00:00,  9.82it/s, v_num=0, train_loss=0.00175, val_loss=0.00135]\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "Epoch 147:  40%|████      | 2/5 [00:00<00:00, 12.74it/s, v_num=0, train_loss=0.00211, val_loss=0.00134]\n",
      "Epoch 149:  40%|████      | 2/5 [00:00<00:00,  6.46it/s, v_num=0, train_loss=0.00192, val_loss=0.00134]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m `Trainer.fit` stopped: `max_epochs=150` reached.\n",
      "2025-09-16 01:13:15,294\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/iatell/projects/meta-learning/tune_logs/cnn_lstm_tuning' in 0.0034s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 5/5 [00:00<00:00,  8.07it/s, v_num=0, train_loss=0.0017, val_loss=0.00135]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 01:13:15,299\tINFO tune.py:1041 -- Total run time: 152.61 seconds (152.57 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Best Config: {'hidden_dim': 256, 'num_layers': 1, 'attention_name': 'tanh_attention', 'lr': 0.004757249272982139, 'optimizer_name': 'adamw', 'scheduler_name': 'reduce_on_plateau', 'scheduler_params': {'factor': 0.2525681845866281, 'patience': 3, 'T_max': 10, 'eta_min': 3.860995904914844e-05}, 'optimizer_params': {'weight_decay': 6.51102196686824e-05}, 'kernels': [3, 5, 7, 11], 'cnn_out_channels': 32, 'first_drop': 0.20189569032451213, 'second_drop': 0.3147822852430936, 'third_drop': 0.45103251677083245, 'batch_size': 64, 'max_epochs': 150}\n",
      "Best Accuracy: -0.0016\n",
      "\n",
      "🔁 Retraining best model on full dataset for saving...\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m \n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m 📊 Validation Metrics (Hungarian matched):\n",
      "\u001b[36m(train_cnn_lstm_tune pid=156014)\u001b[0m   Regression → MSE: 0.001562, MAE: 0.026651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG SAMPLE CHECK (Torch mode) ===\n",
      "\n",
      "--- Sequence 0 ---\n",
      "Label: [1.143628 0.       0.       0.       0.       0.       0.       0.\n",
      " 0.      ] Encoded (padded): [1.143628 0.       0.       0.       0.       0.       0.       0.\n",
      " 0.      ]\n",
      "[main] Shape: (5, 4)\n",
      "[main] First few rows:\n",
      " [[ 0.01562355 -0.00180042 -0.01639293  0.0093857 ]\n",
      " [ 0.00938704  0.12409948  0.04899828  0.12622231]\n",
      " [ 0.12622082 -0.00192766  0.09665822  0.00645032]\n",
      " [ 0.00645032 -0.00251821 -0.02505807 -0.05388233]\n",
      " [-0.04985064 -0.0454773  -0.17924407 -0.07724382]]\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 01:13:15.857479: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-16 01:13:15.868540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757972595.880236  153878 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757972595.883575  153878 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757972595.895234  153878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757972595.895256  153878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757972595.895258  153878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757972595.895259  153878 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-16 01:13:15.899627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name          | Type          | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | branches      | ModuleList    | 3.7 K  | train\n",
      "1 | fusion_conv2d | Sequential    | 15     | train\n",
      "2 | lstm          | LSTM          | 296 K  | train\n",
      "3 | attention     | TanhAttention | 66.0 K | train\n",
      "4 | regressor     | Sequential    | 34.1 K | train\n",
      "5 | loss_fn_reg   | MSELoss       | 0      | train\n",
      "--------------------------------------------------------\n",
      "400 K     Trainable params\n",
      "0         Non-trainable params\n",
      "400 K     Total params\n",
      "1.603     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9065c1df7d4249bfaeb533d64da6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/iatell/envs/Rllib2.43/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ebe68e6c8448338ef5bc07c7bf9d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bcc8341c054b8fbeec9f3e1f150f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f1aa7065564caa8aedf545ad70f1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d9a54032d3437f9b33f595a7c26c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26ee70921174296bdde3f034e77306e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1954792535be4b648f2c4e677c1b7182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68389ae4fe74e828d53c84e980d69c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17200e2e7934ec4a3d00a31669ae050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97de7e4c41ef4e90884baf1a181b0015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1145797eb60548d08514c1a7e06bee3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c13051ce4d4f1ba36843864814a62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f513d2111b01424aa8cff29d57765662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed3bf94e0d7481d8cd1b662b2136712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1902a3c9c5f1469fabd3499cf8a36fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0cd6d988c749eabe28fc8ff1b3da32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefbd1e8a1e94f3299dd88e55a714e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba0badb247341c9a5839f998e014ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a201f7cd95d4bcab85b225db0ff2b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28807227be24b6e99307709e47ca93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b826b6282913492d98030c8e53657a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e13f586f5664dddb18558ce608c9645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d040d167d3946dfbf6da5c056b7bb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeddff37ff164ca48d0e4436b6d00ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c502b23c26240febddae7d01d97dfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605ae7a6a0894b78a49431817d2957c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09340bbb89f7429cbb0b1d07da87fcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d5c54fc6fa4ff3803a8c844dc9aba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c226d66296c94c75b6fa6a8cf68cde3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32731277c35f4ef9a6d3107bed5c2c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5252a995bd7467d9b598aef308047a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf06d5c573e400f8a7cf4540ffdd7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878c2f1b998e4095b54af583f371a55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d4522734ff4353bb7ab0d76e549419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e85daecff504b5fa6ce4c3110e6d43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb9371e045f40d0b3f2032e742172af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c6b6cbed3d49a0bcd36f3e4a374ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11ffeec90174c42abf451df53749882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b239ee81ac3e419ca3c3196620251fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e47680e10e743959db1116fa3a9fa30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fe96a7a5ee470a967b00c553fc9f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecf8fbe64bc4c42a736f2215dd952b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bcc5751288489e887f6b6e3ad4762d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dc097e1ad1416d9fdba00c5f5ca9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df768ecb1234d46a8e975e5291e979c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c38779c23a04fada5be846f98458ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1e1196f63446e4b895016d64106e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af5b2171ae9450fb47a257f5d505187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9d6295bd754ef7b7f5fac38e84ba6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed1fd46456940f99b536241b169683d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5d5e49b1e941c599eaae508a8bff6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2ef2f32e894e37a2645bd3017ba2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800d7d6aaf3e4b9098a70f1ea826e372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f817779f854827bdbc3df989be5ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c342b80f4d4d4e70b443c3d1a7e7b3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903816f55a9045178dad9f22fcd9d9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d24578a2b2244bbb8d37cef0b274ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff72e2ea2cbd4c8e9d270a9e2747169c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e034c6c1c6ab4b4189799db5c19d2325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b531b38dd74251a3476924a4ca3833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90a5e4619264c04baa3b0ed6c80b9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e2f8842f5a447dbcc9940fd49556fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520a620432334bffaf70d45204221abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71175e2d8593493c99d49528bdc56441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb6051f25f1406d94fd38b5a9b610a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b437632a970649cda341ec45cf2274eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6346a1fd3052437bb23a2b043aa6e58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3301761ee840aabec98db708c5d838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2acccca78642719314313154829f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426f25378e164526a940430ebb0c417c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8920e474e4634c449cb0be00313cbaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc3494e58344ae5880a0e3b91ff0dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f422991a744a0fad593de0e3c5eb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdfd96e148d4ae29b2d02c73ec1ef2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1fe91190c544c3b7078286707b0b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddac87070744b8ba1e868c43362a805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f835047e824b309fb8b5288948b55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25efea4492c84bd69566fbcd452640a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737669a67c294d75970eb0f2db5ea018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54ff712d1844ec68b1a5f49b74bcbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b833db63cd174b65abe36635ebcd51c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc83234484fc42a196ac8be8d420108b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eeddb07ad6409c8392956e42d5d5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814cf973e5174d5bba9a87065cec2cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdc2478a2834127b66af63a959ad9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6125f7e1aa6a4c88a8daf2b2bd624321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d262323653014d7599ef8da4c8c4b994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cf454c2ad64a27beef833b67dd2574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08af8ce77cac4a40b2055f1eee95d005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641f70a7851b42fb9ec8e3d5b24b2f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccae0bf10df43afb908fe4429068024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf47cc74d244573af9546384371105d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d2a01a173f434c94018f3bb4372659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d3a17b6a874e1d8380de865ee96a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0770bb3ee0e346d28a394b218ad10ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c340d6080e4d3f94afe6e255d34f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be8d0364d8247af80c6cb2fbd7c8732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c159ec3e84ee4594b99b0cfc9b60907a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c615a7952df4c9aa549fc3abb2e583e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb56ac9af5c4f6fbeb90c8affaebc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9c89fccb0a44f88b8baf6a1fa7556f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9772c739df4f18b4438721cd7e13e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bfcefb45f54958a1b065aaf0dcd2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913c9efd019e4f358b6f0c9790558dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f297e2b19b4a1e83dbaf7093a4c696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5caf913aac14333bb05a69c3ef7b481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb28c742b1fc419cabd7f5c38093de16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec264b6982c475ab91a49a366834a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00e742568b54ed6b8099b69dcc47727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed28903251db452e909ed341d95f14ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433c7916336541cb996ede8d8b1ae010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0630a4a48d674582a4cff166bf9d4dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3380ad5274748de9fb0c759d8348ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27d9a14f8bd4fc28b61f7c2392a14a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716c008ee2444b7594f63b1cfcf265ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01997b1e708d48f98ce0baae8ab7d094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9ea66c4b1a42b3ad3f3d7a9685ebbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c634ce43c84906b8dcaa1aeb13f37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ad84dc7cac4724976121a141f4eef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80038a3a4ad74b3cbc5e6da5f7064bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ff5ddb24e34b58a97786f84bd05268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefecac9f5524162a6acb37e4fb5be1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99916027369c4ac2b6e647e5755d0210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273b6e4620634f4b8d22f1063054ecc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1b7327a6ca4f98acbac5756775276c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba757bac67ed4c4bb81221cfbf513334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc37a6a9e3e4e679d4658d21dfc214c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c6ad44213c4d86b4be494754dd0bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c736b2d0723f45fc8d27fc38725be385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09107698dce461e82a41d0c487744d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714d06ae599e4c07889738c700f37ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ca2927d4494b26ae74d44e94f576e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4a0a46b4944655853298fa9a1e65d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0834a20792400982e8d45e5a980293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da01ba958c74421bd9058cb582cda75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7148eae5ccf424bb573735f340191a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce28d11c7ef47ee84d43e65ea17a149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce57e3e2cf824a06a7585a33df29a0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9c371a90564160a427b71e92aeef18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea465fdfd9d74548a8b122ca7c514efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbb4f056ff14d1593573ba55ad9b4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9d4c736de5420886430461f0a5c432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c17869684e3498082d23b94426957ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fd3ba7d0f34244b5915ec370cf2092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84d7d58f413485795d94ffa20183215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8842bfcd360546d0905e93b738e66a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430a704f2917482499a0ce599fc285d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa15f233f7a24f3883dc273cf90a22bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf97d7143d9f4ac89ea6a60a31a0b4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6568635caf49c78fd2896ee92cddea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad751d281eb4b31a4ea62436d897ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5148ff316f2467690a449a8180450e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to models/saved_models/lstm_model_multireg_multihead_20250916_011315.pt\n",
      "✅ Meta saved to models/saved_models/lstm_meta_multireg_multihead_20250916_011315.pkl\n",
      "\n",
      "📊 Validation Metrics (Hungarian matched):\n",
      "  Regression → MSE: 0.001342, MAE: 0.026533\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import psutil\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray import air\n",
    "def resource_usage():\n",
    "    \"\"\"Print current CPU, RAM, and GPU usage.\"\"\"\n",
    "    cpu = psutil.cpu_percent(interval=0.5)\n",
    "    ram = psutil.virtual_memory().percent\n",
    "    usage = f\"💻 CPU: {cpu:.1f}% | 🧠 RAM: {ram:.1f}%\"\n",
    "    try:\n",
    "        import GPUtil\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        if gpus:\n",
    "            usage += f\" | 🎮 GPU: {gpus[0].load*100:.1f}% VRAM: {gpus[0].memoryUtil*100:.1f}%\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "    print(usage)\n",
    "\n",
    "\n",
    "def train_cnn_lstm_tune(config):\n",
    "    \"\"\"\n",
    "    Single Ray Tune trial.\n",
    "    Args:\n",
    "        config (dict): hyperparameters for this trial.\n",
    "    \"\"\"\n",
    "    resource_usage()  # Show current hardware usage\n",
    "\n",
    "    # Train using existing train_model function\n",
    "    metrics = train_model(\n",
    "        \"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "        \"/home/iatell/projects/meta-learning/data/line_seq_ordered_added.csv\",\n",
    "        do_validation=True,\n",
    "        model_out_dir=\"models/tuned\",\n",
    "        # seq_len=config[\"seq_len\"],\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        # num_layers=config[\"num_layers\"],\n",
    "        lr=config[\"lr\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        return_val_accuracy=True,  # Expects dict with \"accuracy\" and optionally \"loss\"\n",
    "        save_model=False  # Never save during search\n",
    "    )\n",
    "\n",
    "    # Report metrics to Ray Tune\n",
    "    tune.report(metrics)\n",
    "\n",
    "def run_tuning(save_model=True):\n",
    "    \"\"\"Hyperparameter tuning for CNN LSTM with Ray Tune.\"\"\"\n",
    "\n",
    "    search_space = {\n",
    "        # LSTM / model\n",
    "        \"hidden_dim\": tune.choice([32, 64, 128, 256]),\n",
    "        \"num_layers\": tune.choice([1, 2, 3]),\n",
    "        \"attention_name\": tune.choice([\"simple_attention\", \"tanh_attention\"]),\n",
    "\n",
    "        # Learning rate & optimizer\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"optimizer_name\": tune.choice([\"adamw\", \"adam\"]),\n",
    "\n",
    "        # Scheduler\n",
    "        \"scheduler_name\": tune.choice([\"reduce_on_plateau\", \"cosine\", \"onecycle\"]),\n",
    "        \"scheduler_params\": {\n",
    "            \"factor\": tune.loguniform(0.05, 0.5),      # only used for ReduceLROnPlateau\n",
    "            \"patience\": tune.choice([2, 3, 5, 7]),     # only used for ReduceLROnPlateau\n",
    "            \"T_max\": tune.choice([10, 20, 50]),        # only used for CosineAnnealingLR\n",
    "            \"eta_min\": tune.loguniform(1e-6, 1e-4)    # only used for CosineAnnealingLR\n",
    "        },\n",
    "        \"optimizer_params\": {\n",
    "            \"weight_decay\": tune.loguniform(1e-5, 1e-2)\n",
    "        },\n",
    "\n",
    "        # CNN params\n",
    "        \"kernels\": tune.choice([[3,5,7,11], [3,5,7], [3,5]]),\n",
    "        \"cnn_out_channels\": tune.choice([16, 32, 64]),\n",
    "        \"first_drop\": tune.uniform(0.1, 0.5),\n",
    "        \"second_drop\": tune.uniform(0.1, 0.5),\n",
    "        \"third_drop\": tune.uniform(0.1, 0.5),\n",
    "\n",
    "        # Training\n",
    "        \"batch_size\": tune.choice([32, 64, 128]),\n",
    "        \"max_epochs\": tune.choice([50, 100, 150]),\n",
    "    }    \n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"accuracy\",  # must exist in metrics dict from train_model\n",
    "        mode=\"max\",\n",
    "        grace_period=1,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        train_cnn_lstm_tune,\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            num_samples=10\n",
    "        ),\n",
    "    run_config=air.RunConfig(\n",
    "        name=\"cnn_lstm_tuning\",\n",
    "        storage_path=\"/home/iatell/projects/meta-learning/tune_logs\",\n",
    "    ),\n",
    "    # runtime_env=runtime_env\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "    # Best trial\n",
    "    best_result = results.get_best_result(metric=\"accuracy\", mode=\"max\")\n",
    "    print(\"\\n🏆 Best Config:\", best_result.config)\n",
    "    print(f\"Best Accuracy: {best_result.metrics['accuracy']:.4f}\")\n",
    "\n",
    "    # Optional: retrain best model on full data and save\n",
    "    if save_model:\n",
    "        print(\"\\n🔁 Retraining best model on full dataset for saving...\")\n",
    "\n",
    "        # Map scheduler params depending on scheduler type\n",
    "        scheduler_name = best_result.config.get(\"scheduler_name\")\n",
    "        scheduler_params_config = best_result.config.get(\"scheduler_params\", {})\n",
    "\n",
    "        if scheduler_name == \"reduce_on_plateau\":\n",
    "            scheduler_params = {\n",
    "                \"factor\": scheduler_params_config.get(\"factor\", 0.2),\n",
    "                \"patience\": scheduler_params_config.get(\"patience\", 3)\n",
    "            }\n",
    "        elif scheduler_name == \"cosine\":\n",
    "            scheduler_params = {\n",
    "                \"T_max\": scheduler_params_config.get(\"T_max\", 10),\n",
    "                \"eta_min\": scheduler_params_config.get(\"eta_min\", 1e-6)\n",
    "            }\n",
    "        else:  # onecycle or others\n",
    "            scheduler_params = {}\n",
    "\n",
    "        # Optimizer params\n",
    "        optimizer_params = best_result.config.get(\"optimizer_params\", {\"weight_decay\": 0.01})\n",
    "\n",
    "        train_model(\n",
    "            data_csv=\"/home/iatell/projects/meta-learning/data/Bitcoin_BTCUSDT_kaggle_1D_candles.csv\",\n",
    "            labels_csv=\"/home/iatell/projects/meta-learning/data/line_seq_ordered_added.csv\",\n",
    "            do_validation=True,\n",
    "            return_val_accuracy=True,\n",
    "            model_out_dir=\"models/saved_models\",\n",
    "            hidden_dim=best_result.config.get(\"hidden_dim\", 32),\n",
    "            num_layers=best_result.config.get(\"num_layers\", 1),\n",
    "            attention_name=best_result.config.get(\"attention_name\", \"tanh_attention\"),\n",
    "            optimizer_name=best_result.config.get(\"optimizer_name\", \"adamw\"),\n",
    "            lr=best_result.config.get(\"lr\", 1e-3),\n",
    "            batch_size=best_result.config.get(\"batch_size\", 32),\n",
    "            max_epochs=best_result.config.get(\"max_epochs\", 10),\n",
    "            kernels=best_result.config.get(\"kernels\", [3, 5, 7, 11]),\n",
    "            cnn_out_channels=best_result.config.get(\"cnn_out_channels\", 32),\n",
    "            first_drop=best_result.config.get(\"first_drop\", 0.3),\n",
    "            second_drop=best_result.config.get(\"second_drop\", 0.3),\n",
    "            third_drop=best_result.config.get(\"third_drop\", 0.3),\n",
    "            scheduler_name=scheduler_name,\n",
    "            scheduler_params=scheduler_params,\n",
    "            optimizer_params=optimizer_params\n",
    "        )\n",
    "if __name__ == \"__main__\":\n",
    "    run_tuning(save_model=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rllib2.43 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
